{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to graphglue","text":"<p>This is the main documentation page.</p>"},{"location":"API_demo_notebook/","title":"API demo notebook","text":"In\u00a0[1]: Copied! <pre># preferred public surface\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\"..\"))  # adds .../src to sys.path\nfrom annnet.adapters.networkx_adapter import from_nx, to_backend, to_nx\nfrom annnet.core import Graph\n</pre> # preferred public surface import os import sys  sys.path.insert(0, os.path.abspath(\"..\"))  # adds .../src to sys.path from annnet.adapters.networkx_adapter import from_nx, to_backend, to_nx from annnet.core import Graph In\u00a0[3]: Copied! <pre>print(\"=== Basic graph creation ===\")\nG = Graph()\nG.add_vertex(\"A\", color=\"red\", name=\"amine\")\nG.add_vertex(\"B\")\n\n\neidx = G.add_edge({\"A\"}, {\"B\"})\n\neidx = G.add_edge(\"D\", \"C\")\n\nprint(\"Vertices:\", tuple(G.V))\n\nprint(\"Edges (index, (S,T)):\", [(i, G.get_edge(i)) for i in range(G.num_edges)])\nprint(\"Number of vertices:\", G.num_vertices)\nprint(\"Number of edges:\", G.num_edges)\nprint(\"Shape (|V|,|E|):\", G.shape)\nprint()\n\nprint(\"=== Accessing edge info ===\")\nS, T = G.get_edge(eidx)\nprint(\"Edge definition:\", S, \"-&gt;\", T)\nprint()\n\nprint(\"=== Incidence matrix with per-vertex coefficients ===\")\nH = Graph()\nH.add_vertices([\"X\", \"Y\"])\n\ne2 = H.add_edge(\n    \"Xa\", \"Ya\", edge_source_attr={\"Xa\": {\"__value\": 3}}, edge_target_attr={\"Ya\": {\"__value\": 2}}\n)\n\ne2 = H.add_edge(\"X\", \"Y\")\nattrs = H.get_edge_attrs(e2)\nsrc_map = dict(attrs.get(\"__source_attr\", {}))\ntgt_map = dict(attrs.get(\"__target_attr\", {}))\nsrc_map[\"X\"] = {\"__value\": 3}\ntgt_map[\"Y\"] = {\"__value\": 2}\nH.set_edge_attrs(e2, __source_attr=src_map, __target_attr=tgt_map)\n\nmat = H.vertex_incidence_matrix(values=True, sparse=False)\nprint(\"Vertices:\", tuple(H.V))\nprint(\"Incidence matrix:\\n\", mat)\nprint()\n\nprint(\"=== Subgraph ===\")\nSG = G.edge_subgraph([eidx])  # keep only that edge\nprint(\"Edge-subgraph edges:\", [(i, SG.get_edge(i)) for i in range(SG.num_edges)])\nprint()\n\nprint(\"=== Prune (if available) ===\")\nif hasattr(G, \"prune\"):\n    P = G.prune(source=[\"A\"], target=[\"C\"])\n    print(\"Pruned graph edges:\", [(i, P.get_edge(i)) for i in range(P.num_edges)])\nelse:\n    print(\"G.prune not implemented; skipping.\")\nprint()\n\nprint(\"=== slices and per-slice weights ===\")\nD = Graph()\nD.add_vertex(\"A\")\nD.add_vertex(\"B\")\nD.add_slice(\"t0\")\n\ne = D.add_edge(\"A\", \"B\", weight=1.5, label=\"ab\")\neid = D.idx_to_edge[e] if isinstance(e, int) else e\n\n# Attach same edge to slice (preserve stable eid), set per-slice weight\nw = D.edge_weights.get(eid, None)\nD.add_edge(\"A\", \"B\", slice=\"t0\", edge_id=eid, edge_directed=True, weight=w)\nD.set_slice_edge_weight(\"t0\", eid, 2.0)\n\nprint(\"Edge definitions:\", list(D.edge_definitions.items()))\nprint(\"Global edge weight:\", D.edge_weights.get(eid))\nprint(\"slice t0 weight:\", D.get_effective_edge_weight(eid, slice=\"t0\"))\nprint(\"slices containing edge:\", D.edge_presence_across_slices(eid, include_default=True))\n</pre> print(\"=== Basic graph creation ===\") G = Graph() G.add_vertex(\"A\", color=\"red\", name=\"amine\") G.add_vertex(\"B\")   eidx = G.add_edge({\"A\"}, {\"B\"})  eidx = G.add_edge(\"D\", \"C\")  print(\"Vertices:\", tuple(G.V))  print(\"Edges (index, (S,T)):\", [(i, G.get_edge(i)) for i in range(G.num_edges)]) print(\"Number of vertices:\", G.num_vertices) print(\"Number of edges:\", G.num_edges) print(\"Shape (|V|,|E|):\", G.shape) print()  print(\"=== Accessing edge info ===\") S, T = G.get_edge(eidx) print(\"Edge definition:\", S, \"-&gt;\", T) print()  print(\"=== Incidence matrix with per-vertex coefficients ===\") H = Graph() H.add_vertices([\"X\", \"Y\"])  e2 = H.add_edge(     \"Xa\", \"Ya\", edge_source_attr={\"Xa\": {\"__value\": 3}}, edge_target_attr={\"Ya\": {\"__value\": 2}} )  e2 = H.add_edge(\"X\", \"Y\") attrs = H.get_edge_attrs(e2) src_map = dict(attrs.get(\"__source_attr\", {})) tgt_map = dict(attrs.get(\"__target_attr\", {})) src_map[\"X\"] = {\"__value\": 3} tgt_map[\"Y\"] = {\"__value\": 2} H.set_edge_attrs(e2, __source_attr=src_map, __target_attr=tgt_map)  mat = H.vertex_incidence_matrix(values=True, sparse=False) print(\"Vertices:\", tuple(H.V)) print(\"Incidence matrix:\\n\", mat) print()  print(\"=== Subgraph ===\") SG = G.edge_subgraph([eidx])  # keep only that edge print(\"Edge-subgraph edges:\", [(i, SG.get_edge(i)) for i in range(SG.num_edges)]) print()  print(\"=== Prune (if available) ===\") if hasattr(G, \"prune\"):     P = G.prune(source=[\"A\"], target=[\"C\"])     print(\"Pruned graph edges:\", [(i, P.get_edge(i)) for i in range(P.num_edges)]) else:     print(\"G.prune not implemented; skipping.\") print()  print(\"=== slices and per-slice weights ===\") D = Graph() D.add_vertex(\"A\") D.add_vertex(\"B\") D.add_slice(\"t0\")  e = D.add_edge(\"A\", \"B\", weight=1.5, label=\"ab\") eid = D.idx_to_edge[e] if isinstance(e, int) else e  # Attach same edge to slice (preserve stable eid), set per-slice weight w = D.edge_weights.get(eid, None) D.add_edge(\"A\", \"B\", slice=\"t0\", edge_id=eid, edge_directed=True, weight=w) D.set_slice_edge_weight(\"t0\", eid, 2.0)  print(\"Edge definitions:\", list(D.edge_definitions.items())) print(\"Global edge weight:\", D.edge_weights.get(eid)) print(\"slice t0 weight:\", D.get_effective_edge_weight(eid, slice=\"t0\")) print(\"slices containing edge:\", D.edge_presence_across_slices(eid, include_default=True)) <pre>=== Basic graph creation ===\nVertices: ('A', 'B', 'D', 'C')\nEdges (index, (S,T)): [(0, (frozenset({'A'}), frozenset({'B'}))), (1, (frozenset({'D'}), frozenset({'C'})))]\nNumber of vertices: 4\nNumber of edges: 2\nShape (|V|,|E|): (4, 2)\n\n=== Accessing edge info ===\nEdge definition: frozenset({'D'}) -&gt; frozenset({'C'})\n\n=== Incidence matrix with per-vertex coefficients ===\nVertices: ('X', 'Y', 'Xa', 'Ya')\nIncidence matrix:\n [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n [ 0. -1.  0.  0.  0.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.  0.  0.  0.]\n [-1.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n\n=== Subgraph ===\nEdge-subgraph edges: [(0, (frozenset({'D'}), frozenset({'C'})))]\n\n=== Prune (if available) ===\nG.prune not implemented; skipping.\n\n=== slices and per-slice weights ===\nEdge definitions: [('edge_0', ('A', 'B', 'regular'))]\nGlobal edge weight: 1.5\nslice t0 weight: 2.0\nslices containing edge: ['default', 't0']\n</pre> In\u00a0[5]: Copied! <pre># convert to a networkx graph\nimport networkx as nx\n\nnxG = to_backend(G, directed=True)  # or directed=False if you prefer\n\nprint(\"Nodes:\", nxG.nodes(data=True))\nprint(\"Edges:\", nxG.edges(data=True))\n\n# quick visualization\nimport matplotlib.pyplot as plt\n\npos = nx.spring_layout(nxG)\nnx.draw(nxG, pos, with_labels=True, node_color=\"lightblue\")\nnx.draw_networkx_edge_labels(nxG, pos, edge_labels=nx.get_edge_attributes(nxG, \"weight\"))\nplt.show()\n</pre> # convert to a networkx graph import networkx as nx  nxG = to_backend(G, directed=True)  # or directed=False if you prefer  print(\"Nodes:\", nxG.nodes(data=True)) print(\"Edges:\", nxG.edges(data=True))  # quick visualization import matplotlib.pyplot as plt  pos = nx.spring_layout(nxG) nx.draw(nxG, pos, with_labels=True, node_color=\"lightblue\") nx.draw_networkx_edge_labels(nxG, pos, edge_labels=nx.get_edge_attributes(nxG, \"weight\")) plt.show() <pre>Nodes: [('A', {'color': 'red', 'name': 'amine'}), ('B', {'color': None, 'name': None}), ('D', {'color': None, 'name': None}), ('C', {'color': None, 'name': None})]\nEdges: [('A', 'B', {'__weight': 1.0}), ('D', 'C', {'__weight': 1.0})]\n</pre> In\u00a0[7]: Copied! <pre>nxG, man = to_nx(G, directed=True, hyperedge_mode=\"skip\")\n</pre> nxG, man = to_nx(G, directed=True, hyperedge_mode=\"skip\") In\u00a0[9]: Copied! <pre>H2 = from_nx(nxG, man)\nprint(list(H2.edges()))\n</pre> H2 = from_nx(nxG, man) print(list(H2.edges())) <pre>['edge_0', 'edge_1']\n</pre> In\u00a0[11]: Copied! <pre>for eid in H2.edges():  # eid is a string\n    eidx = H2.edge_to_idx[eid]  # index (int), if you need it\n    S, T = H2.get_edge(eid)  # make sure get_edge accepts id OR index\n    attrs = H2.get_edge_attrs(eid)  # &lt;-- use the dict getter you added\n    print(eid, S, T, attrs)\n</pre> for eid in H2.edges():  # eid is a string     eidx = H2.edge_to_idx[eid]  # index (int), if you need it     S, T = H2.get_edge(eid)  # make sure get_edge accepts id OR index     attrs = H2.get_edge_attrs(eid)  # &lt;-- use the dict getter you added     print(eid, S, T, attrs) <pre>edge_0 frozenset({'A'}) frozenset({'B'}) {}\nedge_1 frozenset({'C'}) frozenset({'D'}) {}\n</pre> In\u00a0[13]: Copied! <pre>print(H2.vertex_attributes.head())  # attrs present\nprint(H2.edge_attributes.head())  # attrs present\n</pre> print(H2.vertex_attributes.head())  # attrs present print(H2.edge_attributes.head())  # attrs present <pre>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vertex_id \u2506 color \u2506 name  \u2502\n\u2502 ---       \u2506 ---   \u2506 ---   \u2502\n\u2502 str       \u2506 str   \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A         \u2506 red   \u2506 amine \u2502\n\u2502 B         \u2506 null  \u2506 null  \u2502\n\u2502 D         \u2506 null  \u2506 null  \u2502\n\u2502 C         \u2506 null  \u2506 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nshape: (0, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id \u2502\n\u2502 ---     \u2502\n\u2502 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[15]: Copied! <pre>G.vertex_attributes.head()\n</pre> G.vertex_attributes.head() Out[15]: shape: (4, 3)vertex_idcolornamestrstrstr\"A\"\"red\"\"amine\"\"B\"nullnull\"D\"nullnull\"C\"nullnull <p>g</p> In\u00a0[18]: Copied! <pre>from annnet.adapters.igraph_adapter import from_igraph, to_igraph\n</pre> from annnet.adapters.igraph_adapter import from_igraph, to_igraph In\u00a0[20]: Copied! <pre>igG, manifest = to_igraph(G, directed=True, hyperedge_mode=\"skip\", public_only=False)\nH = from_igraph(igG, manifest)\nH\n</pre> igG, manifest = to_igraph(G, directed=True, hyperedge_mode=\"skip\", public_only=False) H = from_igraph(igG, manifest) H Out[20]: <pre>&lt;annnet.core.graph.Graph at 0x1efffb986e0&gt;</pre> In\u00a0[22]: Copied! <pre>nxG, _ = to_nx(G, directed=True, hyperedge_mode=\"skip\")\n\npos = nx.spring_layout(nxG)\nnx.draw(nxG, pos, with_labels=True, node_color=\"lightblue\")\nnx.draw_networkx_edge_labels(nxG, pos, edge_labels=nx.get_edge_attributes(nxG, \"weight\"))\nplt.show()\n</pre> nxG, _ = to_nx(G, directed=True, hyperedge_mode=\"skip\")  pos = nx.spring_layout(nxG) nx.draw(nxG, pos, with_labels=True, node_color=\"lightblue\") nx.draw_networkx_edge_labels(nxG, pos, edge_labels=nx.get_edge_attributes(nxG, \"weight\")) plt.show() In\u00a0[24]: Copied! <pre>from IPython.display import SVG, display\n\nfrom annnet.utils.plotting import plot\n\nobj = plot(\n    G,\n    backend=\"graphviz\",\n    layout=\"dot\",\n    use_weight_style=False,\n    show_edge_labels=True,\n    **{\n        \"graph_attr\": {\"bgcolor\": \"white\"},\n        \"node_attr\": {\"shape\": \"circle\", \"fixedsize\": \"true\"},\n        \"edge_attr\": {\"color\": \"black\", \"penwidth\": \"3.0\"},\n    },\n)\n\nsvg_data = obj.pipe(format=\"svg\").decode(\"utf-8\")\ndisplay(SVG(svg_data))\n</pre> from IPython.display import SVG, display  from annnet.utils.plotting import plot  obj = plot(     G,     backend=\"graphviz\",     layout=\"dot\",     use_weight_style=False,     show_edge_labels=True,     **{         \"graph_attr\": {\"bgcolor\": \"white\"},         \"node_attr\": {\"shape\": \"circle\", \"fixedsize\": \"true\"},         \"edge_attr\": {\"color\": \"black\", \"penwidth\": \"3.0\"},     }, )  svg_data = obj.pipe(format=\"svg\").decode(\"utf-8\") display(SVG(svg_data)) <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nFile ~\\anaconda3\\Lib\\site-packages\\graphviz\\backend\\execute.py:76, in run_check(cmd, input_lines, encoding, quiet, **kwargs)\n     75         kwargs['stdout'] = kwargs['stderr'] = subprocess.PIPE\n---&gt; 76     proc = _run_input_lines(cmd, input_lines, kwargs=kwargs)\n     77 else:\n\nFile ~\\anaconda3\\Lib\\site-packages\\graphviz\\backend\\execute.py:96, in _run_input_lines(cmd, input_lines, kwargs)\n     95 def _run_input_lines(cmd, input_lines, *, kwargs):\n---&gt; 96     popen = subprocess.Popen(cmd, stdin=subprocess.PIPE, **kwargs)\n     98     stdin_write = popen.stdin.write\n\nFile ~\\anaconda3\\Lib\\subprocess.py:1026, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\n   1023             self.stderr = io.TextIOWrapper(self.stderr,\n   1024                     encoding=encoding, errors=errors)\n-&gt; 1026     self._execute_child(args, executable, preexec_fn, close_fds,\n   1027                         pass_fds, cwd, env,\n   1028                         startupinfo, creationflags, shell,\n   1029                         p2cread, p2cwrite,\n   1030                         c2pread, c2pwrite,\n   1031                         errread, errwrite,\n   1032                         restore_signals,\n   1033                         gid, gids, uid, umask,\n   1034                         start_new_session, process_group)\n   1035 except:\n   1036     # Cleanup if the child failed starting.\n\nFile ~\\anaconda3\\Lib\\subprocess.py:1538, in Popen._execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\n   1537 try:\n-&gt; 1538     hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n   1539                              # no special security\n   1540                              None, None,\n   1541                              int(not close_fds),\n   1542                              creationflags,\n   1543                              env,\n   1544                              cwd,\n   1545                              startupinfo)\n   1546 finally:\n   1547     # Child is launched. Close the parent's copy of those pipe\n   1548     # handles that only the child should have open.  You need\n   (...)\n   1551     # pipe will not close when the child process exits and the\n   1552     # ReadFile will hang.\n\nFileNotFoundError: [WinError 2] Le fichier sp\u00e9cifi\u00e9 est introuvable\n\nThe above exception was the direct cause of the following exception:\n\nExecutableNotFound                        Traceback (most recent call last)\nCell In[24], line 18\n      3 from annnet.utils.plotting import plot\n      5 obj = plot(\n      6     G,\n      7     backend=\"graphviz\",\n   (...)\n     15     },\n     16 )\n---&gt; 18 svg_data = obj.pipe(format=\"svg\").decode(\"utf-8\")\n     19 display(SVG(svg_data))\n\nFile ~\\anaconda3\\Lib\\site-packages\\graphviz\\piping.py:104, in Pipe.pipe(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\n     55 def pipe(self,\n     56          format: typing.Optional[str] = None,\n     57          renderer: typing.Optional[str] = None,\n   (...)\n     61          engine: typing.Optional[str] = None,\n     62          encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]:\n     63     \"\"\"Return the source piped through the Graphviz layout command.\n     64 \n     65     Args:\n   (...)\n    102         '&lt;?xml version='\n    103     \"\"\"\n--&gt; 104     return self._pipe_legacy(format,\n    105                              renderer=renderer,\n    106                              formatter=formatter,\n    107                              neato_no_op=neato_no_op,\n    108                              quiet=quiet,\n    109                              engine=engine,\n    110                              encoding=encoding)\n\nFile ~\\anaconda3\\Lib\\site-packages\\graphviz\\_tools.py:185, in deprecate_positional_args.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n    177     wanted = ', '.join(f'{name}={value!r}'\n    178                        for name, value in deprecated.items())\n    179     warnings.warn(f'The signature of {func_name} will be reduced'\n    180                   f' to {supported_number} positional arg{s_}{qualification}'\n    181                   f' {list(supported)}: pass {wanted} as keyword arg{s_}',\n    182                   stacklevel=stacklevel,\n    183                   category=category)\n--&gt; 185 return func(*args, **kwargs)\n\nFile ~\\anaconda3\\Lib\\site-packages\\graphviz\\piping.py:121, in Pipe._pipe_legacy(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\n    112 @_tools.deprecate_positional_args(supported_number=1, ignore_arg='self')\n    113 def _pipe_legacy(self,\n    114                  format: typing.Optional[str] = None,\n   (...)\n    119                  engine: typing.Optional[str] = None,\n    120                  encoding: typing.Optional[str] = None) -&gt; typing.Union[bytes, str]:\n--&gt; 121     return self._pipe_future(format,\n    122                              renderer=renderer,\n    123                              formatter=formatter,\n    124                              neato_no_op=neato_no_op,\n    125                              quiet=quiet,\n    126                              engine=engine,\n    127                              encoding=encoding)\n\nFile ~\\anaconda3\\Lib\\site-packages\\graphviz\\piping.py:161, in Pipe._pipe_future(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\n    159     else:\n    160         return raw.decode(encoding)\n--&gt; 161 return self._pipe_lines(*args, input_encoding=self.encoding, **kwargs)\n\nFile ~\\anaconda3\\Lib\\site-packages\\graphviz\\backend\\piping.py:161, in pipe_lines(engine, format, input_lines, input_encoding, renderer, formatter, neato_no_op, quiet)\n    155 cmd = dot_command.command(engine, format,\n    156                           renderer=renderer,\n    157                           formatter=formatter,\n    158                           neato_no_op=neato_no_op)\n    159 kwargs = {'input_lines': (line.encode(input_encoding) for line in input_lines)}\n--&gt; 161 proc = execute.run_check(cmd, capture_output=True, quiet=quiet, **kwargs)\n    162 return proc.stdout\n\nFile ~\\anaconda3\\Lib\\site-packages\\graphviz\\backend\\execute.py:81, in run_check(cmd, input_lines, encoding, quiet, **kwargs)\n     79 except OSError as e:\n     80     if e.errno == errno.ENOENT:\n---&gt; 81         raise ExecutableNotFound(cmd) from e\n     82     raise\n     84 if not quiet and proc.stderr:\n\nExecutableNotFound: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"about/","title":"About","text":"<p>This project was created by Daniele Bottazzi.</p>"},{"location":"annnet_showcase/","title":"AnnNet Showcase \u2014 edges, interop, SBML, and I/O","text":"In\u00a0[2]: Copied! <pre>import annnet as an\n\nprint('annnet version:', an.__version__)\n</pre> import annnet as an  print('annnet version:', an.__version__) <pre>annnet version: 0.1.0\n</pre> In\u00a0[5]: Copied! <pre>G = an.Graph(directed=True)  # default direction; can be overridden per-edge\n\n# Add vertices with attributes\nG.add_vertices([\n    ('A', {'name': 'a'}),\n    ('B', {'name': 'b'}),\n    ('C', {'name': 'c'}),\n    ('D', {'name': 'd'}),\n])\n\n# Create layers and set active\nG.add_layer(\"toy\")\nG.add_layer(\"train\")\nG.add_layer(\"eval\")\nG.layers.active = \"toy\"   # same effect as set_active_layer(\"toy\")\n\n# Add vertices (with attributes) in 'toy'\nfor v in [\"A\",\"B\",\"C\",\"D\"]:\n    G.add_vertex(v, label=v, kind=\"gene\")\n\n# 1) Binary directed\ne_dir = G.add_edge(\"A\", \"B\", weight=2.0, edge_directed=True, relation=\"activates\")\n\n# 2) Binary undirected\ne_undir = G.add_edge(\"B\", \"C\", weight=1.0, edge_directed=False, relation=\"binds\")\n\n# 3) Self-loop\ne_loop = G.add_edge(\"D\", \"D\", weight=0.5, edge_directed=True, relation=\"self\")\n\n# 4) Parallel edge\ne_parallel = G.add_parallel_edge(\"A\", \"B\", weight=5.0, relation=\"alternative\")\n\n# 5) Vertex\u2013edge (hybrid) edges\nG.add_edge_entity(\"edge_e1\", description=\"signal\")\ne_vx = G.add_edge(\"edge_e1\", \"C\", edge_type=\"vertex_edge\", edge_directed=True, channel=\"edge-&gt;vertex\")\n\n# 6) Hyperedge (undirected, 3-way membership)\ne_hyper_undir = G.add_hyperedge(members=[\"A\",\"C\",\"D\"], weight=1.0, tag=\"complex\")\n\n# 7) Hyperedge (directed head\u2192tail)\ne_hyper_dir = G.add_hyperedge(head=[\"A\",\"B\"], tail=[\"C\",\"D\"], weight=1.0, reaction=\"A+B-&gt;C+D\")\n\nprint(\"Vertices:\", G.num_vertices, \"Edges:\", G.num_edges)\n</pre> G = an.Graph(directed=True)  # default direction; can be overridden per-edge  # Add vertices with attributes G.add_vertices([     ('A', {'name': 'a'}),     ('B', {'name': 'b'}),     ('C', {'name': 'c'}),     ('D', {'name': 'd'}), ])  # Create layers and set active G.add_layer(\"toy\") G.add_layer(\"train\") G.add_layer(\"eval\") G.layers.active = \"toy\"   # same effect as set_active_layer(\"toy\")  # Add vertices (with attributes) in 'toy' for v in [\"A\",\"B\",\"C\",\"D\"]:     G.add_vertex(v, label=v, kind=\"gene\")  # 1) Binary directed e_dir = G.add_edge(\"A\", \"B\", weight=2.0, edge_directed=True, relation=\"activates\")  # 2) Binary undirected e_undir = G.add_edge(\"B\", \"C\", weight=1.0, edge_directed=False, relation=\"binds\")  # 3) Self-loop e_loop = G.add_edge(\"D\", \"D\", weight=0.5, edge_directed=True, relation=\"self\")  # 4) Parallel edge e_parallel = G.add_parallel_edge(\"A\", \"B\", weight=5.0, relation=\"alternative\")  # 5) Vertex\u2013edge (hybrid) edges G.add_edge_entity(\"edge_e1\", description=\"signal\") e_vx = G.add_edge(\"edge_e1\", \"C\", edge_type=\"vertex_edge\", edge_directed=True, channel=\"edge-&gt;vertex\")  # 6) Hyperedge (undirected, 3-way membership) e_hyper_undir = G.add_hyperedge(members=[\"A\",\"C\",\"D\"], weight=1.0, tag=\"complex\")  # 7) Hyperedge (directed head\u2192tail) e_hyper_dir = G.add_hyperedge(head=[\"A\",\"B\"], tail=[\"C\",\"D\"], weight=1.0, reaction=\"A+B-&gt;C+D\")  print(\"Vertices:\", G.num_vertices, \"Edges:\", G.num_edges)  <pre>Vertices: 4 Edges: 7\n</pre> In\u00a0[8]: Copied! <pre># Sparse incidence matrix (SciPy)\nX = G.X\nprint(\"X shape:\", X, \"type:\", type(X).__name__)\n\n# Node attributes (Polars DF (DataFrame))\nobs = G.obs\nprint(\"obs schema:\", obs.schema)\ndisplay(obs.head(8))\n\n# Edge attributes (Polars DF (DataFrame))\nvar = G.var\nprint(\"var schema:\", var.schema)\ndisplay(var.head(10))\n\n# Layers manager\nprint(\"All layers:\", G.layers.list(include_default=True))\nprint(\"Active layer:\", G.layers.active)\n</pre> # Sparse incidence matrix (SciPy) X = G.X print(\"X shape:\", X, \"type:\", type(X).__name__)  # Node attributes (Polars DF (DataFrame)) obs = G.obs print(\"obs schema:\", obs.schema) display(obs.head(8))  # Edge attributes (Polars DF (DataFrame)) var = G.var print(\"var schema:\", var.schema) display(var.head(10))  # Layers manager print(\"All layers:\", G.layers.list(include_default=True)) print(\"Active layer:\", G.layers.active)  <pre>X shape: &lt;bound method Graph.X of &lt;annnet.core.graph.Graph object at 0x000001DFFF7B5B80&gt;&gt; type: method\nobs schema: Schema({'vertex_id': String, 'name': String, 'label': String, 'kind': String, 'description': String})\n</pre> shape: (5, 5)vertex_idnamelabelkinddescriptionstrstrstrstrstr\"A\"\"a\"\"A\"\"gene\"null\"B\"\"b\"\"B\"\"gene\"null\"C\"\"c\"\"C\"\"gene\"null\"D\"\"d\"\"D\"\"gene\"null\"edge_e1\"nullnullnull\"signal\" <pre>var schema: Schema({'edge_id': String, 'relation': String, 'channel': String, 'tag': String, 'reaction': String})\n</pre> shape: (7, 5)edge_idrelationchanneltagreactionstrstrstrstrstr\"edge_0\"\"activates\"nullnullnull\"edge_1\"\"binds\"nullnullnull\"edge_2\"\"self\"nullnullnull\"edge_3\"\"alternative\"nullnullnull\"edge_4\"null\"edge-&gt;vertex\"nullnull\"edge_5\"nullnull\"complex\"null\"edge_6\"nullnullnull\"A+B-&gt;C+D\" <pre>All layers: ['default', 'toy', 'train', 'eval']\nActive layer: toy\n</pre> In\u00a0[10]: Copied! <pre># Adjacency cache\n\nA = G.cache.adjacency\n\nA.toarray()\n</pre> # Adjacency cache  A = G.cache.adjacency  A.toarray() Out[10]: <pre>array([[ 31.  , -28.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n       [-28.  ,  31.  ,   0.  ,  -1.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n       [  0.  ,   0.  ,   4.  ,   2.  ,  -1.  ,   0.  ,   0.  ,   0.  ],\n       [  0.  ,  -1.  ,   2.  ,   2.25,   0.  ,   0.  ,   0.  ,   0.  ],\n       [  0.  ,   0.  ,  -1.  ,   0.  ,   1.  ,   0.  ,   0.  ,   0.  ],\n       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ]],\n      dtype=float32)</pre> In\u00a0[13]: Copied! <pre># Put different content in other layers\nG.layers.active = \"train\"\nG.add_vertex(\"X\", phase=\"train\")\nex = G.add_edge(\"X\", \"A\", weight=1.0, relation=\"train_edge\")\n\nG.layers.active = \"eval\"\nG.add_vertex(\"Y\", phase=\"eval\")\ney = G.add_edge(\"Y\", \"B\", weight=1.0, relation=\"eval_edge\")\n\n# Compute set operations (no new layers created)\nU = G.layer_union([\"train\",\"eval\"])          # {'vertices': set[str], 'edges': set[str]}\nI = G.layer_intersection([\"train\",\"eval\"])\nD = G.layer_difference(\"train\", \"eval\")\n\nprint(\"Union sizes:\", {k: len(v) for k,v in U.items()})\nprint(\"Intersection sizes:\", {k: len(v) for k,v in I.items()})\nprint(\"Train \\\\ Eval sizes:\", {k: len(v) for k,v in D.items()})\n</pre> # Put different content in other layers G.layers.active = \"train\" G.add_vertex(\"X\", phase=\"train\") ex = G.add_edge(\"X\", \"A\", weight=1.0, relation=\"train_edge\")  G.layers.active = \"eval\" G.add_vertex(\"Y\", phase=\"eval\") ey = G.add_edge(\"Y\", \"B\", weight=1.0, relation=\"eval_edge\")  # Compute set operations (no new layers created) U = G.layer_union([\"train\",\"eval\"])          # {'vertices': set[str], 'edges': set[str]} I = G.layer_intersection([\"train\",\"eval\"]) D = G.layer_difference(\"train\", \"eval\")  print(\"Union sizes:\", {k: len(v) for k,v in U.items()}) print(\"Intersection sizes:\", {k: len(v) for k,v in I.items()}) print(\"Train \\\\ Eval sizes:\", {k: len(v) for k,v in D.items()})  <pre>Union sizes: {'vertices': 4, 'edges': 2}\nIntersection sizes: {'vertices': 0, 'edges': 0}\nTrain \\ Eval sizes: {'vertices': 2, 'edges': 1}\n</pre> In\u00a0[16]: Copied! <pre>ev = G.edges_view(include_directed=True, include_weight=True, resolved_weight=True)\nev\n</pre> ev = G.edges_view(include_directed=True, include_weight=True, resolved_weight=True) ev Out[16]: shape: (9, 15)edge_idkinddirectedglobal_weightsourcetargetedge_typeheadtailmembersrelationchanneltagreactioneffective_weightstrstrboolf64strstrstrlist[str]list[str]list[str]strstrstrstrf64\"edge_0\"\"binary\"true2.0\"A\"\"B\"\"regular\"nullnullnull\"activates\"nullnullnull2.0\"edge_1\"\"binary\"false1.0\"B\"\"C\"\"regular\"nullnullnull\"binds\"nullnullnull1.0\"edge_2\"\"binary\"true0.5\"D\"\"D\"\"regular\"nullnullnull\"self\"nullnullnull0.5\"edge_3\"\"binary\"true5.0\"A\"\"B\"\"regular\"nullnullnull\"alternative\"nullnullnull5.0\"edge_4\"\"binary\"true1.0\"edge_e1\"\"C\"\"vertex_edge\"nullnullnullnull\"edge-&gt;vertex\"nullnull1.0\"edge_5\"\"hyper\"false1.0nullnullnullnullnull[\"A\", \"C\", \"D\"]nullnull\"complex\"null1.0\"edge_6\"\"hyper\"true1.0nullnullnull[\"A\", \"B\"][\"C\", \"D\"]nullnullnullnull\"A+B-&gt;C+D\"1.0\"edge_7\"\"binary\"true1.0\"X\"\"A\"\"regular\"nullnullnull\"train_edge\"nullnullnull1.0\"edge_8\"\"binary\"true1.0\"Y\"\"B\"\"regular\"nullnullnull\"eval_edge\"nullnullnull1.0 In\u00a0[23]: Copied! <pre>import matplotlib.pyplot as plt\nimport networkx as nx\n\n# Obtain a simple NX view (collapse Multi* edges with sensible aggregations)\nnxG, manifest = an.to_nx(G, directed=True, hyperedge_mode=\"skip\") # skip, expand or reify\npos = nx.spring_layout(nxG, seed=42)\nplt.figure(figsize=(6,4))\nnx.draw(nxG, pos, with_labels=True, node_size=800)\nnx.draw_networkx_edge_labels(nxG, pos, edge_labels=nx.get_edge_attributes(nxG, 'weight'))\nplt.title('Demo graph (simple NX view)')\nplt.show()\n</pre> import matplotlib.pyplot as plt import networkx as nx  # Obtain a simple NX view (collapse Multi* edges with sensible aggregations) nxG, manifest = an.to_nx(G, directed=True, hyperedge_mode=\"skip\") # skip, expand or reify pos = nx.spring_layout(nxG, seed=42) plt.figure(figsize=(6,4)) nx.draw(nxG, pos, with_labels=True, node_size=800) nx.draw_networkx_edge_labels(nxG, pos, edge_labels=nx.get_edge_attributes(nxG, 'weight')) plt.title('Demo graph (simple NX view)') plt.show() In\u00a0[26]: Copied! <pre># Shortest path over weights (collapse parallel edges first)\nsp_len = G.nx.shortest_path_length(G, source='A', target='D', weight='weight', _nx_simple=True)\nsp = G.nx.shortest_path(G, source='A', target='D', weight='weight', _nx_simple=True)\nprint('Shortest path length A\u2192D:', sp_len)\nprint('Shortest path A\u2192D:', sp)\n\n# Betweenness centrality (weighted)\nbc = G.nx.betweenness_centrality(G, weight='weight', _nx_simple=True)\nbc\n</pre> # Shortest path over weights (collapse parallel edges first) sp_len = G.nx.shortest_path_length(G, source='A', target='D', weight='weight', _nx_simple=True) sp = G.nx.shortest_path(G, source='A', target='D', weight='weight', _nx_simple=True) print('Shortest path length A\u2192D:', sp_len) print('Shortest path A\u2192D:', sp)  # Betweenness centrality (weighted) bc = G.nx.betweenness_centrality(G, weight='weight', _nx_simple=True) bc <pre>Shortest path length A\u2192D: 1.0\nShortest path A\u2192D: ['A', 'D']\n</pre> <pre>C:\\Users\\pc\\anaconda3\\Lib\\site-packages\\annnet\\core\\graph.py:6857: RuntimeWarning: Graph\u2192NX conversion is lossy: multiple layers flattened into single NX graph.\n  nxG = self._convert_to_nx(\n</pre> Out[26]: <pre>{'A': 0.1,\n 'B': 0.1,\n 'C': 0.2,\n 'D': 0.03333333333333333,\n 'X': 0.0,\n 'Y': 0.0,\n 'edge_e1': 0.0}</pre> In\u00a0[28]: Copied! <pre># Community detection using Label Propagation (works on undirected views)\ncomms = list(G.nx.asyn_lpa_communities(G, weight='weight', _nx_directed=False, _nx_simple=True))\ncomms\n</pre> # Community detection using Label Propagation (works on undirected views) comms = list(G.nx.asyn_lpa_communities(G, weight='weight', _nx_directed=False, _nx_simple=True)) comms Out[28]: <pre>[{'A', 'B', 'C', 'D', 'X', 'Y', 'edge_e1'}]</pre> In\u00a0[33]: Copied! <pre># Top-level adapter function (returns NX graph; may also return a manifest)\nfrom pathlib import Path\n\nnx_out = an.to_nx(G, directed=True, hyperedge_mode='expand')\nnxG2 = nx_out[0] if isinstance(nx_out, tuple) else nx_out\nassert isinstance(nxG2, (nx.Graph, nx.DiGraph, nx.MultiGraph, nx.MultiDiGraph))\n\n# Optional: export to GraphML and SIF for demonstration\nassets = Path('showcase'); assets.mkdir(exist_ok=True, parents=True)\nan.to_graphml(G, assets/'demo.graphml')\nan.to_sif(G, assets/'demo.sif')\nstr(assets.resolve())\n</pre> # Top-level adapter function (returns NX graph; may also return a manifest) from pathlib import Path  nx_out = an.to_nx(G, directed=True, hyperedge_mode='expand') nxG2 = nx_out[0] if isinstance(nx_out, tuple) else nx_out assert isinstance(nxG2, (nx.Graph, nx.DiGraph, nx.MultiGraph, nx.MultiDiGraph))  # Optional: export to GraphML and SIF for demonstration assets = Path('showcase'); assets.mkdir(exist_ok=True, parents=True) an.to_graphml(G, assets/'demo.graphml') an.to_sif(G, assets/'demo.sif') str(assets.resolve()) Out[33]: <pre>'C:\\\\Users\\\\pc\\\\_VIXYN\\\\TST\\\\showcase'</pre> In\u00a0[36]: Copied! <pre># Path relative to this notebook (the repo includes docs/Elowitz.sbml.xml)\nsbml_path = 'Elowitz.sbml.xml'\nprint('SBML file:', sbml_path)\nR = an.from_sbml(str(sbml_path)) if sbml_path else None\nR\n</pre> # Path relative to this notebook (the repo includes docs/Elowitz.sbml.xml) sbml_path = 'Elowitz.sbml.xml' print('SBML file:', sbml_path) R = an.from_sbml(str(sbml_path)) if sbml_path else None R <pre>SBML file: Elowitz.sbml.xml\n</pre> <pre>Model does not contain SBML fbc package information.\nSBML package 'layout' not supported by cobrapy, information is not parsed\nSBML package 'render' not supported by cobrapy, information is not parsed\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction1 \"degradation of LacI transcripts\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction1 \"degradation of LacI transcripts\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction2 \"degradation of TetR transcripts\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction2 \"degradation of TetR transcripts\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction3 \"degradation of CI transcripts\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction3 \"degradation of CI transcripts\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction4 \"translation of LacI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction4 \"translation of LacI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction5 \"translation of TetR\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction5 \"translation of TetR\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction6 \"translation of CI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction6 \"translation of CI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction7 \"degradation of LacI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction7 \"degradation of LacI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction8 \"degradation of TetR\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction8 \"degradation of TetR\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction9 \"degradation of CI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction9 \"degradation of CI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction10 \"transcription of LacI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction10 \"transcription of LacI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction11 \"transcription of TetR\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction11 \"transcription of TetR\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction12 \"transcription of CI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction12 \"transcription of CI\"&gt;'\nNo objective coefficients in model. Unclear what should be optimized\nMissing flux bounds on reactions set to default bounds.As best practise and to avoid confusion flux bounds should be set explicitly on all reactions.\n</pre> Out[36]: <pre>&lt;annnet.core.graph.Graph at 0x1dfc1493ec0&gt;</pre> In\u00a0[38]: Copied! <pre>if R is not None:\n    print('Repressilator \u2014 vertices:', R.num_vertices, 'edges:', R.num_edges)\n    R.edges_view().head(10)\n</pre> if R is not None:     print('Repressilator \u2014 vertices:', R.num_vertices, 'edges:', R.num_edges)     R.edges_view().head(10) <pre>Repressilator \u2014 vertices: 8 edges: 12\n</pre> In\u00a0[41]: Copied! <pre>out_dir = Path('showcase'); out_dir.mkdir(exist_ok=True, parents=True)\ndemo_path = out_dir/'demo.annnet'\nsbml_path_out = out_dir/'repressilator.annnet'\n\nG.write(demo_path, overwrite=True)  # lossless save\nprint('Wrote:', demo_path)\n\n# Round-trip check\nG2 = an.Graph.read(demo_path)\nprint('Round-trip OK?', (G2.num_vertices, G2.num_edges) == (G.num_vertices, G.num_edges))\n\nif R is not None:\n    R.write(sbml_path_out, overwrite=True)\n    print('Saved repressilator to:', sbml_path_out)\n</pre> out_dir = Path('showcase'); out_dir.mkdir(exist_ok=True, parents=True) demo_path = out_dir/'demo.annnet' sbml_path_out = out_dir/'repressilator.annnet'  G.write(demo_path, overwrite=True)  # lossless save print('Wrote:', demo_path)  # Round-trip check G2 = an.Graph.read(demo_path) print('Round-trip OK?', (G2.num_vertices, G2.num_edges) == (G.num_vertices, G.num_edges))  if R is not None:     R.write(sbml_path_out, overwrite=True)     print('Saved repressilator to:', sbml_path_out) <pre>Wrote: showcase\\demo.annnet\nRound-trip OK? True\nSaved repressilator to: showcase\\repressilator.annnet\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"annnet_showcase/#annnet-showcase-edges-interop-sbml-and-annnet-io","title":"AnnNet Showcase \u2014 edges, interop, SBML, and <code>.annnet</code> I/O\u00b6","text":"<p>This notebook demonstrates:</p> <ul> <li>constructing a graph with all edge types supported (binary directed/undirected, self-loops, parallel edges, vertex\u2013edge links, and k\u2011ary hyperedges),</li> <li>quick inspection &amp; visualization,</li> <li>running algorithms in NetworkX (NX) via the lazy proxy,</li> <li>importing a repressilator (Elowitz) model from SBML (Systems Biology Markup Language),</li> <li>and saving/loading the lossless <code>.annnet</code> format.</li> </ul> <p>Tip: the <code>G.nx.&lt;algo&gt;(G, ...)</code> proxy builds a cached NX view of <code>G</code> on\u2011demand, and supports knobs like <code>_nx_simple=True</code> (collapse parallel edges) and <code>_nx_directed=False</code> (undirected view).</p>"},{"location":"annnet_showcase/#1-build-a-demo-graph-with-all-edge-types-layers-and-metadata-anndata-like-api","title":"1) Build a demo graph with all edge types, layers and metadata + AnnData like API\u00b6","text":""},{"location":"annnet_showcase/#anndata-style-access-x-obs-var-layers","title":"AnnData-style access: <code>.X</code>, <code>.obs</code>, <code>.var</code>, <code>.layers</code>\u00b6","text":""},{"location":"annnet_showcase/#layers-api-application-programming-interface-demo-attach-set-operations","title":"Layers API (application programming interface) demo \u2014 attach + set operations\u00b6","text":""},{"location":"annnet_showcase/#inspect-edges-structured-view","title":"Inspect edges (structured view)\u00b6","text":""},{"location":"annnet_showcase/#quick-visualization-via-nx-networkx","title":"Quick visualization via NX (NetworkX)\u00b6","text":""},{"location":"annnet_showcase/#2-run-a-few-nx-networkx-algorithms-via-the-proxy","title":"2) Run a few NX (NetworkX) algorithms via the proxy\u00b6","text":""},{"location":"annnet_showcase/#3-interoperability-adapters","title":"3) Interoperability (adapters)\u00b6","text":""},{"location":"annnet_showcase/#4-import-repressilator-elowitz-from-sbml","title":"4) Import repressilator (Elowitz) from SBML\u00b6","text":""},{"location":"annnet_showcase/#5-save-load-the-lossless-annnet-format","title":"5) Save &amp; load the lossless <code>.annnet</code> format\u00b6","text":""},{"location":"installation/","title":"Installation guide","text":"<p>We strongly recommend installing a few prerequisites to ensure a smooth experience. These prerequisites are:</p> <ol> <li>Python 3 (version &gt;= 3.10)<ul> <li>Install Python 3</li> </ul> </li> <li>Poetry (Python packaging and dependency manager)<ul> <li>Install Poetry</li> </ul> </li> <li>git (version control manager)<ul> <li>Install git</li> </ul> </li> <li>Docker (containerization technology) [optional]<ul> <li>Install Docker</li> </ul> </li> </ol> <p>Tip</p> <p>If you are missing any of those pre-requisites, please follow the installation guide in each resource before you continue.</p>"},{"location":"installation/#checking-prerequisites","title":"Checking prerequisites","text":"<p>You can verify access to these components in your terminal:</p> <ol> <li><code>Python</code> version 3.10 or higher.    <pre><code>python --version\n</code></pre></li> <li><code>Poetry</code> <pre><code>poetry --version\n</code></pre></li> <li><code>git</code> <pre><code>git --version\n</code></pre></li> <li><code>Docker</code> <pre><code>docker --version\n</code></pre></li> </ol>"},{"location":"io_annnet/","title":"AnnNet Zero-Loss Serialization: Zarr + Parquet","text":""},{"location":"io_annnet/#design-goals","title":"Design Goals","text":"<ol> <li>Zero topology loss: Preserve exact incidence matrix, all edge types, hyperedges, parallel edges</li> <li>Complete metadata: All attributes, layers, history, provenance</li> <li>Cross-platform: Works on Windows/Linux/Mac, Python/R/Julia</li> <li>Incremental updates: Can append without full rewrite</li> <li>Cloud support: S3/GCS/Azure compatible via Zarr</li> <li>Fast random access: Chunked storage for large graphs</li> </ol>"},{"location":"io_annnet/#file-structure","title":"File Structure","text":"<pre><code>graph.annnet/\n\u251c\u2500\u2500 manifest.json                 # root descriptor (format, counts, compression, etc.)\n\u251c\u2500\u2500 structure/\n\u2502   \u251c\u2500\u2500 incidence.zarr/           # Zarr v3 group holding COO [coordinate list] arrays\n\u2502   \u2502   \u251c\u2500\u2500 zarr.json             # Zarr v3 group metadata (includes group attributes)\n\u2502   \u2502   \u251c\u2500\u2500 row/                  # Zarr array (int32) of entity indices (COO row)\n\u2502   \u2502   \u251c\u2500\u2500 col/                  # Zarr array (int32) of edge indices   (COO col)\n\u2502   \u2502   \u2514\u2500\u2500 data/                 # Zarr array (float32) of weights       (COO data)\n\u2502   \u2502   # group attributes include: {\"shape\": [n_entities, n_edges]}\n\u2502   \u251c\u2500\u2500 entity_to_idx.parquet     # entity_id \u2192 row index\n\u2502   \u251c\u2500\u2500 idx_to_entity.parquet     # row index \u2192 entity_id\n\u2502   \u251c\u2500\u2500 entity_types.parquet      # entity_id \u2192 \"vertex\" | \"edge\"\n\u2502   \u251c\u2500\u2500 edge_to_idx.parquet       # edge_id \u2192 column index\n\u2502   \u251c\u2500\u2500 idx_to_edge.parquet       # column index \u2192 edge_id\n\u2502   \u251c\u2500\u2500 edge_definitions.parquet  # edge_id \u2192 (source, target, edge_type) for simple edges\n\u2502   \u251c\u2500\u2500 edge_weights.parquet      # edge_id \u2192 weight\n\u2502   \u251c\u2500\u2500 edge_directed.parquet     # edge_id \u2192 bool | null\n\u2502   \u251c\u2500\u2500 edge_kind.parquet         # edge_id \u2192 \"binary\" | \"hyper\"\n\u2502   \u2514\u2500\u2500 hyperedge_definitions.parquet\n\u2502       # columns: edge_id, directed(bool), members(List[Utf8]) OR head(List[Utf8]), tail(List[Utf8])\n\u2502\n\u251c\u2500\u2500 tables/\n\u2502   \u251c\u2500\u2500 vertex_attributes.parquet     # vertex-level DF [dataframe]\n\u2502   \u251c\u2500\u2500 edge_attributes.parquet       # edge-level DF\n\u2502   \u251c\u2500\u2500 layer_attributes.parquet      # layer metadata\n\u2502   \u2514\u2500\u2500 edge_layer_attributes.parquet # (layer_id, edge_id, weight)\n\u2502\n\u251c\u2500\u2500 layers/\n\u2502   \u251c\u2500\u2500 registry.parquet              # layer_id, name, metadata\u2026\n\u2502   \u251c\u2500\u2500 vertex_memberships.parquet    # (layer_id, vertex_id)\n\u2502   \u2514\u2500\u2500 edge_memberships.parquet      # (layer_id, edge_id, weight)\n\u2502\n\u251c\u2500\u2500 cache/                            # optional materialized views\n\u2502   \u251c\u2500\u2500 csr.zarr/                     # CSR [compressed sparse row] cache\n\u2502   \u2514\u2500\u2500 csc.zarr/                     # CSC [compressed sparse column] cache\n\u2502\n\u251c\u2500\u2500 audit/\n\u2502   \u251c\u2500\u2500 history.parquet               # operation log (nested payloads stringified to JSON [JavaScript Object Notation])\n\u2502   \u251c\u2500\u2500 snapshots/                    # optional labeled snapshots\n\u2502   \u2514\u2500\u2500 provenance.json               # creation time, software versions, etc.\n\u2502\n\u2514\u2500\u2500 uns/                              # unstructured metadata &amp; results\n    \u251c\u2500\u2500 graph_attributes.json\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"io_annnet/#manifest-schema-manifestjson","title":"Manifest Schema (<code>manifest.json</code>)","text":"<pre><code>{\n  \"format\": \"annnet\",\n  \"version\": \"1.0.0\",\n  \"created\": \"2025-10-23T10:30:00Z\",\n  \"annnet_version\": \"0.1.0\",\n  \"graph_version\": 42,\n  \"directed\": true,\n  \"counts\": {\n    \"vertices\": 1000,\n    \"edges\": 5000,\n    \"entities\": 1050,\n    \"layers\": 3,\n    \"hyperedges\": 50\n  },\n  \"layers\": [\"default\", \"temporal_2023\", \"temporal_2024\"],\n  \"active_layer\": \"default\",\n  \"default_layer\": \"default\",\n  \"schema_version\": \"1.0\",\n  \"checksum\": \"sha256:abcdef...\",\n  \"compression\": \"zstd\",\n  \"encoding\": {\n    \"zarr\": \"v3\",\n    \"parquet\": \"2.0\"\n  }\n}\n</code></pre>"},{"location":"io_annnet/#advantages","title":"Advantages","text":"<ol> <li>Zero loss: topology + metadata round-trip exactly</li> <li>Portable: Parquet/Zarr are first-class in Python/R/Julia</li> <li>Incremental: replace just the parts you touched</li> <li>Cloud-native: Zarr stores are compatible with S3/GCS/Azure</li> <li>Interoperable: PaParquet works with Pandas/DuckDB/Arrow ecosystems</li> <li>Compressed: zstd/lz4 where supported</li> <li>Chunked: fast random access on large graphs</li> <li>Schema evolution: add new tables without breaking old readers</li> </ol>"},{"location":"ppi_example/","title":"Ppi example","text":"In\u00a0[1]: Copied! <pre>import sys\nimport os\nsys.path.insert(0, os.path.abspath(\"..\"))\nfrom annnet.core.graph import Graph\n</pre> import sys import os sys.path.insert(0, os.path.abspath(\"..\")) from annnet.core.graph import Graph In\u00a0[3]: Copied! <pre># ---------- Setup ----------\nG = Graph(directed=True)\nconditions = [\"Healthy\", \"Stressed\", \"Disease\"]\nfor c in conditions:\n    G.add_slice(c, condition=c)\n\n# Entities\nproteins = [f\"P{i}\" for i in range(1, 151)]  # P1..P150\ntranscripts = [f\"T{i}\" for i in range(1, 61)]  # T1..T60 (treat as vertices)\nenz_edge_entities = [f\"edge_rxn_{i}\" for i in range(1, 11)]  # edge-entities for reactions\n\n# Seed some vertex attributes\nfor p in proteins[:10]:\n    G.add_vertex(p, slice=\"Healthy\", family=\"kinase\")\nfor p in proteins[10:]:\n    G.add_vertex(p, slice=\"Healthy\")\nfor t in transcripts:\n    G.add_vertex(t, slice=\"Healthy\", kind=\"transcript\")\nfor ee in enz_edge_entities:\n    G.add_edge_entity(ee, slice=\"Healthy\", role=\"enzyme\")\n\n# Propagate initial vertices to all slices (cheaply)\nfor lid in [\"Stressed\", \"Disease\"]:\n    G._slices[lid][\"vertices\"].update(G._slices[\"Healthy\"][\"vertices\"])\n</pre> # ---------- Setup ---------- G = Graph(directed=True) conditions = [\"Healthy\", \"Stressed\", \"Disease\"] for c in conditions:     G.add_slice(c, condition=c)  # Entities proteins = [f\"P{i}\" for i in range(1, 151)]  # P1..P150 transcripts = [f\"T{i}\" for i in range(1, 61)]  # T1..T60 (treat as vertices) enz_edge_entities = [f\"edge_rxn_{i}\" for i in range(1, 11)]  # edge-entities for reactions  # Seed some vertex attributes for p in proteins[:10]:     G.add_vertex(p, slice=\"Healthy\", family=\"kinase\") for p in proteins[10:]:     G.add_vertex(p, slice=\"Healthy\") for t in transcripts:     G.add_vertex(t, slice=\"Healthy\", kind=\"transcript\") for ee in enz_edge_entities:     G.add_edge_entity(ee, slice=\"Healthy\", role=\"enzyme\")  # Propagate initial vertices to all slices (cheaply) for lid in [\"Stressed\", \"Disease\"]:     G._slices[lid][\"vertices\"].update(G._slices[\"Healthy\"][\"vertices\"]) In\u00a0[5]: Copied! <pre># ---------- Build PPI edges in all slices ----------\nimport random\n\n\ndef rand_weight(base=1.0, jitter=0.5):\n    return max(0.05, base + (random.random() - 0.5) * 2 * jitter)\n\n\nppis = []\nfor _ in range(320):\n    u, v = random.sample(proteins, 2)\n    w = rand_weight(1.2, 0.6)\n    e = G.add_edge(u, v, slice=\"Healthy\", weight=w, edge_directed=False)\n    ppis.append(e)\n\n# Stress/disease slice variants (override per-slice weights)\nfor eid in ppis:\n    # Stressed: mostly +10% with jitter\n    G.add_edge_to_slice(\"Stressed\", eid)\n    G.set_edge_slice_attrs(\"Stressed\", eid, weight=G.edge_weights[eid] * rand_weight(1.10, 0.1))\n    # Disease: some edges get weaker; others stronger\n    G.add_edge_to_slice(\"Disease\", eid)\n    factor = 0.7 if random.random() &lt; 0.4 else 1.3\n    G.set_edge_slice_attrs(\"Disease\", eid, weight=G.edge_weights[eid] * rand_weight(factor, 0.15))\n</pre> # ---------- Build PPI edges in all slices ---------- import random   def rand_weight(base=1.0, jitter=0.5):     return max(0.05, base + (random.random() - 0.5) * 2 * jitter)   ppis = [] for _ in range(320):     u, v = random.sample(proteins, 2)     w = rand_weight(1.2, 0.6)     e = G.add_edge(u, v, slice=\"Healthy\", weight=w, edge_directed=False)     ppis.append(e)  # Stress/disease slice variants (override per-slice weights) for eid in ppis:     # Stressed: mostly +10% with jitter     G.add_edge_to_slice(\"Stressed\", eid)     G.set_edge_slice_attrs(\"Stressed\", eid, weight=G.edge_weights[eid] * rand_weight(1.10, 0.1))     # Disease: some edges get weaker; others stronger     G.add_edge_to_slice(\"Disease\", eid)     factor = 0.7 if random.random() &lt; 0.4 else 1.3     G.set_edge_slice_attrs(\"Disease\", eid, weight=G.edge_weights[eid] * rand_weight(factor, 0.15)) In\u00a0[7]: Copied! <pre># ---------- Complexes as undirected hyperedges ----------\ncomplexes = []\nfor _ in range(12):\n    members = set(random.sample(proteins, random.choice([3, 4, 5])))\n    hid = G.add_hyperedge(members=members, slice=\"Healthy\", weight=rand_weight(1.0, 0.2))\n    complexes.append(hid)\n    # complex exists in all slices (same membership)\n    for lid in [\"Stressed\", \"Disease\"]:\n        G.add_edge_to_slice(lid, hid)\n</pre> # ---------- Complexes as undirected hyperedges ---------- complexes = [] for _ in range(12):     members = set(random.sample(proteins, random.choice([3, 4, 5])))     hid = G.add_hyperedge(members=members, slice=\"Healthy\", weight=rand_weight(1.0, 0.2))     complexes.append(hid)     # complex exists in all slices (same membership)     for lid in [\"Stressed\", \"Disease\"]:         G.add_edge_to_slice(lid, hid) In\u00a0[9]: Copied! <pre># ---------- Directed signaling cascades as hyperedges ----------\ncascades = []\nwhile len(cascades) &lt; 8:\n    head = set(random.sample(proteins, random.choice([1, 2])))\n    tail = set(random.sample(proteins, random.choice([2, 3, 4])))\n    if head &amp; tail:\n        continue  # resample until disjoint\n    hid = G.add_hyperedge(head=head, tail=tail, slice=\"Healthy\", weight=rand_weight(1.0, 0.4))\n    cascades.append(hid)\n    for lid in [\"Stressed\", \"Disease\"]:\n        G.add_edge_to_slice(lid, hid)\n</pre> # ---------- Directed signaling cascades as hyperedges ---------- cascades = [] while len(cascades) &lt; 8:     head = set(random.sample(proteins, random.choice([1, 2])))     tail = set(random.sample(proteins, random.choice([2, 3, 4])))     if head &amp; tail:         continue  # resample until disjoint     hid = G.add_hyperedge(head=head, tail=tail, slice=\"Healthy\", weight=rand_weight(1.0, 0.4))     cascades.append(hid)     for lid in [\"Stressed\", \"Disease\"]:         G.add_edge_to_slice(lid, hid) In\u00a0[11]: Copied! <pre># ---------- Reactions connecting vertices to edge-entities ----------\nfor ee in enz_edge_entities:\n    s, t = random.sample(proteins, 2)\n    G.add_edge(s, ee, slice=\"Healthy\", edge_type=\"vertex_edge\", weight=1.0 + random.random())\n    G.add_edge(ee, t, slice=\"Healthy\", edge_type=\"vertex_edge\", weight=1.0 + random.random())\n    # propagate across slices\n    for lid in [\"Stressed\", \"Disease\"]:\n        G._slices[lid][\"edges\"].update(G._slices[\"Healthy\"][\"edges\"])\n</pre> # ---------- Reactions connecting vertices to edge-entities ---------- for ee in enz_edge_entities:     s, t = random.sample(proteins, 2)     G.add_edge(s, ee, slice=\"Healthy\", edge_type=\"vertex_edge\", weight=1.0 + random.random())     G.add_edge(ee, t, slice=\"Healthy\", edge_type=\"vertex_edge\", weight=1.0 + random.random())     # propagate across slices     for lid in [\"Stressed\", \"Disease\"]:         G._slices[lid][\"edges\"].update(G._slices[\"Healthy\"][\"edges\"]) In\u00a0[13]: Copied! <pre># ---------- Basic sanity ----------\nprint(\"vertices:\", G.number_of_vertices(), \"Edges:\", G.number_of_edges())\n\n# Only true \"vertices\" are counted by number_of_vertices() (proteins + transcripts)\nexpected_vertices = len(set(proteins)) + len(set(transcripts))  # 150 + 60 = 210\nassert G.number_of_vertices() &gt;= expected_vertices, (\n    f\"Expected \u2265{expected_vertices}, got {G.number_of_vertices()}\"\n)\n\n# Edge-entities are tracked as entity_type == 'edge' (not included in number_of_vertices)\nedge_entity_ids = set(enz_edge_entities)\nedge_entity_count = sum(\n    1 for _id, et in G.entity_types.items() if et == \"edge\" and _id in edge_entity_ids\n)\nassert edge_entity_count == len(edge_entity_ids), (\n    f\"Expected {len(edge_entity_ids)} edge-entities, got {edge_entity_count}\"\n)\n\n# Edges: PPIs (320) + complexes (12) + cascades (8) + reaction links (10*2) = 360 minimum\nassert G.number_of_edges() &gt;= 320 + 12 + 8 + (10 * 2)\n</pre> # ---------- Basic sanity ---------- print(\"vertices:\", G.number_of_vertices(), \"Edges:\", G.number_of_edges())  # Only true \"vertices\" are counted by number_of_vertices() (proteins + transcripts) expected_vertices = len(set(proteins)) + len(set(transcripts))  # 150 + 60 = 210 assert G.number_of_vertices() &gt;= expected_vertices, (     f\"Expected \u2265{expected_vertices}, got {G.number_of_vertices()}\" )  # Edge-entities are tracked as entity_type == 'edge' (not included in number_of_vertices) edge_entity_ids = set(enz_edge_entities) edge_entity_count = sum(     1 for _id, et in G.entity_types.items() if et == \"edge\" and _id in edge_entity_ids ) assert edge_entity_count == len(edge_entity_ids), (     f\"Expected {len(edge_entity_ids)} edge-entities, got {edge_entity_count}\" )  # Edges: PPIs (320) + complexes (12) + cascades (8) + reaction links (10*2) = 360 minimum assert G.number_of_edges() &gt;= 320 + 12 + 8 + (10 * 2) <pre>vertices: 210 Edges: 360\n</pre> In\u00a0[15]: Copied! <pre># ---------- Views &amp; top edges by condition ----------\nimport polars as pl\n\nfor cond in conditions:\n    EV = G.edges_view(slice=cond, resolved_weight=True)\n    print(f\"[{cond}] edges_view rows =\", EV.height)\n    top = (\n        EV.filter(pl.col(\"kind\") == \"binary\")\n        .sort(\"effective_weight\", descending=True)\n        .select([\"edge_id\", \"source\", \"target\", \"effective_weight\"])\n        .head(5)\n    )\n    print(f\"\\nTop 5 binary edges by effective_weight in {cond}:\")\n    print(top)\n</pre> # ---------- Views &amp; top edges by condition ---------- import polars as pl  for cond in conditions:     EV = G.edges_view(slice=cond, resolved_weight=True)     print(f\"[{cond}] edges_view rows =\", EV.height)     top = (         EV.filter(pl.col(\"kind\") == \"binary\")         .sort(\"effective_weight\", descending=True)         .select([\"edge_id\", \"source\", \"target\", \"effective_weight\"])         .head(5)     )     print(f\"\\nTop 5 binary edges by effective_weight in {cond}:\")     print(top) <pre>[Healthy] edges_view rows = 360\n\nTop 5 binary edges by effective_weight in Healthy:\nshape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id  \u2506 source     \u2506 target     \u2506 effective_weight \u2502\n\u2502 ---      \u2506 ---        \u2506 ---        \u2506 ---              \u2502\n\u2502 str      \u2506 str        \u2506 str        \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_344 \u2506 P66        \u2506 edge_rxn_3 \u2506 1.973542         \u2502\n\u2502 edge_345 \u2506 edge_rxn_3 \u2506 P85        \u2506 1.918338         \u2502\n\u2502 edge_39  \u2506 P8         \u2506 P97        \u2506 1.797781         \u2502\n\u2502 edge_56  \u2506 P102       \u2506 P29        \u2506 1.794341         \u2502\n\u2502 edge_159 \u2506 P110       \u2506 P30        \u2506 1.785787         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[Stressed] edges_view rows = 360\n\nTop 5 binary edges by effective_weight in Stressed:\nshape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id  \u2506 source \u2506 target \u2506 effective_weight \u2502\n\u2502 ---      \u2506 ---    \u2506 ---    \u2506 ---              \u2502\n\u2502 str      \u2506 str    \u2506 str    \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_56  \u2506 P102   \u2506 P29    \u2506 2.139637         \u2502\n\u2502 edge_41  \u2506 P16    \u2506 P126   \u2506 2.098382         \u2502\n\u2502 edge_159 \u2506 P110   \u2506 P30    \u2506 2.072207         \u2502\n\u2502 edge_198 \u2506 P102   \u2506 P136   \u2506 2.031831         \u2502\n\u2502 edge_251 \u2506 P77    \u2506 P91    \u2506 2.025662         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[Disease] edges_view rows = 360\n\nTop 5 binary edges by effective_weight in Disease:\nshape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id  \u2506 source \u2506 target \u2506 effective_weight \u2502\n\u2502 ---      \u2506 ---    \u2506 ---    \u2506 ---              \u2502\n\u2502 str      \u2506 str    \u2506 str    \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_242 \u2506 P118   \u2506 P137   \u2506 2.506427         \u2502\n\u2502 edge_117 \u2506 P142   \u2506 P135   \u2506 2.489469         \u2502\n\u2502 edge_243 \u2506 P135   \u2506 P81    \u2506 2.448115         \u2502\n\u2502 edge_84  \u2506 P98    \u2506 P49    \u2506 2.391741         \u2502\n\u2502 edge_155 \u2506 P149   \u2506 P134   \u2506 2.368377         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[16]: Copied! <pre># ---------- slice analytics ----------\nstats = G.slice_statistics()\nprint(\"\\nslice stats:\", stats)\n\nconserved = G.conserved_edges(min_slices=3)  # present in all 3 conditions\nprint(\"\\nConserved edges (in all conditions):\", len(conserved))\n\ndisease_specific = G.slice_specific_edges(\"Disease\")\nprint(\"Disease-specific edges:\", len(disease_specific))\n\nchanges = G.temporal_dynamics([\"Healthy\", \"Stressed\", \"Disease\"], metric=\"edge_change\")\nprint(\"\\nTemporal edge changes (Healthy\u2192Stressed\u2192Disease):\", changes)\nassert len(changes) == 2\n</pre> # ---------- slice analytics ---------- stats = G.slice_statistics() print(\"\\nslice stats:\", stats)  conserved = G.conserved_edges(min_slices=3)  # present in all 3 conditions print(\"\\nConserved edges (in all conditions):\", len(conserved))  disease_specific = G.slice_specific_edges(\"Disease\") print(\"Disease-specific edges:\", len(disease_specific))  changes = G.temporal_dynamics([\"Healthy\", \"Stressed\", \"Disease\"], metric=\"edge_change\") print(\"\\nTemporal edge changes (Healthy\u2192Stressed\u2192Disease):\", changes) assert len(changes) == 2 <pre>\nslice stats: {'Healthy': {'vertices': 220, 'edges': 360, 'attributes': {'condition': 'Healthy'}}, 'Stressed': {'vertices': 220, 'edges': 360, 'attributes': {'condition': 'Stressed'}}, 'Disease': {'vertices': 220, 'edges': 360, 'attributes': {'condition': 'Disease'}}}\n\nConserved edges (in all conditions): 360\nDisease-specific edges: 0\n\nTemporal edge changes (Healthy\u2192Stressed\u2192Disease): [{'added': 0, 'removed': 0, 'net_change': 0}, {'added': 0, 'removed': 0, 'net_change': 0}]\n</pre> In\u00a0[17]: Copied! <pre># ---------- Presence queries ----------\nsome_e = next(iter(G.edge_to_idx.keys()))\nprint(\"\\nEdge presence for\", some_e, \":\", G.edge_presence_across_slices(edge_id=some_e))\nsome_p = random.choice(proteins)\nprint(\"vertex presence for\", some_p, \":\", G.vertex_presence_across_slices(some_p))\n</pre> # ---------- Presence queries ---------- some_e = next(iter(G.edge_to_idx.keys())) print(\"\\nEdge presence for\", some_e, \":\", G.edge_presence_across_slices(edge_id=some_e)) some_p = random.choice(proteins) print(\"vertex presence for\", some_p, \":\", G.vertex_presence_across_slices(some_p)) <pre>\nEdge presence for edge_0 : ['Healthy', 'Stressed', 'Disease']\nvertex presence for P78 : ['Healthy', 'Stressed', 'Disease']\n</pre> In\u00a0[18]: Copied! <pre># ---------- Traversal checks ----------\nq = random.choice(proteins)\nprint(f\"\\nNeighbors({q}) =&gt;\", G.neighbors(q)[:10])\nprint(f\"Out({q}) =&gt;\", G.out_neighbors(q)[:10])\nprint(f\"In({q}) =&gt;\", G.in_neighbors(q)[:10])\n</pre> # ---------- Traversal checks ---------- q = random.choice(proteins) print(f\"\\nNeighbors({q}) =&gt;\", G.neighbors(q)[:10]) print(f\"Out({q}) =&gt;\", G.out_neighbors(q)[:10]) print(f\"In({q}) =&gt;\", G.in_neighbors(q)[:10]) <pre>\nNeighbors(P134) =&gt; ['P62', 'P149', 'P72', 'P99', 'P14', 'P128', 'P69', 'P118']\nOut(P134) =&gt; ['P62', 'P149', 'P72', 'P99', 'P14', 'P128', 'P69', 'P118']\nIn(P134) =&gt; ['P128', 'P72', 'P99', 'P149']\n</pre> In\u00a0[19]: Copied! <pre># ---------- Subgraph slice &amp; copy ----------\nH = G.subgraph_from_slice(\"Disease\", resolve_slice_weights=True)\n# assert set(H.vertices()).issubset(set(G.vertices()))\nassert set(H.edges()).issubset(set(G.edges()))\nprint(\"\\nDisease subgraph: vertices =\", H.number_of_vertices(), \"edges =\", H.number_of_edges())\n\nK = G.copy()\nassert set(K.vertices()) == set(G.vertices())\nassert set(K.edges()) == set(G.edges())\n# hyperedge shape preserved\nany_hyper = next(e for e, k in G.edge_kind.items() if k == \"hyper\")\nassert K.edge_kind.get(any_hyper) == \"hyper\"\n# slice sets preserved\nfor lid in G.list_slices(include_default=True):\n    assert K._slices[lid][\"vertices\"] == G._slices[lid][\"vertices\"]\n    assert K._slices[lid][\"edges\"] == G._slices[lid][\"edges\"]\nprint(\"copy() OK\")\n</pre> # ---------- Subgraph slice &amp; copy ---------- H = G.subgraph_from_slice(\"Disease\", resolve_slice_weights=True) # assert set(H.vertices()).issubset(set(G.vertices())) assert set(H.edges()).issubset(set(G.edges())) print(\"\\nDisease subgraph: vertices =\", H.number_of_vertices(), \"edges =\", H.number_of_edges())  K = G.copy() assert set(K.vertices()) == set(G.vertices()) assert set(K.edges()) == set(G.edges()) # hyperedge shape preserved any_hyper = next(e for e, k in G.edge_kind.items() if k == \"hyper\") assert K.edge_kind.get(any_hyper) == \"hyper\" # slice sets preserved for lid in G.list_slices(include_default=True):     assert K._slices[lid][\"vertices\"] == G._slices[lid][\"vertices\"]     assert K._slices[lid][\"edges\"] == G._slices[lid][\"edges\"] print(\"copy() OK\") <pre>\nDisease subgraph: vertices = 220 edges = 360\ncopy() OK\n</pre> In\u00a0[20]: Copied! <pre># ---------- Remove operations stress ----------\nto_drop_vertices = random.sample(proteins, 5)\nfor n in to_drop_vertices:\n    if n in G.entity_to_idx:\n        G.remove_vertex(n)\nprint(\n    \"\\nAfter removing 5 proteins: vertices =\",\n    G.number_of_vertices(),\n    \"edges =\",\n    G.number_of_edges(),\n)\n\nto_drop_edges = list(G.edge_to_idx.keys())[:10]\nfor eid in to_drop_edges:\n    if eid in G.edge_to_idx:\n        G.remove_edge(eid)\nprint(\"After removing 10 edges: vertices =\", G.number_of_vertices(), \"edges =\", G.number_of_edges())\n</pre> # ---------- Remove operations stress ---------- to_drop_vertices = random.sample(proteins, 5) for n in to_drop_vertices:     if n in G.entity_to_idx:         G.remove_vertex(n) print(     \"\\nAfter removing 5 proteins: vertices =\",     G.number_of_vertices(),     \"edges =\",     G.number_of_edges(), )  to_drop_edges = list(G.edge_to_idx.keys())[:10] for eid in to_drop_edges:     if eid in G.edge_to_idx:         G.remove_edge(eid) print(\"After removing 10 edges: vertices =\", G.number_of_vertices(), \"edges =\", G.number_of_edges()) <pre>\nAfter removing 5 proteins: vertices = 205 edges = 338\nAfter removing 10 edges: vertices = 205 edges = 328\n</pre> In\u00a0[21]: Copied! <pre># ---------- Audit &amp; memory ----------\naudit = G.audit_attributes()\nprint(\"\\nAudit:\", audit)\nmem_bytes = G.memory_usage()\nprint(\"Approx memory usage (bytes):\", int(mem_bytes))\nassert mem_bytes &gt; 0\n\nprint(\"\\nReality-check finished \u2705\")\n</pre> # ---------- Audit &amp; memory ---------- audit = G.audit_attributes() print(\"\\nAudit:\", audit) mem_bytes = G.memory_usage() print(\"Approx memory usage (bytes):\", int(mem_bytes)) assert mem_bytes &gt; 0  print(\"\\nReality-check finished \u2705\") <pre>\nAudit: {'extra_vertex_rows': ['edge_rxn_5', 'edge_rxn_10', 'edge_rxn_8', 'edge_rxn_9', 'edge_rxn_6', 'edge_rxn_2', 'edge_rxn_4', 'edge_rxn_7', 'edge_rxn_1', 'edge_rxn_3'], 'extra_edge_rows': [], 'missing_vertex_rows': [], 'missing_edge_rows': ['edge_22', 'edge_201', 'edge_299', 'edge_355', 'edge_137', 'edge_255', 'edge_343', 'edge_35', 'edge_127', 'edge_227', 'edge_21', 'edge_195', 'edge_288', 'edge_178', 'edge_144', 'edge_197', 'edge_354', 'edge_292', 'edge_199', 'edge_33', 'edge_156', 'edge_38', 'edge_244', 'edge_348', 'edge_146', 'edge_249', 'edge_208', 'edge_303', 'edge_60', 'edge_26', 'edge_31', 'edge_142', 'edge_350', 'edge_13', 'edge_187', 'edge_284', 'edge_14', 'edge_77', 'edge_294', 'edge_344', 'edge_114', 'edge_143', 'edge_172', 'edge_268', 'edge_141', 'edge_289', 'edge_145', 'edge_293', 'edge_24', 'edge_149', 'edge_242', 'edge_351', 'edge_226', 'edge_175', 'edge_295', 'edge_117', 'edge_215', 'edge_128', 'edge_86', 'edge_125', 'edge_93', 'edge_190', 'edge_193', 'edge_163', 'edge_76', 'edge_319', 'edge_165', 'edge_87', 'edge_132', 'edge_315', 'edge_326', 'edge_357', 'edge_307', 'edge_130', 'edge_159', 'edge_207', 'edge_221', 'edge_191', 'edge_91', 'edge_347', 'edge_46', 'edge_290', 'edge_104', 'edge_108', 'edge_136', 'edge_282', 'edge_62', 'edge_70', 'edge_72', 'edge_262', 'edge_92', 'edge_246', 'edge_162', 'edge_275', 'edge_119', 'edge_61', 'edge_138', 'edge_202', 'edge_323', 'edge_161', 'edge_160', 'edge_328', 'edge_29', 'edge_59', 'edge_81', 'edge_153', 'edge_241', 'edge_239', 'edge_134', 'edge_217', 'edge_304', 'edge_259', 'edge_220', 'edge_45', 'edge_340', 'edge_140', 'edge_213', 'edge_105', 'edge_167', 'edge_219', 'edge_113', 'edge_158', 'edge_64', 'edge_309', 'edge_230', 'edge_148', 'edge_205', 'edge_17', 'edge_180', 'edge_110', 'edge_263', 'edge_53', 'edge_280', 'edge_327', 'edge_123', 'edge_196', 'edge_194', 'edge_171', 'edge_291', 'edge_34', 'edge_173', 'edge_283', 'edge_206', 'edge_15', 'edge_25', 'edge_332', 'edge_287', 'edge_300', 'edge_186', 'edge_337', 'edge_23', 'edge_204', 'edge_88', 'edge_211', 'edge_65', 'edge_42', 'edge_231', 'edge_308', 'edge_269', 'edge_183', 'edge_353', 'edge_188', 'edge_336', 'edge_44', 'edge_166', 'edge_229', 'edge_154', 'edge_179', 'edge_248', 'edge_47', 'edge_155', 'edge_36', 'edge_182', 'edge_75', 'edge_169', 'edge_306', 'edge_276', 'edge_198', 'edge_258', 'edge_235', 'edge_170', 'edge_247', 'edge_298', 'edge_94', 'edge_152', 'edge_100', 'edge_52', 'edge_32', 'edge_101', 'edge_78', 'edge_356', 'edge_30', 'edge_174', 'edge_238', 'edge_312', 'edge_311', 'edge_265', 'edge_322', 'edge_266', 'edge_245', 'edge_349', 'edge_85', 'edge_286', 'edge_57', 'edge_177', 'edge_218', 'edge_126', 'edge_115', 'edge_252', 'edge_16', 'edge_185', 'edge_112', 'edge_251', 'edge_69', 'edge_129', 'edge_95', 'edge_314', 'edge_50', 'edge_120', 'edge_254', 'edge_359', 'edge_184', 'edge_318', 'edge_147', 'edge_20', 'edge_228', 'edge_39', 'edge_297', 'edge_107', 'edge_338', 'edge_358', 'edge_189', 'edge_316', 'edge_320', 'edge_334', 'edge_214', 'edge_151', 'edge_210', 'edge_164', 'edge_83', 'edge_56', 'edge_261', 'edge_48', 'edge_277', 'edge_281', 'edge_116', 'edge_79', 'edge_176', 'edge_243', 'edge_139', 'edge_19', 'edge_279', 'edge_331', 'edge_131', 'edge_233', 'edge_73', 'edge_106', 'edge_122', 'edge_150', 'edge_267', 'edge_12', 'edge_41', 'edge_68', 'edge_28', 'edge_55', 'edge_237', 'edge_285', 'edge_97', 'edge_253', 'edge_234', 'edge_296', 'edge_240', 'edge_324', 'edge_250', 'edge_37', 'edge_310', 'edge_278', 'edge_225', 'edge_301', 'edge_352', 'edge_71', 'edge_118', 'edge_43', 'edge_212', 'edge_317', 'edge_96', 'edge_111', 'edge_274', 'edge_99', 'edge_49', 'edge_305', 'edge_270', 'edge_302', 'edge_51', 'edge_168', 'edge_209', 'edge_257', 'edge_345', 'edge_200', 'edge_271', 'edge_124', 'edge_181', 'edge_329', 'edge_273', 'edge_256', 'edge_223', 'edge_90', 'edge_84', 'edge_133', 'edge_80', 'edge_58', 'edge_222', 'edge_66', 'edge_339', 'edge_121', 'edge_103', 'edge_341', 'edge_346', 'edge_313', 'edge_109', 'edge_342', 'edge_67', 'edge_236', 'edge_74', 'edge_102', 'edge_264', 'edge_272', 'edge_18'], 'invalid_edge_slice_rows': []}\nApprox memory usage (bytes): 96868\n\nReality-check finished \u2705\n</pre> In\u00a0[23]: Copied! <pre>events = G.history()  # list[dict]\ndf = G.history(as_df=True)  # Polars DF [DataFrame]\n</pre> events = G.history()  # list[dict] df = G.history(as_df=True)  # Polars DF [DataFrame] In\u00a0[24]: Copied! <pre>print(df.head())\nevents[:3]\n</pre> print(df.head()) events[:3] <pre>shape: (5, 10)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 version \u2506 ts_utc                      \u2506 mono_ns \u2506 op              \u2506 \u2026 \u2506 result \u2506 vertex_id \u2506 slice   \u2506 attributes \u2502\n\u2502 ---     \u2506 ---                         \u2506 ---     \u2506 ---             \u2506   \u2506 ---    \u2506 ---       \u2506 ---     \u2506 ---        \u2502\n\u2502 i64     \u2506 str                         \u2506 i64     \u2506 str             \u2506   \u2506 str    \u2506 str       \u2506 str     \u2506 struct[1]  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2025-11-18T19:03:34.224858Z \u2506 3010000 \u2506 set_slice_attrs \u2506 \u2026 \u2506 null   \u2506 null      \u2506 null    \u2506 null       \u2502\n\u2502 2       \u2506 2025-11-18T19:03:34.225858Z \u2506 3119800 \u2506 set_slice_attrs \u2506 \u2026 \u2506 null   \u2506 null      \u2506 null    \u2506 null       \u2502\n\u2502 3       \u2506 2025-11-18T19:03:34.225858Z \u2506 3191100 \u2506 set_slice_attrs \u2506 \u2026 \u2506 null   \u2506 null      \u2506 null    \u2506 null       \u2502\n\u2502 4       \u2506 2025-11-18T19:03:34.226868Z \u2506 4205400 \u2506 add_vertex      \u2506 \u2026 \u2506 P1     \u2506 P1        \u2506 Healthy \u2506 {\"kinase\"} \u2502\n\u2502 5       \u2506 2025-11-18T19:03:34.226868Z \u2506 4662900 \u2506 add_vertex      \u2506 \u2026 \u2506 P2     \u2506 P2        \u2506 Healthy \u2506 {\"kinase\"} \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> Out[24]: <pre>[{'version': 1,\n  'ts_utc': '2025-11-18T19:03:34.224858Z',\n  'mono_ns': 3010000,\n  'op': 'set_slice_attrs',\n  'slice_id': 'Healthy',\n  'attrs': {'condition': 'Healthy'},\n  'result': None},\n {'version': 2,\n  'ts_utc': '2025-11-18T19:03:34.225858Z',\n  'mono_ns': 3119800,\n  'op': 'set_slice_attrs',\n  'slice_id': 'Stressed',\n  'attrs': {'condition': 'Stressed'},\n  'result': None},\n {'version': 3,\n  'ts_utc': '2025-11-18T19:03:34.225858Z',\n  'mono_ns': 3191100,\n  'op': 'set_slice_attrs',\n  'slice_id': 'Disease',\n  'attrs': {'condition': 'Disease'},\n  'result': None}]</pre> In\u00a0[26]: Copied! <pre>import pathlib\nimport sys\n\n# add repo root to Python path\nsys.path.append(str(pathlib.Path.cwd().parent))\n</pre> import pathlib import sys  # add repo root to Python path sys.path.append(str(pathlib.Path.cwd().parent)) In\u00a0[29]: Copied! <pre>csv1_path = \"csv1_edges.csv\"\npl.DataFrame(\n    {\n        \"source\": [\"A\", \"A\", \"B\", \"C\", \"D\"],\n        \"target\": [\"B\", \"C\", \"C\", \"D\", \"A\"],\n        \"weight\": [1, 2, 3, 1, 5],\n        \"directed\": [True, True, False, True, True],\n        \"slice\": [\"L1\", \"L1\", \"L1\", \"L2\", \"L2\"],\n    }\n)  # .write_csv(csv1_path)\n\n# pl.read_csv(csv1_path).head()\n</pre> csv1_path = \"csv1_edges.csv\" pl.DataFrame(     {         \"source\": [\"A\", \"A\", \"B\", \"C\", \"D\"],         \"target\": [\"B\", \"C\", \"C\", \"D\", \"A\"],         \"weight\": [1, 2, 3, 1, 5],         \"directed\": [True, True, False, True, True],         \"slice\": [\"L1\", \"L1\", \"L1\", \"L2\", \"L2\"],     } )  # .write_csv(csv1_path)  # pl.read_csv(csv1_path).head() Out[29]: shape: (5, 5)sourcetargetweightdirectedslicestrstri64boolstr\"A\"\"B\"1true\"L1\"\"A\"\"C\"2true\"L1\"\"B\"\"C\"3false\"L1\"\"C\"\"D\"1true\"L2\"\"D\"\"A\"5true\"L2\" In\u00a0[30]: Copied! <pre>\"\"\"G = graph_csv.load_csv_to_graph(\n    csv1_path,\n    schema=\"auto\",            # or 'edge_list'/'incidence'/'adjacency'/'hyperedge'/'lil'\n    default_slice=None,       # fallback if no slice column is present\n    default_directed=None,    # fallback if no directed column and cannot infer\n    default_weight=1.0,\n)\n\n# Quick sanity: show first rows of an edges view (columns depend on your Graph implementation)\nedges = G.edges_view(slice=None, include_directed=True, resolved_weight=True)\nedges.head()\n\"\"\"\n</pre> \"\"\"G = graph_csv.load_csv_to_graph(     csv1_path,     schema=\"auto\",            # or 'edge_list'/'incidence'/'adjacency'/'hyperedge'/'lil'     default_slice=None,       # fallback if no slice column is present     default_directed=None,    # fallback if no directed column and cannot infer     default_weight=1.0, )  # Quick sanity: show first rows of an edges view (columns depend on your Graph implementation) edges = G.edges_view(slice=None, include_directed=True, resolved_weight=True) edges.head() \"\"\" Out[30]: <pre>'G = graph_csv.load_csv_to_graph(\\n    csv1_path,\\n    schema=\"auto\",            # or \\'edge_list\\'/\\'incidence\\'/\\'adjacency\\'/\\'hyperedge\\'/\\'lil\\'\\n    default_slice=None,       # fallback if no slice column is present\\n    default_directed=None,    # fallback if no directed column and cannot infer\\n    default_weight=1.0,\\n)\\n\\n# Quick sanity: show first rows of an edges view (columns depend on your Graph implementation)\\nedges = G.edges_view(slice=None, include_directed=True, resolved_weight=True)\\nedges.head()\\n'</pre> In\u00a0[31]: Copied! <pre># Count entities and edges (attribute names based on your class; adjust if different)\nnum_entities = G.global_entity_count  # vertices + edge-entities\nnum_edges = G.global_edge_count  # binary + hyper\n\nprint(\"entities:\", num_entities, \"edges:\", num_edges)\n\n# A light \u201cdegree\u201d summary from edges_view for binary edges only (skip hyper)\ndf = G.edges_view(include_directed=True, resolved_weight=True)\n\n# Try to find endpoint column names robustly\ncols = {c.lower(): c for c in df.columns}\n# Common possibilities:\nsrc_col = next((cols[c] for c in [\"source\", \"src\", \"u\", \"from\"]), None)\ndst_col = next((cols[c] for c in [\"target\", \"dst\", \"v\", \"to\"]), None)\n\nif src_col and dst_col:\n    # out-degree (directed) / degree (undirected)\n    out_deg = df.group_by(src_col).len().rename({src_col: \"vertex\", \"len\": \"out_degree\"})\n    in_deg = df.group_by(dst_col).len().rename({dst_col: \"vertex\", \"len\": \"in_degree\"})\n    deg = out_deg.join(in_deg, on=\"vertex\", how=\"outer\").fill_null(0)\n    deg = deg.with_columns((pl.col(\"out_degree\") + pl.col(\"in_degree\")).alias(\"total_degree\"))\n    deg.sort(\"total_degree\", descending=True).head(10)\nelse:\n    print(\n        \"Skip degree summary: endpoint columns not found in edges_view output (likely hyperedge-only or different schema).\"\n    )\n</pre> # Count entities and edges (attribute names based on your class; adjust if different) num_entities = G.global_entity_count  # vertices + edge-entities num_edges = G.global_edge_count  # binary + hyper  print(\"entities:\", num_entities, \"edges:\", num_edges)  # A light \u201cdegree\u201d summary from edges_view for binary edges only (skip hyper) df = G.edges_view(include_directed=True, resolved_weight=True)  # Try to find endpoint column names robustly cols = {c.lower(): c for c in df.columns} # Common possibilities: src_col = next((cols[c] for c in [\"source\", \"src\", \"u\", \"from\"]), None) dst_col = next((cols[c] for c in [\"target\", \"dst\", \"v\", \"to\"]), None)  if src_col and dst_col:     # out-degree (directed) / degree (undirected)     out_deg = df.group_by(src_col).len().rename({src_col: \"vertex\", \"len\": \"out_degree\"})     in_deg = df.group_by(dst_col).len().rename({dst_col: \"vertex\", \"len\": \"in_degree\"})     deg = out_deg.join(in_deg, on=\"vertex\", how=\"outer\").fill_null(0)     deg = deg.with_columns((pl.col(\"out_degree\") + pl.col(\"in_degree\")).alias(\"total_degree\"))     deg.sort(\"total_degree\", descending=True).head(10) else:     print(         \"Skip degree summary: endpoint columns not found in edges_view output (likely hyperedge-only or different schema).\"     ) <pre>entities: &lt;bound method Graph.global_entity_count of &lt;annnet.core.graph.Graph object at 0x000001F26347ED80&gt;&gt; edges: &lt;bound method Graph.global_edge_count of &lt;annnet.core.graph.Graph object at 0x000001F26347ED80&gt;&gt;\n</pre> <pre>C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_5544\\338457354.py:20: DeprecationWarning: use of `how='outer'` should be replaced with `how='full'`.\n(Deprecated in version 0.20.29)\n  deg = out_deg.join(in_deg, on=\"vertex\", how=\"outer\").fill_null(0)\n</pre> In\u00a0[32]: Copied! <pre># Add a new vertex and an edge on slice L3\nG.add_vertex(\"E\")\neid = G.add_edge(\"E\", \"A\", slice=\"L3\", directed=True, weight=2.5)\n\n# Per-slice weight override example:\nG.set_edge_slice_attrs(\"L3\", eid, weight=3.0)\n\n# Inspect the updated edges\nG.edges_view(include_directed=True, resolved_weight=True).tail()\n</pre> # Add a new vertex and an edge on slice L3 G.add_vertex(\"E\") eid = G.add_edge(\"E\", \"A\", slice=\"L3\", directed=True, weight=2.5)  # Per-slice weight override example: G.set_edge_slice_attrs(\"L3\", eid, weight=3.0)  # Inspect the updated edges G.edges_view(include_directed=True, resolved_weight=True).tail() Out[32]: shape: (5, 11)edge_idkinddirectedglobal_weightsourcetargetedge_typeheadtailmemberseffective_weightstrstrboolf64strstrstrlist[str]list[str]list[str]f64\"edge_356\"\"binary\"true1.338084\"P38\"\"edge_rxn_9\"\"vertex_edge\"nullnullnull1.338084\"edge_357\"\"binary\"true1.08058\"edge_rxn_9\"\"P148\"\"vertex_edge\"nullnullnull1.08058\"edge_358\"\"binary\"true1.102427\"P126\"\"edge_rxn_10\"\"vertex_edge\"nullnullnull1.102427\"edge_359\"\"binary\"true1.230621\"edge_rxn_10\"\"P23\"\"vertex_edge\"nullnullnull1.230621\"edge_360\"\"binary\"true2.5\"E\"\"A\"\"regular\"nullnullnull2.5 In\u00a0[34]: Copied! <pre>csv2_path = \"csv2_edges_view.csv\"\nG.edges_view(slice=None, include_directed=True, resolved_weight=True)\ncsv2_path\n</pre> csv2_path = \"csv2_edges_view.csv\" G.edges_view(slice=None, include_directed=True, resolved_weight=True) csv2_path Out[34]: <pre>'csv2_edges_view.csv'</pre> In\u00a0[45]: Copied! <pre>def export_edge_list_csv(G, path, slice=None):\n    df = G.edges_view(slice=slice, include_directed=True, resolved_weight=True)\n    cols = {c.lower(): c for c in df.columns}\n    src_col = next((cols[c] for c in [\"source\", \"src\", \"u\", \"from\"]), None)\n    dst_col = next((cols[c] for c in [\"target\", \"dst\", \"v\", \"to\"]), None)\n    dir_col = next((cols[c] for c in [\"directed\"]), None)\n    w_eff = next((cols[c] for c in [\"effective_weight\", \"weight\", \"w\"]), None)\n\n    if not (src_col and dst_col):\n        raise ValueError(\n            \"No binary endpoint columns found; the view may be hyperedge-only. Try the generic edges_view export.\"\n        )\n\n    out = pl.DataFrame(\n        {\n            \"source\": df[src_col],\n            \"target\": df[dst_col],\n            \"weight\": df[w_eff] if w_eff else pl.Series([1.0] * df.height),\n            \"directed\": df[dir_col] if dir_col in df.columns else pl.Series([None] * df.height),\n            \"slice\": pl.Series([slice] * df.height) if slice else pl.Series([None] * df.height),\n        }\n    )\n    out  # .write_csv(path)\n\n\n# Usage:\ncsv2_edge_list_path = \"csv2_edge_list.csv\"\nexport_edge_list_csv(G, csv2_edge_list_path, slice=None)\ncsv2_edge_list_path\n</pre> def export_edge_list_csv(G, path, slice=None):     df = G.edges_view(slice=slice, include_directed=True, resolved_weight=True)     cols = {c.lower(): c for c in df.columns}     src_col = next((cols[c] for c in [\"source\", \"src\", \"u\", \"from\"]), None)     dst_col = next((cols[c] for c in [\"target\", \"dst\", \"v\", \"to\"]), None)     dir_col = next((cols[c] for c in [\"directed\"]), None)     w_eff = next((cols[c] for c in [\"effective_weight\", \"weight\", \"w\"]), None)      if not (src_col and dst_col):         raise ValueError(             \"No binary endpoint columns found; the view may be hyperedge-only. Try the generic edges_view export.\"         )      out = pl.DataFrame(         {             \"source\": df[src_col],             \"target\": df[dst_col],             \"weight\": df[w_eff] if w_eff else pl.Series([1.0] * df.height),             \"directed\": df[dir_col] if dir_col in df.columns else pl.Series([None] * df.height),             \"slice\": pl.Series([slice] * df.height) if slice else pl.Series([None] * df.height),         }     )     out  # .write_csv(path)   # Usage: csv2_edge_list_path = \"csv2_edge_list.csv\" export_edge_list_csv(G, csv2_edge_list_path, slice=None) csv2_edge_list_path Out[45]: <pre>'csv2_edge_list.csv'</pre> In\u00a0[47]: Copied! <pre># In-memory look at last few events\nhist = G.history(as_df=True)  # DF [DataFrame]\nhist.tail()\n\n# Save to Parquet/CSV/JSON [JavaScript Object Notation]/NDJSON [Newline-Delimited JSON]\nG.export_history(\"graph_history.parquet\")\n</pre> # In-memory look at last few events hist = G.history(as_df=True)  # DF [DataFrame] hist.tail()  # Save to Parquet/CSV/JSON [JavaScript Object Notation]/NDJSON [Newline-Delimited JSON] G.export_history(\"graph_history.parquet\") Out[47]: <pre>1274</pre> In\u00a0[49]: Copied! <pre>import pathlib\nimport sys\n\nsys.path.append(str(pathlib.Path.cwd().parent))\n\nfrom annnet.core.graph import Graph\n</pre> import pathlib import sys  sys.path.append(str(pathlib.Path.cwd().parent))  from annnet.core.graph import Graph In\u00a0[51]: Copied! <pre># G = graph_excel.load_excel_to_graph(\"graph_input.xlsx\", schema=\"auto\")\n</pre> # G = graph_excel.load_excel_to_graph(\"graph_input.xlsx\", schema=\"auto\") In\u00a0[53]: Copied! <pre>G.global_entity_count()\n</pre> G.global_entity_count() Out[53]: <pre>217</pre> In\u00a0[55]: Copied! <pre>from annnet.adapters.networkx_adapter import to_nx\n</pre> from annnet.adapters.networkx_adapter import to_nx In\u00a0[57]: Copied! <pre>nxG, man = to_nx(G, directed=True, hyperedge_mode=\"skip\")\n</pre> nxG, man = to_nx(G, directed=True, hyperedge_mode=\"skip\") In\u00a0[58]: Copied! <pre>from annnet.adapters.igraph_adapter import to_igraph\n</pre> from annnet.adapters.igraph_adapter import to_igraph In\u00a0[59]: Copied! <pre>nxG, man = to_igraph(G, directed=True, hyperedge_mode=\"skip\", public_only=False)\n</pre> nxG, man = to_igraph(G, directed=True, hyperedge_mode=\"skip\", public_only=False) In\u00a0[60]: Copied! <pre>G.shape\n</pre> G.shape Out[60]: <pre>(207, 329)</pre> In\u00a0[61]: Copied! <pre># Deterministic smoke tests for the lazy NX (NetworkX) proxy\n\n# ---------- G1: PATH GRAPH (for weighted/unweighted shortest paths) ----------\ndef build_path_graph() -&gt; Graph:\n    \"\"\"Directed chain a\u2192b\u2192c\u2192d\u2192e\u2192f with weights on each edge.\n    - Weighted shortest path a\u2192f = 1+2+3+1+4 = 11\n    - Unweighted hops a\u2192f = 5\n    \"\"\"\n    G = Graph(directed=True)\n    # vertices (+ labels for the label\u2192ID mapping)\n    G.add_vertex(\"a\", name=\"alpha\")\n    G.add_vertex(\"b\", name=\"bravo\")\n    G.add_vertex(\"c\", name=\"charlie\")\n    G.add_vertex(\"d\", name=\"delta\")\n    G.add_vertex(\"e\", name=\"echo\")\n    G.add_vertex(\"f\", name=\"phi\")\n\n    # pure chain (NO chords)\n    G.add_edge(\"a\", \"b\", weight=1)\n    G.add_edge(\"b\", \"c\", weight=2)\n    G.add_edge(\"c\", \"d\", weight=3)\n    G.add_edge(\"d\", \"e\", weight=1)\n    G.add_edge(\"e\", \"f\", weight=4)\n\n    return G\n\n\n# ---------- G2: COMMUNITY GRAPH (two cliques + weak bridge) ----------\ndef build_community_graph() -&gt; Graph:\n    \"\"\"Two undirected cliques: K6 on {a..f} and K4 on {w,x,y,z}, joined by a single weak bridge e--x (weight=0.01).\n    Louvain should give communities of sizes [4,6] (stable with seed); betweenness top in {'e','x'};\n    PR (PageRank) top is 'e' in undirected view (highest degree).\n    \"\"\"\n    G = Graph(directed=True)  # we\u2019ll add undirected edges explicitly\n\n    for v in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"w\", \"x\", \"y\", \"z\"]:\n        G.add_vertex(v)\n\n    # K6 clique on a..f (undirected, weight=1)\n    k6 = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n    for i in range(len(k6)):\n        for j in range(i + 1, len(k6)):\n            G.add_edge(k6[i], k6[j], weight=1, edge_directed=False)\n\n    # K4 clique on w,x,y,z (undirected, weight=1)\n    k4 = [\"w\", \"x\", \"y\", \"z\"]\n    for i in range(len(k4)):\n        for j in range(i + 1, len(k4)):\n            G.add_edge(k4[i], k4[j], weight=1, edge_directed=False)\n\n    # Single weak bridge e--x\n    G.add_edge(\"e\", \"x\", weight=0.01, edge_directed=False)\n\n    return G\n\n\ndef run_tests():\n    # ----- G1: shortest paths -----\n    G1 = build_path_graph()\n\n    # Weighted Dijkstra via labels -&gt; expect 11.0\n    dist_w = G1.nx.shortest_path_length(\n        G1, source=\"alpha\", target=\"phi\", weight=\"weight\", _nx_label_field=\"name\"\n    )\n    print(\"[G1:dijkstra weighted] alpha-&gt;phi:\", dist_w, \"(expect 11.0)\")\n\n    # Unweighted hop count -&gt; expect 5 (a-b-c-d-e-f)\n    dist_hops = G1.nx.shortest_path_length(G1, source=\"a\", target=\"f\", weight=None)\n    print(\"[G1:unweighted hops] a-&gt;f:\", dist_hops, \"(expect 5)\")\n\n    # Cache invalidation: add fast a-&gt;f edge (weight=2) and re-check -&gt; expect 2.0\n    G1.add_edge(\"a\", \"f\", weight=2)\n    dist_new = G1.nx.shortest_path_length(G1, source=\"a\", target=\"f\", weight=\"weight\")\n    print(\"[G1:after mutation] a-&gt;f:\", dist_new, \"(expect 2.0)\")\n\n    # ----- G2: communities / centrality / PR / components -----\n    G2 = build_community_graph()\n\n    # Louvain on undirected view -&gt; expect sizes [4, 6]\n    comms = G2.nx.louvain_communities(G2, _nx_directed=False, seed=42, weight=\"weight\")\n    sizes = sorted(len(c) for c in comms)\n    print(\"[G2:louvain] sizes:\", sizes, \"(expect [4, 6])\")\n\n    # Betweenness centrality (unweighted, undirected) -&gt; bridge endpoints dominate\n    bc = G2.nx.betweenness_centrality(G2, _nx_directed=False, normalized=True)\n    top_bc = max(bc, key=bc.get)\n    print(\"[G2:betweenness] top:\", top_bc, \"(expect 'e' or 'x')\")\n\n    # PageRank (PR) on undirected view (unweighted edges) -&gt; highest degree node is 'e'\n    pr = G2.nx.pagerank(G2, _nx_directed=False)\n    top_pr = max(pr, key=pr.get)\n    print(\"[G2:pagerank] top:\", top_pr, \"(expect 'e')\")\n\n    # Connected components (undirected) -&gt; single component of size 10\n    comps = list(G2.nx.connected_components(G2, _nx_directed=False))\n    print(\n        \"[G2:connected components]:\",\n        [sorted(c) for c in comps],\n        \"(expect one component of size 10)\",\n    )\n\n\nif __name__ == \"__main__\":\n    run_tests()\n</pre> # Deterministic smoke tests for the lazy NX (NetworkX) proxy  # ---------- G1: PATH GRAPH (for weighted/unweighted shortest paths) ---------- def build_path_graph() -&gt; Graph:     \"\"\"Directed chain a\u2192b\u2192c\u2192d\u2192e\u2192f with weights on each edge.     - Weighted shortest path a\u2192f = 1+2+3+1+4 = 11     - Unweighted hops a\u2192f = 5     \"\"\"     G = Graph(directed=True)     # vertices (+ labels for the label\u2192ID mapping)     G.add_vertex(\"a\", name=\"alpha\")     G.add_vertex(\"b\", name=\"bravo\")     G.add_vertex(\"c\", name=\"charlie\")     G.add_vertex(\"d\", name=\"delta\")     G.add_vertex(\"e\", name=\"echo\")     G.add_vertex(\"f\", name=\"phi\")      # pure chain (NO chords)     G.add_edge(\"a\", \"b\", weight=1)     G.add_edge(\"b\", \"c\", weight=2)     G.add_edge(\"c\", \"d\", weight=3)     G.add_edge(\"d\", \"e\", weight=1)     G.add_edge(\"e\", \"f\", weight=4)      return G   # ---------- G2: COMMUNITY GRAPH (two cliques + weak bridge) ---------- def build_community_graph() -&gt; Graph:     \"\"\"Two undirected cliques: K6 on {a..f} and K4 on {w,x,y,z}, joined by a single weak bridge e--x (weight=0.01).     Louvain should give communities of sizes [4,6] (stable with seed); betweenness top in {'e','x'};     PR (PageRank) top is 'e' in undirected view (highest degree).     \"\"\"     G = Graph(directed=True)  # we\u2019ll add undirected edges explicitly      for v in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"w\", \"x\", \"y\", \"z\"]:         G.add_vertex(v)      # K6 clique on a..f (undirected, weight=1)     k6 = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]     for i in range(len(k6)):         for j in range(i + 1, len(k6)):             G.add_edge(k6[i], k6[j], weight=1, edge_directed=False)      # K4 clique on w,x,y,z (undirected, weight=1)     k4 = [\"w\", \"x\", \"y\", \"z\"]     for i in range(len(k4)):         for j in range(i + 1, len(k4)):             G.add_edge(k4[i], k4[j], weight=1, edge_directed=False)      # Single weak bridge e--x     G.add_edge(\"e\", \"x\", weight=0.01, edge_directed=False)      return G   def run_tests():     # ----- G1: shortest paths -----     G1 = build_path_graph()      # Weighted Dijkstra via labels -&gt; expect 11.0     dist_w = G1.nx.shortest_path_length(         G1, source=\"alpha\", target=\"phi\", weight=\"weight\", _nx_label_field=\"name\"     )     print(\"[G1:dijkstra weighted] alpha-&gt;phi:\", dist_w, \"(expect 11.0)\")      # Unweighted hop count -&gt; expect 5 (a-b-c-d-e-f)     dist_hops = G1.nx.shortest_path_length(G1, source=\"a\", target=\"f\", weight=None)     print(\"[G1:unweighted hops] a-&gt;f:\", dist_hops, \"(expect 5)\")      # Cache invalidation: add fast a-&gt;f edge (weight=2) and re-check -&gt; expect 2.0     G1.add_edge(\"a\", \"f\", weight=2)     dist_new = G1.nx.shortest_path_length(G1, source=\"a\", target=\"f\", weight=\"weight\")     print(\"[G1:after mutation] a-&gt;f:\", dist_new, \"(expect 2.0)\")      # ----- G2: communities / centrality / PR / components -----     G2 = build_community_graph()      # Louvain on undirected view -&gt; expect sizes [4, 6]     comms = G2.nx.louvain_communities(G2, _nx_directed=False, seed=42, weight=\"weight\")     sizes = sorted(len(c) for c in comms)     print(\"[G2:louvain] sizes:\", sizes, \"(expect [4, 6])\")      # Betweenness centrality (unweighted, undirected) -&gt; bridge endpoints dominate     bc = G2.nx.betweenness_centrality(G2, _nx_directed=False, normalized=True)     top_bc = max(bc, key=bc.get)     print(\"[G2:betweenness] top:\", top_bc, \"(expect 'e' or 'x')\")      # PageRank (PR) on undirected view (unweighted edges) -&gt; highest degree node is 'e'     pr = G2.nx.pagerank(G2, _nx_directed=False)     top_pr = max(pr, key=pr.get)     print(\"[G2:pagerank] top:\", top_pr, \"(expect 'e')\")      # Connected components (undirected) -&gt; single component of size 10     comps = list(G2.nx.connected_components(G2, _nx_directed=False))     print(         \"[G2:connected components]:\",         [sorted(c) for c in comps],         \"(expect one component of size 10)\",     )   if __name__ == \"__main__\":     run_tests() <pre>[G1:dijkstra weighted] alpha-&gt;phi: 11 (expect 11.0)\n[G1:unweighted hops] a-&gt;f: 5 (expect 5)\n[G1:after mutation] a-&gt;f: 2 (expect 2.0)\n[G2:louvain] sizes: [4, 6] (expect [4, 6])\n[G2:betweenness] top: e (expect 'e' or 'x')\n[G2:pagerank] top: e (expect 'e')\n[G2:connected components]: [['a', 'b', 'c', 'd', 'e', 'f', 'w', 'x', 'y', 'z']] (expect one component of size 10)\n</pre> In\u00a0[62]: Copied! <pre># Deterministic smoke tests for the lazy ig (igraph) proxy\n\nfrom annnet.core.graph import Graph\n\n\n# ---------- G1: PATH GRAPH (for weighted/unweighted shortest paths) ----------\ndef build_path_graph() -&gt; Graph:\n    \"\"\"Directed chain a\u2192b\u2192c\u2192d\u2192e\u2192f with weights on each edge.\n    - Weighted shortest path a\u2192f = 1+2+3+1+4 = 11\n    - Unweighted hops a\u2192f = 5\n    \"\"\"\n    G = Graph(directed=True)\n    # vertices (+ labels for the label\u2192ID mapping)\n    G.add_vertex(\"a\", name=\"alpha\")\n    G.add_vertex(\"b\", name=\"bravo\")\n    G.add_vertex(\"c\", name=\"charlie\")\n    G.add_vertex(\"d\", name=\"delta\")\n    G.add_vertex(\"e\", name=\"echo\")\n    G.add_vertex(\"f\", name=\"phi\")\n\n    # pure chain (NO chords)\n    G.add_edge(\"a\", \"b\", weight=1)\n    G.add_edge(\"b\", \"c\", weight=2)\n    G.add_edge(\"c\", \"d\", weight=3)\n    G.add_edge(\"d\", \"e\", weight=1)\n    G.add_edge(\"e\", \"f\", weight=4)\n    return G\n\n\n# ---------- G2: COMMUNITY GRAPH (two cliques + weak bridge) ----------\ndef build_community_graph() -&gt; Graph:\n    \"\"\"Two undirected cliques: K6 on {a..f} and K4 on {w,x,y,z}, joined by a single weak bridge e--x (weight=0.01).\n    multilevel (Louvain-like) should give communities of sizes [4,6] (stable with seed);\n    betweenness top in {'e','x'}; PageRank (PR) top is 'e'.\n    \"\"\"\n    G = Graph(directed=True)  # add undirected edges explicitly\n\n    for v in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"w\", \"x\", \"y\", \"z\"]:\n        G.add_vertex(v)\n\n    # K6 clique on a..f (undirected, weight=1)\n    k6 = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n    for i in range(len(k6)):\n        for j in range(i + 1, len(k6)):\n            G.add_edge(k6[i], k6[j], weight=1, edge_directed=False)\n\n    # K4 clique on w,x,y,z (undirected, weight=1)\n    k4 = [\"w\", \"x\", \"y\", \"z\"]\n    for i in range(len(k4)):\n        for j in range(i + 1, len(k4)):\n            G.add_edge(k4[i], k4[j], weight=1, edge_directed=False)\n\n    # Single weak bridge e--x\n    G.add_edge(\"e\", \"x\", weight=0.01, edge_directed=False)\n    return G\n\n\n# ---------- helpers ----------\ndef _unwrap_ig_distance(obj):\n    \"\"\"Igraph returns [[dist]] for single source/target; unwrap to a scalar.\"\"\"\n    if isinstance(obj, (list, tuple)) and len(obj) == 1:\n        inner = obj[0]\n        if isinstance(inner, (list, tuple)) and len(inner) == 1:\n            return inner[0]\n        return inner\n    return obj\n\n\ndef run_tests():\n    # ----- G1: shortest paths -----\n    G1 = build_path_graph()\n\n    # Weighted Dijkstra via labels -&gt; expect 11.0\n    dist_w = G1.ig.shortest_paths_dijkstra(\n        source=\"alpha\", target=\"phi\", weights=\"weight\", _ig_guess_labels=False\n    )\n    dist_w = _unwrap_ig_distance(dist_w)\n    print(\"[IG:G1 dijkstra weighted] alpha-&gt;phi:\", dist_w, \"(expect 11.0)\")\n\n    # Unweighted hop count -&gt; expect 5 (alpha\u2192phi)\n    dist_hops = G1.ig.distances(source=\"alpha\", target=\"phi\", weights=None, _ig_guess_labels=False)\n    dist_hops = _unwrap_ig_distance(dist_hops)\n    print(\"[IG:G1 unweighted hops] alpha-&gt;phi:\", dist_hops, \"(expect 5)\")\n\n    # Cache invalidation: add fast a-&gt;f edge (weight=2) and re-check -&gt; expect 2.0\n    G1.add_edge(\"a\", \"f\", weight=2)\n    dist_new = G1.ig.shortest_paths_dijkstra(\n        source=\"alpha\", target=\"phi\", weights=\"weight\", _ig_guess_labels=False\n    )\n    dist_new = _unwrap_ig_distance(dist_new)\n    print(\"[IG:G1 after mutation] alpha-&gt;phi:\", dist_new, \"(expect 2.0)\")\n\n    # ----- G2: communities / betweenness / PR / components -----\n    G2 = build_community_graph()\n\n    # multilevel (Louvain-like) on undirected view -&gt; expect sizes [4, 6]\n    vc = G2.ig.community_multilevel(weights=\"weight\", _ig_directed=False)\n    sizes = sorted(vc.sizes())\n    print(\"[IG:G2 multilevel] sizes:\", sizes, \"(expect [4, 6])\")\n\n    # Betweenness centrality (undirected). igraph returns list aligned to vertex order.\n    igG_und = G2.ig.backend(directed=False)  # no simple=True needed here\n    names = (\n        igG_und.vs[\"name\"] if \"name\" in igG_und.vs.attributes() else list(range(igG_und.vcount()))\n    )\n    bc_vals = G2.ig.betweenness(directed=False, weights=None)\n    top_bc = max(dict(zip(names, bc_vals)), key=lambda k: dict(zip(names, bc_vals))[k])\n    print(\"[IG:G2 betweenness] top:\", top_bc, \"(expect 'e' or 'x')\")\n\n    # PageRank (undirected, unweighted) -&gt; 'e' should dominate\n    pr_vals = G2.ig.pagerank(directed=False)\n    top_pr = max(dict(zip(names, pr_vals)), key=lambda k: dict(zip(names, pr_vals))[k])\n    print(\"[IG:G2 pagerank] top:\", top_pr, \"(expect 'e')\")\n\n    # Connected components (undirected) -&gt; single component of size 10\n    comps = G2.ig.components(_ig_directed=False)  # VertexClustering\n    comp_sizes = sorted(comps.sizes())\n    print(\"[IG:G2 connected components]:\", comp_sizes, \"(expect [10])\")\n\n    # ----- Optional: verify simple collapse + aggregation policy -----\n    G3 = Graph(directed=True)\n    G3.add_vertex(\"u\")\n    G3.add_vertex(\"v\")\n    # parallel undirected edges with attrs\n    G3.add_edge(\"u\", \"v\", weight=5, capacity=3, edge_directed=False)\n    G3.add_edge(\"u\", \"v\", weight=2, capacity=7, edge_directed=False)\n\n    try:\n        # Preferred: via proxy (needs the one-line proxy patch shown above)\n        igG_simple = G3.ig.backend(\n            directed=False,\n            simple=True,\n            needed_attrs={\"weight\", \"capacity\"},\n            edge_aggs={\"weight\": \"min\", \"capacity\": \"sum\"},\n        )\n        e = igG_simple.es[0]\n        print(\n            \"[IG:G3 collapse agg via proxy] weight:\",\n            e[\"weight\"],\n            \"capacity:\",\n            e[\"capacity\"],\n            \"(expect 2, 10)\",\n        )\n    except Exception:\n        # Fallback: ensure attrs exist, then collapse using igraph.simplify\n        igG_raw = G3.ig.backend(directed=False, needed_attrs={\"weight\", \"capacity\"})\n        igG_raw.simplify(\n            multiple=True, loops=True, combine_edges={\"weight\": \"min\", \"capacity\": \"sum\"}\n        )\n        e = igG_raw.es[0]\n        # after simplify, attrs may be missing if combine didn't carry them; handle safely\n        w = e[\"weight\"] if \"weight\" in igG_raw.es.attributes() else None\n        c = e[\"capacity\"] if \"capacity\" in igG_raw.es.attributes() else None\n        print(\"[IG:G3 collapse agg via simplify] weight:\", w, \"capacity:\", c, \"(expect 2, 10)\")\n\n\nif __name__ == \"__main__\":\n    run_tests()\n</pre> # Deterministic smoke tests for the lazy ig (igraph) proxy  from annnet.core.graph import Graph   # ---------- G1: PATH GRAPH (for weighted/unweighted shortest paths) ---------- def build_path_graph() -&gt; Graph:     \"\"\"Directed chain a\u2192b\u2192c\u2192d\u2192e\u2192f with weights on each edge.     - Weighted shortest path a\u2192f = 1+2+3+1+4 = 11     - Unweighted hops a\u2192f = 5     \"\"\"     G = Graph(directed=True)     # vertices (+ labels for the label\u2192ID mapping)     G.add_vertex(\"a\", name=\"alpha\")     G.add_vertex(\"b\", name=\"bravo\")     G.add_vertex(\"c\", name=\"charlie\")     G.add_vertex(\"d\", name=\"delta\")     G.add_vertex(\"e\", name=\"echo\")     G.add_vertex(\"f\", name=\"phi\")      # pure chain (NO chords)     G.add_edge(\"a\", \"b\", weight=1)     G.add_edge(\"b\", \"c\", weight=2)     G.add_edge(\"c\", \"d\", weight=3)     G.add_edge(\"d\", \"e\", weight=1)     G.add_edge(\"e\", \"f\", weight=4)     return G   # ---------- G2: COMMUNITY GRAPH (two cliques + weak bridge) ---------- def build_community_graph() -&gt; Graph:     \"\"\"Two undirected cliques: K6 on {a..f} and K4 on {w,x,y,z}, joined by a single weak bridge e--x (weight=0.01).     multilevel (Louvain-like) should give communities of sizes [4,6] (stable with seed);     betweenness top in {'e','x'}; PageRank (PR) top is 'e'.     \"\"\"     G = Graph(directed=True)  # add undirected edges explicitly      for v in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"w\", \"x\", \"y\", \"z\"]:         G.add_vertex(v)      # K6 clique on a..f (undirected, weight=1)     k6 = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]     for i in range(len(k6)):         for j in range(i + 1, len(k6)):             G.add_edge(k6[i], k6[j], weight=1, edge_directed=False)      # K4 clique on w,x,y,z (undirected, weight=1)     k4 = [\"w\", \"x\", \"y\", \"z\"]     for i in range(len(k4)):         for j in range(i + 1, len(k4)):             G.add_edge(k4[i], k4[j], weight=1, edge_directed=False)      # Single weak bridge e--x     G.add_edge(\"e\", \"x\", weight=0.01, edge_directed=False)     return G   # ---------- helpers ---------- def _unwrap_ig_distance(obj):     \"\"\"Igraph returns [[dist]] for single source/target; unwrap to a scalar.\"\"\"     if isinstance(obj, (list, tuple)) and len(obj) == 1:         inner = obj[0]         if isinstance(inner, (list, tuple)) and len(inner) == 1:             return inner[0]         return inner     return obj   def run_tests():     # ----- G1: shortest paths -----     G1 = build_path_graph()      # Weighted Dijkstra via labels -&gt; expect 11.0     dist_w = G1.ig.shortest_paths_dijkstra(         source=\"alpha\", target=\"phi\", weights=\"weight\", _ig_guess_labels=False     )     dist_w = _unwrap_ig_distance(dist_w)     print(\"[IG:G1 dijkstra weighted] alpha-&gt;phi:\", dist_w, \"(expect 11.0)\")      # Unweighted hop count -&gt; expect 5 (alpha\u2192phi)     dist_hops = G1.ig.distances(source=\"alpha\", target=\"phi\", weights=None, _ig_guess_labels=False)     dist_hops = _unwrap_ig_distance(dist_hops)     print(\"[IG:G1 unweighted hops] alpha-&gt;phi:\", dist_hops, \"(expect 5)\")      # Cache invalidation: add fast a-&gt;f edge (weight=2) and re-check -&gt; expect 2.0     G1.add_edge(\"a\", \"f\", weight=2)     dist_new = G1.ig.shortest_paths_dijkstra(         source=\"alpha\", target=\"phi\", weights=\"weight\", _ig_guess_labels=False     )     dist_new = _unwrap_ig_distance(dist_new)     print(\"[IG:G1 after mutation] alpha-&gt;phi:\", dist_new, \"(expect 2.0)\")      # ----- G2: communities / betweenness / PR / components -----     G2 = build_community_graph()      # multilevel (Louvain-like) on undirected view -&gt; expect sizes [4, 6]     vc = G2.ig.community_multilevel(weights=\"weight\", _ig_directed=False)     sizes = sorted(vc.sizes())     print(\"[IG:G2 multilevel] sizes:\", sizes, \"(expect [4, 6])\")      # Betweenness centrality (undirected). igraph returns list aligned to vertex order.     igG_und = G2.ig.backend(directed=False)  # no simple=True needed here     names = (         igG_und.vs[\"name\"] if \"name\" in igG_und.vs.attributes() else list(range(igG_und.vcount()))     )     bc_vals = G2.ig.betweenness(directed=False, weights=None)     top_bc = max(dict(zip(names, bc_vals)), key=lambda k: dict(zip(names, bc_vals))[k])     print(\"[IG:G2 betweenness] top:\", top_bc, \"(expect 'e' or 'x')\")      # PageRank (undirected, unweighted) -&gt; 'e' should dominate     pr_vals = G2.ig.pagerank(directed=False)     top_pr = max(dict(zip(names, pr_vals)), key=lambda k: dict(zip(names, pr_vals))[k])     print(\"[IG:G2 pagerank] top:\", top_pr, \"(expect 'e')\")      # Connected components (undirected) -&gt; single component of size 10     comps = G2.ig.components(_ig_directed=False)  # VertexClustering     comp_sizes = sorted(comps.sizes())     print(\"[IG:G2 connected components]:\", comp_sizes, \"(expect [10])\")      # ----- Optional: verify simple collapse + aggregation policy -----     G3 = Graph(directed=True)     G3.add_vertex(\"u\")     G3.add_vertex(\"v\")     # parallel undirected edges with attrs     G3.add_edge(\"u\", \"v\", weight=5, capacity=3, edge_directed=False)     G3.add_edge(\"u\", \"v\", weight=2, capacity=7, edge_directed=False)      try:         # Preferred: via proxy (needs the one-line proxy patch shown above)         igG_simple = G3.ig.backend(             directed=False,             simple=True,             needed_attrs={\"weight\", \"capacity\"},             edge_aggs={\"weight\": \"min\", \"capacity\": \"sum\"},         )         e = igG_simple.es[0]         print(             \"[IG:G3 collapse agg via proxy] weight:\",             e[\"weight\"],             \"capacity:\",             e[\"capacity\"],             \"(expect 2, 10)\",         )     except Exception:         # Fallback: ensure attrs exist, then collapse using igraph.simplify         igG_raw = G3.ig.backend(directed=False, needed_attrs={\"weight\", \"capacity\"})         igG_raw.simplify(             multiple=True, loops=True, combine_edges={\"weight\": \"min\", \"capacity\": \"sum\"}         )         e = igG_raw.es[0]         # after simplify, attrs may be missing if combine didn't carry them; handle safely         w = e[\"weight\"] if \"weight\" in igG_raw.es.attributes() else None         c = e[\"capacity\"] if \"capacity\" in igG_raw.es.attributes() else None         print(\"[IG:G3 collapse agg via simplify] weight:\", w, \"capacity:\", c, \"(expect 2, 10)\")   if __name__ == \"__main__\":     run_tests() <pre>[IG:G1 dijkstra weighted] alpha-&gt;phi: 11.0 (expect 11.0)\n[IG:G1 unweighted hops] alpha-&gt;phi: 5 (expect 5)\n[IG:G1 after mutation] alpha-&gt;phi: 2.0 (expect 2.0)\n[IG:G2 multilevel] sizes: [4, 6] (expect [4, 6])\n[IG:G2 betweenness] top: e (expect 'e' or 'x')\n[IG:G2 pagerank] top: e (expect 'e')\n[IG:G2 connected components]: [10] (expect [10])\n[IG:G3 collapse agg via proxy] weight: 2 capacity: 10 (expect 2, 10)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"sbml_io/","title":"SBML adapter (Elowitz repressilator)","text":"In\u00a0[4]: Copied! <pre>import os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\"..\"))\n\nfrom datetime import datetime\n\nimport polars as pl\n\nfrom annnet.adapters.sbml_adapter import BOUNDARY_SINK, BOUNDARY_SOURCE, from_sbml\nfrom annnet.core.graph import Graph\n</pre> import os import sys  sys.path.insert(0, os.path.abspath(\"..\"))  from datetime import datetime  import polars as pl  from annnet.adapters.sbml_adapter import BOUNDARY_SINK, BOUNDARY_SOURCE, from_sbml from annnet.core.graph import Graph In\u00a0[7]: Copied! <pre>G = from_sbml(\"Elowitz.sbml.xml\", graph=Graph(directed=True), preserve_stoichiometry=True)\n\nprint(\"vertices:\", G.num_vertices)  # expect 8 (6 real + 2 boundary)\nprint(\"edges:\", G.num_edges)  # expect 12\nprint(\"boundary nodes:\", BOUNDARY_SOURCE in G.entity_to_idx, BOUNDARY_SINK in G.entity_to_idx)\nprint(\"sample edges:\", list(G.edge_to_idx)[:5])\n</pre> G = from_sbml(\"Elowitz.sbml.xml\", graph=Graph(directed=True), preserve_stoichiometry=True)  print(\"vertices:\", G.num_vertices)  # expect 8 (6 real + 2 boundary) print(\"edges:\", G.num_edges)  # expect 12 print(\"boundary nodes:\", BOUNDARY_SOURCE in G.entity_to_idx, BOUNDARY_SINK in G.entity_to_idx) print(\"sample edges:\", list(G.edge_to_idx)[:5]) <pre>Model does not contain SBML fbc package information.\nSBML package 'layout' not supported by cobrapy, information is not parsed\nSBML package 'render' not supported by cobrapy, information is not parsed\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction1 \"degradation of LacI transcripts\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction1 \"degradation of LacI transcripts\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction2 \"degradation of TetR transcripts\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction2 \"degradation of TetR transcripts\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction3 \"degradation of CI transcripts\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction3 \"degradation of CI transcripts\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction4 \"translation of LacI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction4 \"translation of LacI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction5 \"translation of TetR\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction5 \"translation of TetR\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction6 \"translation of CI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction6 \"translation of CI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction7 \"degradation of LacI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction7 \"degradation of LacI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction8 \"degradation of TetR\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction8 \"degradation of TetR\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction9 \"degradation of CI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction9 \"degradation of CI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction10 \"transcription of LacI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction10 \"transcription of LacI\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction11 \"transcription of TetR\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction11 \"transcription of TetR\"&gt;'\nMissing lower flux bound set to '-1000.0' for reaction: '&lt;Reaction Reaction12 \"transcription of CI\"&gt;'\nMissing upper flux bound set to '1000.0' for reaction: '&lt;Reaction Reaction12 \"transcription of CI\"&gt;'\nNo objective coefficients in model. Unclear what should be optimized\nMissing flux bounds on reactions set to default bounds.As best practise and to avoid confusion flux bounds should be set explicitly on all reactions.\n</pre> <pre>vertices: 8\nedges: 12\nboundary nodes: True True\nsample edges: ['Reaction1', 'Reaction2', 'Reaction3', 'Reaction4', 'Reaction5']\n</pre> In\u00a0[9]: Copied! <pre>def show_reaction(eid: str):\n    if eid not in G.edge_to_idx:\n        print(\"no such edge:\", eid)\n        return\n    h = G.hyperedge_definitions[eid]\n    attrs = G.get_edge_attrs(eid)\n    sto = attrs.get(\n        \"stoich\"\n    )  # present if you didn't add set_hyperedge_coeffs OR adapter stored it anyway\n    print(f\"[{eid}]\")\n    print(\"  head (products):\", sorted(h[\"head\"]))\n    print(\"  tail (reactants):\", sorted(h[\"tail\"]))\n    if sto:\n        # filter zeros if any\n        sto = {k: float(v) for k, v in sto.items() if abs(float(v)) &gt; 1e-12}\n        print(\"  stoich map:\", sto)\n\n\n# examples\nshow_reaction(\"Reaction1\")\nshow_reaction(\"Reaction4\")\n</pre> def show_reaction(eid: str):     if eid not in G.edge_to_idx:         print(\"no such edge:\", eid)         return     h = G.hyperedge_definitions[eid]     attrs = G.get_edge_attrs(eid)     sto = attrs.get(         \"stoich\"     )  # present if you didn't add set_hyperedge_coeffs OR adapter stored it anyway     print(f\"[{eid}]\")     print(\"  head (products):\", sorted(h[\"head\"]))     print(\"  tail (reactants):\", sorted(h[\"tail\"]))     if sto:         # filter zeros if any         sto = {k: float(v) for k, v in sto.items() if abs(float(v)) &gt; 1e-12}         print(\"  stoich map:\", sto)   # examples show_reaction(\"Reaction1\") show_reaction(\"Reaction4\") <pre>[Reaction1]\n  head (products): ['__BOUNDARY_SINK__']\n  tail (reactants): ['X']\n[Reaction4]\n  head (products): ['PX']\n  tail (reactants): ['__BOUNDARY_SOURCE__']\n</pre> In\u00a0[11]: Copied! <pre>BOUNDARY = {BOUNDARY_SOURCE, BOUNDARY_SINK}\n\nproduced = dict.fromkeys(G.entity_to_idx, 0)\nconsumed = dict.fromkeys(G.entity_to_idx, 0)\n\nfor eid in G.edge_to_idx:\n    h = G.hyperedge_definitions[eid]\n    for v in h[\"head\"]:\n        produced[v] += 1\n    for v in h[\"tail\"]:\n        consumed[v] += 1\n\nreal_species = [v for v in G.entity_to_idx if v not in BOUNDARY]\nstats = [(v, produced[v], consumed[v]) for v in real_species]\nstats.sort(key=lambda t: (t[1], t[2]), reverse=True)\nfor v, p, c in stats:\n    print(f\"{v:&gt;4}  produced_in={p}  consumed_in={c}\")\n</pre> BOUNDARY = {BOUNDARY_SOURCE, BOUNDARY_SINK}  produced = dict.fromkeys(G.entity_to_idx, 0) consumed = dict.fromkeys(G.entity_to_idx, 0)  for eid in G.edge_to_idx:     h = G.hyperedge_definitions[eid]     for v in h[\"head\"]:         produced[v] += 1     for v in h[\"tail\"]:         consumed[v] += 1  real_species = [v for v in G.entity_to_idx if v not in BOUNDARY] stats = [(v, produced[v], consumed[v]) for v in real_species] stats.sort(key=lambda t: (t[1], t[2]), reverse=True) for v, p, c in stats:     print(f\"{v:&gt;4}  produced_in={p}  consumed_in={c}\") <pre>  PX  produced_in=1  consumed_in=1\n  PY  produced_in=1  consumed_in=1\n  PZ  produced_in=1  consumed_in=1\n   X  produced_in=1  consumed_in=1\n   Y  produced_in=1  consumed_in=1\n   Z  produced_in=1  consumed_in=1\n</pre> In\u00a0[13]: Copied! <pre>species_to_reactions = {v: {\"as_product\": [], \"as_reactant\": []} for v in G.entity_to_idx}\n\nfor eid in G.edge_to_idx:\n    h = G.hyperedge_definitions[eid]\n    for v in h[\"head\"]:\n        species_to_reactions[v][\"as_product\"].append(eid)\n    for v in h[\"tail\"]:\n        species_to_reactions[v][\"as_reactant\"].append(eid)\n\n# example: show for each real species\nfor v in real_species:\n    rp = species_to_reactions[v][\"as_product\"]\n    rr = species_to_reactions[v][\"as_reactant\"]\n    print(f\"\\n{v}\")\n    print(\"  as product :\", rp)\n    print(\"  as reactant:\", rr)\n</pre> species_to_reactions = {v: {\"as_product\": [], \"as_reactant\": []} for v in G.entity_to_idx}  for eid in G.edge_to_idx:     h = G.hyperedge_definitions[eid]     for v in h[\"head\"]:         species_to_reactions[v][\"as_product\"].append(eid)     for v in h[\"tail\"]:         species_to_reactions[v][\"as_reactant\"].append(eid)  # example: show for each real species for v in real_species:     rp = species_to_reactions[v][\"as_product\"]     rr = species_to_reactions[v][\"as_reactant\"]     print(f\"\\n{v}\")     print(\"  as product :\", rp)     print(\"  as reactant:\", rr) <pre>\nPX\n  as product : ['Reaction4']\n  as reactant: ['Reaction7']\n\nPY\n  as product : ['Reaction5']\n  as reactant: ['Reaction8']\n\nPZ\n  as product : ['Reaction6']\n  as reactant: ['Reaction9']\n\nX\n  as product : ['Reaction10']\n  as reactant: ['Reaction1']\n\nY\n  as product : ['Reaction11']\n  as reactant: ['Reaction2']\n\nZ\n  as product : ['Reaction12']\n  as reactant: ['Reaction3']\n</pre> In\u00a0[15]: Copied! <pre># signs: by definition head=products (+), tail=reactants (\u2212)\ndef signs_consistent(eid):\n    h = G.hyperedge_definitions[eid]\n    attrs = G.get_edge_attrs(eid)\n    sto = attrs.get(\"stoich\")\n    if not sto:\n        # no per-vertex coeffs exposed via attrs; just check sets are present\n        return bool(h[\"head\"] or h[\"tail\"])\n    # if stoich map exists, check sign consistency vs head/tail sets\n    ok = True\n    for v, coeff in sto.items():\n        coeff = float(coeff)\n        if v in h[\"head\"]:\n            ok &amp;= coeff &gt; 0\n        if v in h[\"tail\"]:\n            ok &amp;= coeff &lt; 0\n    return ok\n\n\nsign_ok_all = all(signs_consistent(e) for e in G.edge_to_idx)\nprint(\"signs consistent (based on available attrs):\", sign_ok_all)\n\n\n# balance: if stoich map exists, sum should be ~0 including boundary nodes\ndef balanced(eid):\n    attrs = G.get_edge_attrs(eid)\n    sto = attrs.get(\"stoich\")\n    if not sto:\n        return True  # can't check without exposed coeffs; treat as pass\n    s = sum(float(v) for v in sto.values())\n    return abs(s) &lt; 1e-9\n\n\nbal_ok_all = all(balanced(e) for e in G.edge_to_idx)\nprint(\"columns balanced (based on available attrs):\", bal_ok_all)\n</pre> # signs: by definition head=products (+), tail=reactants (\u2212) def signs_consistent(eid):     h = G.hyperedge_definitions[eid]     attrs = G.get_edge_attrs(eid)     sto = attrs.get(\"stoich\")     if not sto:         # no per-vertex coeffs exposed via attrs; just check sets are present         return bool(h[\"head\"] or h[\"tail\"])     # if stoich map exists, check sign consistency vs head/tail sets     ok = True     for v, coeff in sto.items():         coeff = float(coeff)         if v in h[\"head\"]:             ok &amp;= coeff &gt; 0         if v in h[\"tail\"]:             ok &amp;= coeff &lt; 0     return ok   sign_ok_all = all(signs_consistent(e) for e in G.edge_to_idx) print(\"signs consistent (based on available attrs):\", sign_ok_all)   # balance: if stoich map exists, sum should be ~0 including boundary nodes def balanced(eid):     attrs = G.get_edge_attrs(eid)     sto = attrs.get(\"stoich\")     if not sto:         return True  # can't check without exposed coeffs; treat as pass     s = sum(float(v) for v in sto.values())     return abs(s) &lt; 1e-9   bal_ok_all = all(balanced(e) for e in G.edge_to_idx) print(\"columns balanced (based on available attrs):\", bal_ok_all) <pre>signs consistent (based on available attrs): True\ncolumns balanced (based on available attrs): True\n</pre> In\u00a0[17]: Copied! <pre># degrees on your underlying NX projection (pass G explicitly)\ndeg = dict(G.nx.degree(G=G))\nprint(\"nx nodes:\", G.nx.number_of_nodes(G=G), \" nx edges:\", G.nx.number_of_edges(G=G))\nprint(\"top-degree nodes:\", sorted(deg.items(), key=lambda kv: kv[1], reverse=True)[:10])\n\n# simple paths (between two species if connected in your projection)\ntry:\n    path = G.nx.shortest_path(G=G, source=\"X\", target=\"PX\")  # tweak names if different\n    print(\"shortest path X\u2192PX:\", path)\nexcept Exception as e:\n    print(\"shortest_path failed:\", e)\n\n# cycles (directed projection)\ntry:\n    cyc = list(G.nx.simple_cycles(G=G))\n    print(\"cycles(count):\", len(cyc))\n    print(\"sample cycles:\", cyc[:3])\nexcept Exception as e:\n    print(\"simple_cycles failed:\", e)\n\n# neighbors / predecessors / successors\ntry:\n    print(\"neighbors(X):\", list(G.nx.neighbors(G=G, n=\"X\")))\nexcept Exception as e:\n    print(\"neighbors failed:\", e)\n\ntry:\n    print(\"successors(X):\", list(G.nx.successors(G=G, n=\"X\")))\n    print(\"predecessors(X):\", list(G.nx.predecessors(G=G, n=\"X\")))\nexcept Exception:\n    pass\n\n# connected components (weakly for directed; else undirected)\ntry:\n    comps = list(G.nx.weakly_connected_components(G=G))\n    print(\"weakly components:\", len(comps))\n    print(\"largest component size:\", max(len(c) for c in comps))\nexcept Exception:\n    try:\n        comps = list(G.nx.connected_components(G=G))\n        print(\"connected components:\", len(comps))\n        print(\"largest component size:\", max(len(c) for c in comps))\n    except Exception as e:\n        print(\"components failed:\", e)\n\n# degree centrality (works the same way)\ntry:\n    dc = G.nx.degree_centrality(G=G)\n    print(\"top degree_centrality:\", sorted(dc.items(), key=lambda kv: kv[1], reverse=True)[:5])\nexcept Exception as e:\n    print(\"degree_centrality failed:\", e)\n\n# species-only subgraph with the proxy (filters out boundary nodes)\nBOUNDARY = {\"__BOUNDARY_SOURCE__\", \"__BOUNDARY_SINK__\"}\ntry:\n    species = [n for n in G.nx.nodes(G=G) if n not in BOUNDARY]\n    SG = G.nx.subgraph(G=G, nbunch=species)  # returns an NX graph\n    print(\"species-subgraph nodes:\", SG.number_of_nodes(), \"edges:\", SG.number_of_edges())\nexcept Exception as e:\n    print(\"subgraph failed:\", e)\n</pre> # degrees on your underlying NX projection (pass G explicitly) deg = dict(G.nx.degree(G=G)) print(\"nx nodes:\", G.nx.number_of_nodes(G=G), \" nx edges:\", G.nx.number_of_edges(G=G)) print(\"top-degree nodes:\", sorted(deg.items(), key=lambda kv: kv[1], reverse=True)[:10])  # simple paths (between two species if connected in your projection) try:     path = G.nx.shortest_path(G=G, source=\"X\", target=\"PX\")  # tweak names if different     print(\"shortest path X\u2192PX:\", path) except Exception as e:     print(\"shortest_path failed:\", e)  # cycles (directed projection) try:     cyc = list(G.nx.simple_cycles(G=G))     print(\"cycles(count):\", len(cyc))     print(\"sample cycles:\", cyc[:3]) except Exception as e:     print(\"simple_cycles failed:\", e)  # neighbors / predecessors / successors try:     print(\"neighbors(X):\", list(G.nx.neighbors(G=G, n=\"X\"))) except Exception as e:     print(\"neighbors failed:\", e)  try:     print(\"successors(X):\", list(G.nx.successors(G=G, n=\"X\")))     print(\"predecessors(X):\", list(G.nx.predecessors(G=G, n=\"X\"))) except Exception:     pass  # connected components (weakly for directed; else undirected) try:     comps = list(G.nx.weakly_connected_components(G=G))     print(\"weakly components:\", len(comps))     print(\"largest component size:\", max(len(c) for c in comps)) except Exception:     try:         comps = list(G.nx.connected_components(G=G))         print(\"connected components:\", len(comps))         print(\"largest component size:\", max(len(c) for c in comps))     except Exception as e:         print(\"components failed:\", e)  # degree centrality (works the same way) try:     dc = G.nx.degree_centrality(G=G)     print(\"top degree_centrality:\", sorted(dc.items(), key=lambda kv: kv[1], reverse=True)[:5]) except Exception as e:     print(\"degree_centrality failed:\", e)  # species-only subgraph with the proxy (filters out boundary nodes) BOUNDARY = {\"__BOUNDARY_SOURCE__\", \"__BOUNDARY_SINK__\"} try:     species = [n for n in G.nx.nodes(G=G) if n not in BOUNDARY]     SG = G.nx.subgraph(G=G, nbunch=species)  # returns an NX graph     print(\"species-subgraph nodes:\", SG.number_of_nodes(), \"edges:\", SG.number_of_edges()) except Exception as e:     print(\"subgraph failed:\", e) <pre>nx nodes: 8  nx edges: 12\ntop-degree nodes: [('__BOUNDARY_SOURCE__', 6), ('__BOUNDARY_SINK__', 6), ('PX', 2), ('PY', 2), ('PZ', 2), ('X', 2), ('Y', 2), ('Z', 2)]\nshortest_path failed: No path between X and PX.\ncycles(count): 0\nsample cycles: []\nneighbors(X): ['__BOUNDARY_SOURCE__']\nweakly components: 1\nlargest component size: 8\ntop degree_centrality: [('__BOUNDARY_SOURCE__', 0.8571428571428571), ('__BOUNDARY_SINK__', 0.8571428571428571), ('PX', 0.2857142857142857), ('PY', 0.2857142857142857), ('PZ', 0.2857142857142857)]\nspecies-subgraph nodes: 6 edges: 0\n</pre> In\u00a0[20]: Copied! <pre># Create graph\nG = Graph(directed=True)\n\nprint(\"=\" * 60)\nprint(\"CREATING MULTI-LAYER TEMPORAL GRAPH\")\nprint(\"=\" * 60)\n\n# Add layers\nG.layers.add(\"2022\", year=2022, description=\"Year 2022\")\nG.layers.add(\"2023\", year=2023, description=\"Year 2023\")\nG.layers.add(\"2024\", year=2024, description=\"Year 2024\")\n\nprint(f\"\\n\u2713 Created {G.layers.count()} layers\")\nprint(f\"  Layers: {G.layers.list()}\")\n</pre> # Create graph G = Graph(directed=True)  print(\"=\" * 60) print(\"CREATING MULTI-LAYER TEMPORAL GRAPH\") print(\"=\" * 60)  # Add layers G.layers.add(\"2022\", year=2022, description=\"Year 2022\") G.layers.add(\"2023\", year=2023, description=\"Year 2023\") G.layers.add(\"2024\", year=2024, description=\"Year 2024\")  print(f\"\\n\u2713 Created {G.layers.count()} layers\") print(f\"  Layers: {G.layers.list()}\") <pre>============================================================\nCREATING MULTI-LAYER TEMPORAL GRAPH\n============================================================\n\n\u2713 Created 4 layers\n  Layers: ['2022', '2023', '2024']\n</pre> In\u00a0[22]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"LAYER 2022: Adding nodes\")\nprint(\"=\" * 60)\n\nG.layers.active = \"2022\"\n\n# Add people\npeople_2022 = {\n    \"alice\": {\"name\": \"Alice\", \"age\": 25, \"role\": \"engineer\", \"salary\": 80000},\n    \"bob\": {\"name\": \"Bob\", \"age\": 30, \"role\": \"manager\", \"salary\": 95000},\n    \"charlie\": {\"name\": \"Charlie\", \"age\": 28, \"role\": \"engineer\", \"salary\": 85000},\n    \"diana\": {\"name\": \"Diana\", \"age\": 35, \"role\": \"director\", \"salary\": 120000},\n}\n\nfor vid, attrs in people_2022.items():\n    G.add_vertex(vid, **attrs)\n\n# Add collaborations (edges)\ncollaborations_2022 = [\n    (\"alice\", \"bob\", 0.8, {\"project\": \"ProjectX\", \"hours\": 120}),\n    (\"alice\", \"charlie\", 0.9, {\"project\": \"ProjectX\", \"hours\": 150}),\n    (\"bob\", \"diana\", 0.7, {\"project\": \"Management\", \"hours\": 80}),\n    (\"charlie\", \"diana\", 0.6, {\"project\": \"ProjectY\", \"hours\": 60}),\n]\n\nfor source, target, weight, attrs in collaborations_2022:\n    G.add_edge(source, target, weight=weight, **attrs)\n\nprint(\"\\n\u2713 Layer 2022:\")\nprint(f\"  Vertices: {G.number_of_vertices()}\")\nprint(f\"  Edges: {G.number_of_edges()}\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"LAYER 2022: Adding nodes\") print(\"=\" * 60)  G.layers.active = \"2022\"  # Add people people_2022 = {     \"alice\": {\"name\": \"Alice\", \"age\": 25, \"role\": \"engineer\", \"salary\": 80000},     \"bob\": {\"name\": \"Bob\", \"age\": 30, \"role\": \"manager\", \"salary\": 95000},     \"charlie\": {\"name\": \"Charlie\", \"age\": 28, \"role\": \"engineer\", \"salary\": 85000},     \"diana\": {\"name\": \"Diana\", \"age\": 35, \"role\": \"director\", \"salary\": 120000}, }  for vid, attrs in people_2022.items():     G.add_vertex(vid, **attrs)  # Add collaborations (edges) collaborations_2022 = [     (\"alice\", \"bob\", 0.8, {\"project\": \"ProjectX\", \"hours\": 120}),     (\"alice\", \"charlie\", 0.9, {\"project\": \"ProjectX\", \"hours\": 150}),     (\"bob\", \"diana\", 0.7, {\"project\": \"Management\", \"hours\": 80}),     (\"charlie\", \"diana\", 0.6, {\"project\": \"ProjectY\", \"hours\": 60}), ]  for source, target, weight, attrs in collaborations_2022:     G.add_edge(source, target, weight=weight, **attrs)  print(\"\\n\u2713 Layer 2022:\") print(f\"  Vertices: {G.number_of_vertices()}\") print(f\"  Edges: {G.number_of_edges()}\") <pre>\n============================================================\nLAYER 2022: Adding nodes\n============================================================\n\n\u2713 Layer 2022:\n  Vertices: 4\n  Edges: 4\n</pre> In\u00a0[24]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"LAYER 2023: Adding nodes and edges\")\nprint(\"=\" * 60)\n\nG.layers.active = \"2023\"\n\n# Add existing people (some with updated attributes)\npeople_2023 = {\n    \"alice\": {\"name\": \"Alice\", \"age\": 26, \"role\": \"senior_engineer\", \"salary\": 92000},\n    \"bob\": {\"name\": \"Bob\", \"age\": 31, \"role\": \"senior_manager\", \"salary\": 105000},\n    \"charlie\": {\"name\": \"Charlie\", \"age\": 29, \"role\": \"engineer\", \"salary\": 88000},\n    \"diana\": {\"name\": \"Diana\", \"age\": 36, \"role\": \"director\", \"salary\": 125000},\n    \"eve\": {\"name\": \"Eve\", \"age\": 27, \"role\": \"engineer\", \"salary\": 83000},  # New hire\n}\n\nfor vid, attrs in people_2023.items():\n    if not G.has_vertex(vid):\n        G.add_vertex(vid, **attrs)\n\n# New collaborations\ncollaborations_2023 = [\n    (\"alice\", \"bob\", 0.85, {\"project\": \"ProjectZ\", \"hours\": 140}),\n    (\"alice\", \"eve\", 0.95, {\"project\": \"ProjectZ\", \"hours\": 180}),  # New collaboration\n    (\"bob\", \"diana\", 0.75, {\"project\": \"Management\", \"hours\": 90}),\n    (\"charlie\", \"eve\", 0.8, {\"project\": \"ProjectW\", \"hours\": 100}),\n    (\"eve\", \"diana\", 0.7, {\"project\": \"ProjectW\", \"hours\": 70}),\n]\n\nfor source, target, weight, attrs in collaborations_2023:\n    G.add_edge(source, target, weight=weight, **attrs)\n\nprint(\"\\n\u2713 Layer 2023:\")\nprint(f\"  Total vertices: {G.number_of_vertices()}\")\nprint(f'  Edges in layer: {len(G.layers.edges(\"2023\"))}')\n</pre> print(\"\\n\" + \"=\" * 60) print(\"LAYER 2023: Adding nodes and edges\") print(\"=\" * 60)  G.layers.active = \"2023\"  # Add existing people (some with updated attributes) people_2023 = {     \"alice\": {\"name\": \"Alice\", \"age\": 26, \"role\": \"senior_engineer\", \"salary\": 92000},     \"bob\": {\"name\": \"Bob\", \"age\": 31, \"role\": \"senior_manager\", \"salary\": 105000},     \"charlie\": {\"name\": \"Charlie\", \"age\": 29, \"role\": \"engineer\", \"salary\": 88000},     \"diana\": {\"name\": \"Diana\", \"age\": 36, \"role\": \"director\", \"salary\": 125000},     \"eve\": {\"name\": \"Eve\", \"age\": 27, \"role\": \"engineer\", \"salary\": 83000},  # New hire }  for vid, attrs in people_2023.items():     if not G.has_vertex(vid):         G.add_vertex(vid, **attrs)  # New collaborations collaborations_2023 = [     (\"alice\", \"bob\", 0.85, {\"project\": \"ProjectZ\", \"hours\": 140}),     (\"alice\", \"eve\", 0.95, {\"project\": \"ProjectZ\", \"hours\": 180}),  # New collaboration     (\"bob\", \"diana\", 0.75, {\"project\": \"Management\", \"hours\": 90}),     (\"charlie\", \"eve\", 0.8, {\"project\": \"ProjectW\", \"hours\": 100}),     (\"eve\", \"diana\", 0.7, {\"project\": \"ProjectW\", \"hours\": 70}), ]  for source, target, weight, attrs in collaborations_2023:     G.add_edge(source, target, weight=weight, **attrs)  print(\"\\n\u2713 Layer 2023:\") print(f\"  Total vertices: {G.number_of_vertices()}\") print(f'  Edges in layer: {len(G.layers.edges(\"2023\"))}') <pre>\n============================================================\nLAYER 2023: Adding nodes and edges\n============================================================\n\n\u2713 Layer 2023:\n  Total vertices: 5\n  Edges in layer: 5\n</pre> In\u00a0[26]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"LAYER 2024: Adding nodes and edges\")\nprint(\"=\" * 60)\n\nG.layers.active = \"2024\"\n\n# 2024 people (Bob left, Frank joined)\npeople_2024 = {\n    \"alice\": {\"name\": \"Alice\", \"age\": 27, \"role\": \"tech_lead\", \"salary\": 110000},\n    \"charlie\": {\"name\": \"Charlie\", \"age\": 30, \"role\": \"senior_engineer\", \"salary\": 98000},\n    \"diana\": {\"name\": \"Diana\", \"age\": 37, \"role\": \"vp\", \"salary\": 150000},\n    \"eve\": {\"name\": \"Eve\", \"age\": 28, \"role\": \"senior_engineer\", \"salary\": 95000},\n    \"frank\": {\"name\": \"Frank\", \"age\": 32, \"role\": \"manager\", \"salary\": 100000},  # Replaced Bob\n}\n\nfor vid, attrs in people_2024.items():\n    if not G.has_vertex(vid):\n        G.add_vertex(vid, **attrs)\n\n# 2024 collaborations\ncollaborations_2024 = [\n    (\"alice\", \"frank\", 0.9, {\"project\": \"NextGen\", \"hours\": 160}),\n    (\"alice\", \"eve\", 0.92, {\"project\": \"NextGen\", \"hours\": 170}),\n    (\"charlie\", \"eve\", 0.85, {\"project\": \"NextGen\", \"hours\": 120}),\n    (\"frank\", \"diana\", 0.8, {\"project\": \"Strategy\", \"hours\": 100}),\n    (\"eve\", \"diana\", 0.75, {\"project\": \"Strategy\", \"hours\": 80}),\n]\n\nfor source, target, weight, attrs in collaborations_2024:\n    G.add_edge(source, target, weight=weight, **attrs)\n\nprint(\"\\n\u2713 Layer 2024:\")\nprint(f\"  Total vertices: {G.number_of_vertices()}\")\nprint(f'  Edges in layer: {len(G.layers.edges(\"2024\"))}')\n\nprint(f'\\n{\"=\" * 60}')\nprint(\"GRAPH SUMMARY\")\nprint(f'{\"=\" * 60}')\nprint(f\"Total unique vertices: {G.number_of_vertices()}\")\nprint(f\"Total unique edges: {G.number_of_edges()}\")\nprint(f\"Layers: {G.layers.count()}\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"LAYER 2024: Adding nodes and edges\") print(\"=\" * 60)  G.layers.active = \"2024\"  # 2024 people (Bob left, Frank joined) people_2024 = {     \"alice\": {\"name\": \"Alice\", \"age\": 27, \"role\": \"tech_lead\", \"salary\": 110000},     \"charlie\": {\"name\": \"Charlie\", \"age\": 30, \"role\": \"senior_engineer\", \"salary\": 98000},     \"diana\": {\"name\": \"Diana\", \"age\": 37, \"role\": \"vp\", \"salary\": 150000},     \"eve\": {\"name\": \"Eve\", \"age\": 28, \"role\": \"senior_engineer\", \"salary\": 95000},     \"frank\": {\"name\": \"Frank\", \"age\": 32, \"role\": \"manager\", \"salary\": 100000},  # Replaced Bob }  for vid, attrs in people_2024.items():     if not G.has_vertex(vid):         G.add_vertex(vid, **attrs)  # 2024 collaborations collaborations_2024 = [     (\"alice\", \"frank\", 0.9, {\"project\": \"NextGen\", \"hours\": 160}),     (\"alice\", \"eve\", 0.92, {\"project\": \"NextGen\", \"hours\": 170}),     (\"charlie\", \"eve\", 0.85, {\"project\": \"NextGen\", \"hours\": 120}),     (\"frank\", \"diana\", 0.8, {\"project\": \"Strategy\", \"hours\": 100}),     (\"eve\", \"diana\", 0.75, {\"project\": \"Strategy\", \"hours\": 80}), ]  for source, target, weight, attrs in collaborations_2024:     G.add_edge(source, target, weight=weight, **attrs)  print(\"\\n\u2713 Layer 2024:\") print(f\"  Total vertices: {G.number_of_vertices()}\") print(f'  Edges in layer: {len(G.layers.edges(\"2024\"))}')  print(f'\\n{\"=\" * 60}') print(\"GRAPH SUMMARY\") print(f'{\"=\" * 60}') print(f\"Total unique vertices: {G.number_of_vertices()}\") print(f\"Total unique edges: {G.number_of_edges()}\") print(f\"Layers: {G.layers.count()}\") <pre>\n============================================================\nLAYER 2024: Adding nodes and edges\n============================================================\n\n\u2713 Layer 2024:\n  Total vertices: 6\n  Edges in layer: 5\n\n============================================================\nGRAPH SUMMARY\n============================================================\nTotal unique vertices: 6\nTotal unique edges: 14\nLayers: 4\n</pre> In\u00a0[28]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING ANNNET PROPERTIES\")\nprint(\"=\" * 60)\n\n# Test obs (vertex attributes)\nprint(\"\\n1. obs (vertex attributes):\")\nprint(G.obs)\nprint(f\"\\n   Shape: {G.obs.shape}\")\nprint(f\"   Columns: {G.obs.columns}\")\n\n# Test var (edge attributes)\nprint(\"\\n2. var (edge attributes):\")\nprint(G.var.head())\nprint(f\"\\n   Shape: {G.var.shape}\")\nprint(f\"   Columns: {G.var.columns}\")\n\n# Test X (incidence matrix)\nprint(\"\\n3. X (incidence matrix):\")\nX = G.X()\nprint(f\"   Type: {type(X)}\")\nprint(f\"   Shape: {X.shape}\")\nprint(f\"   Non-zero entries: {X.nnz}\")\nprint(f\"   Density: {X.nnz / (X.shape[0] * X.shape[1]):.4f}\")\n\n# Test uns (unstructured metadata)\nprint(\"\\n4. uns (unstructured metadata):\")\nG.uns[\"dataset_name\"] = \"Company Collaboration Network\"\nG.uns[\"created\"] = datetime.now().isoformat()\nG.uns[\"description\"] = \"Multi-year collaboration network\"\nprint(f\"   {G.uns}\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING ANNNET PROPERTIES\") print(\"=\" * 60)  # Test obs (vertex attributes) print(\"\\n1. obs (vertex attributes):\") print(G.obs) print(f\"\\n   Shape: {G.obs.shape}\") print(f\"   Columns: {G.obs.columns}\")  # Test var (edge attributes) print(\"\\n2. var (edge attributes):\") print(G.var.head()) print(f\"\\n   Shape: {G.var.shape}\") print(f\"   Columns: {G.var.columns}\")  # Test X (incidence matrix) print(\"\\n3. X (incidence matrix):\") X = G.X() print(f\"   Type: {type(X)}\") print(f\"   Shape: {X.shape}\") print(f\"   Non-zero entries: {X.nnz}\") print(f\"   Density: {X.nnz / (X.shape[0] * X.shape[1]):.4f}\")  # Test uns (unstructured metadata) print(\"\\n4. uns (unstructured metadata):\") G.uns[\"dataset_name\"] = \"Company Collaboration Network\" G.uns[\"created\"] = datetime.now().isoformat() G.uns[\"description\"] = \"Multi-year collaboration network\" print(f\"   {G.uns}\") <pre>\n============================================================\nTESTING ANNNET PROPERTIES\n============================================================\n\n1. obs (vertex attributes):\nshape: (6, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vertex_id \u2506 name    \u2506 age \u2506 role     \u2506 salary \u2502\n\u2502 ---       \u2506 ---     \u2506 --- \u2506 ---      \u2506 ---    \u2502\n\u2502 str       \u2506 str     \u2506 i64 \u2506 str      \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 alice     \u2506 Alice   \u2506 25  \u2506 engineer \u2506 80000  \u2502\n\u2502 bob       \u2506 Bob     \u2506 30  \u2506 manager  \u2506 95000  \u2502\n\u2502 charlie   \u2506 Charlie \u2506 28  \u2506 engineer \u2506 85000  \u2502\n\u2502 diana     \u2506 Diana   \u2506 35  \u2506 director \u2506 120000 \u2502\n\u2502 eve       \u2506 Eve     \u2506 27  \u2506 engineer \u2506 83000  \u2502\n\u2502 frank     \u2506 Frank   \u2506 32  \u2506 manager  \u2506 100000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n   Shape: (6, 5)\n   Columns: ['vertex_id', 'name', 'age', 'role', 'salary']\n\n2. var (edge attributes):\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id \u2506 project    \u2506 hours \u2502\n\u2502 ---     \u2506 ---        \u2506 ---   \u2502\n\u2502 str     \u2506 str        \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_0  \u2506 ProjectX   \u2506 120   \u2502\n\u2502 edge_1  \u2506 ProjectX   \u2506 150   \u2502\n\u2502 edge_2  \u2506 Management \u2506 80    \u2502\n\u2502 edge_3  \u2506 ProjectY   \u2506 60    \u2502\n\u2502 edge_4  \u2506 ProjectZ   \u2506 140   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n   Shape: (14, 3)\n   Columns: ['edge_id', 'project', 'hours']\n\n3. X (incidence matrix):\n   Type: &lt;class 'scipy.sparse._dok.dok_matrix'&gt;\n   Shape: (8, 16)\n   Non-zero entries: 28\n   Density: 0.2188\n\n4. uns (unstructured metadata):\n   {'dataset_name': 'Company Collaboration Network', 'created': '2025-10-28T21:08:39.405270', 'description': 'Multi-year collaboration network'}\n</pre> In\u00a0[30]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING LAYERMANAGER\")\nprint(\"=\" * 60)\n\n# Basic operations\nprint(\"\\n1. Layer Info:\")\nprint(f\"   Active layer: {G.layers.active}\")\nprint(f\"   All layers: {G.layers.list()}\")\nprint(f\"   Layer count: {G.layers.count()}\")\n\n# Layer statistics\nprint(\"\\n2. Layer Statistics:\")\nstats = G.layers.stats()\nfor layer_id, info in stats.items():\n    print(f\"\\n   {layer_id}:\")\n    print(f'     Vertices: {info[\"vertices\"]}')\n    print(f'     Edges: {info[\"edges\"]}')\n    print(f'     Attributes: {info[\"attributes\"]}')\n\n# Layer operations - union\nprint(\"\\n3. Union of 2022 and 2023:\")\nunion_result = G.layers.union([\"2022\", \"2023\"])\nprint(f'   Vertices: {len(union_result[\"vertices\"])}')\nprint(f'   Edges: {len(union_result[\"edges\"])}')\nprint(f'   Vertex IDs: {sorted(union_result[\"vertices\"])}')\n\n# Layer operations - intersection\nprint(\"\\n4. Intersection of 2022 and 2023:\")\nintersect_result = G.layers.intersect([\"2022\", \"2023\"])\nprint(f'   Common vertices: {sorted(intersect_result[\"vertices\"])}')\nprint(f'   Common edges: {len(intersect_result[\"edges\"])}')\n\n# Create aggregated layer\nprint(\"\\n5. Create 'all_years' layer (union):\")\nG.layers.union_create([\"2022\", \"2023\", \"2024\"], \"all_years\", description=\"All years combined\")\nprint(\"   \u2713 Created layer: all_years\")\nprint(f'   Vertices: {len(G.layers.vertices(\"all_years\"))}')\nprint(f'   Edges: {len(G.layers.edges(\"all_years\"))}')\n\n# Summary\nprint(\"\\n6. Layer Summary:\")\nprint(G.layers.summary())\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING LAYERMANAGER\") print(\"=\" * 60)  # Basic operations print(\"\\n1. Layer Info:\") print(f\"   Active layer: {G.layers.active}\") print(f\"   All layers: {G.layers.list()}\") print(f\"   Layer count: {G.layers.count()}\")  # Layer statistics print(\"\\n2. Layer Statistics:\") stats = G.layers.stats() for layer_id, info in stats.items():     print(f\"\\n   {layer_id}:\")     print(f'     Vertices: {info[\"vertices\"]}')     print(f'     Edges: {info[\"edges\"]}')     print(f'     Attributes: {info[\"attributes\"]}')  # Layer operations - union print(\"\\n3. Union of 2022 and 2023:\") union_result = G.layers.union([\"2022\", \"2023\"]) print(f'   Vertices: {len(union_result[\"vertices\"])}') print(f'   Edges: {len(union_result[\"edges\"])}') print(f'   Vertex IDs: {sorted(union_result[\"vertices\"])}')  # Layer operations - intersection print(\"\\n4. Intersection of 2022 and 2023:\") intersect_result = G.layers.intersect([\"2022\", \"2023\"]) print(f'   Common vertices: {sorted(intersect_result[\"vertices\"])}') print(f'   Common edges: {len(intersect_result[\"edges\"])}')  # Create aggregated layer print(\"\\n5. Create 'all_years' layer (union):\") G.layers.union_create([\"2022\", \"2023\", \"2024\"], \"all_years\", description=\"All years combined\") print(\"   \u2713 Created layer: all_years\") print(f'   Vertices: {len(G.layers.vertices(\"all_years\"))}') print(f'   Edges: {len(G.layers.edges(\"all_years\"))}')  # Summary print(\"\\n6. Layer Summary:\") print(G.layers.summary()) <pre>\n============================================================\nTESTING LAYERMANAGER\n============================================================\n\n1. Layer Info:\n   Active layer: 2024\n   All layers: ['2022', '2023', '2024']\n   Layer count: 4\n\n2. Layer Statistics:\n\n   2022:\n     Vertices: 4\n     Edges: 4\n     Attributes: {'year': 2022, 'description': 'Year 2022'}\n\n   2023:\n     Vertices: 5\n     Edges: 5\n     Attributes: {'year': 2023, 'description': 'Year 2023'}\n\n   2024:\n     Vertices: 5\n     Edges: 5\n     Attributes: {'year': 2024, 'description': 'Year 2024'}\n\n3. Union of 2022 and 2023:\n   Vertices: 5\n   Edges: 9\n   Vertex IDs: ['alice', 'bob', 'charlie', 'diana', 'eve']\n\n4. Intersection of 2022 and 2023:\n   Common vertices: ['alice', 'bob', 'charlie', 'diana']\n   Common edges: 0\n\n5. Create 'all_years' layer (union):\n   \u2713 Created layer: all_years\n   Vertices: 6\n   Edges: 14\n\n6. Layer Summary:\nLayers: 5\n\u251c\u2500 default: 0 vertices, 0 edges\n\u251c\u2500 2022: 4 vertices, 4 edges\n\u251c\u2500 2023: 5 vertices, 5 edges\n\u251c\u2500 2024: 5 vertices, 5 edges\n\u2514\u2500 all_years: 6 vertices, 14 edges\n</pre> In\u00a0[32]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING CROSS-LAYER ANALYTICS\")\nprint(\"=\" * 60)\n\n# Vertex presence\nprint(\"\\n1. Vertex Presence Across Layers:\")\nfor vid in [\"alice\", \"bob\", \"eve\", \"frank\"]:\n    layers = G.layers.vertex_presence(vid)\n    print(f\"   {vid}: {layers}\")\n\n# Edge presence\nprint(\"\\n2. Edge Presence (alice\u2192bob):\")\nedge_presence = G.layers.edge_presence(source=\"alice\", target=\"bob\")\nfor layer_id, edge_ids in edge_presence.items():\n    print(f\"   {layer_id}: {edge_ids}\")\n\n# Conserved edges\nprint(\"\\n3. Conserved Edges (in 2+ layers):\")\nconserved = G.layers.conserved_edges(min_layers=2)\nprint(f\"   Found {len(conserved)} conserved edges:\")\nfor eid, count in sorted(conserved.items(), key=lambda x: x[1], reverse=True)[:5]:\n    edge_def = G.edge_definitions.get(eid)\n    if edge_def:\n        print(f\"   {eid}: {edge_def[0]} \u2192 {edge_def[1]} (in {count} layers)\")\n\n# Layer-specific edges\nprint(\"\\n4. Layer-Specific Edges:\")\nfor layer_id in [\"2022\", \"2023\", \"2024\"]:\n    specific = G.layers.specific_edges(layer_id)\n    print(f\"   {layer_id} only: {len(specific)} edges\")\n\n# Temporal dynamics\nprint(\"\\n5. Temporal Dynamics:\")\nchanges = G.layers.temporal_dynamics([\"2022\", \"2023\", \"2024\"], metric=\"edge_change\")\nfor i, change in enumerate(changes):\n    year_from = [\"2022\", \"2023\"][i]\n    year_to = [\"2023\", \"2024\"][i]\n    print(f\"\\n   {year_from} \u2192 {year_to}:\")\n    print(f'     Edges added: {change[\"added\"]}')\n    print(f'     Edges removed: {change[\"removed\"]}')\n    print(f'     Net change: {change[\"net_change\"]:+d}')\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING CROSS-LAYER ANALYTICS\") print(\"=\" * 60)  # Vertex presence print(\"\\n1. Vertex Presence Across Layers:\") for vid in [\"alice\", \"bob\", \"eve\", \"frank\"]:     layers = G.layers.vertex_presence(vid)     print(f\"   {vid}: {layers}\")  # Edge presence print(\"\\n2. Edge Presence (alice\u2192bob):\") edge_presence = G.layers.edge_presence(source=\"alice\", target=\"bob\") for layer_id, edge_ids in edge_presence.items():     print(f\"   {layer_id}: {edge_ids}\")  # Conserved edges print(\"\\n3. Conserved Edges (in 2+ layers):\") conserved = G.layers.conserved_edges(min_layers=2) print(f\"   Found {len(conserved)} conserved edges:\") for eid, count in sorted(conserved.items(), key=lambda x: x[1], reverse=True)[:5]:     edge_def = G.edge_definitions.get(eid)     if edge_def:         print(f\"   {eid}: {edge_def[0]} \u2192 {edge_def[1]} (in {count} layers)\")  # Layer-specific edges print(\"\\n4. Layer-Specific Edges:\") for layer_id in [\"2022\", \"2023\", \"2024\"]:     specific = G.layers.specific_edges(layer_id)     print(f\"   {layer_id} only: {len(specific)} edges\")  # Temporal dynamics print(\"\\n5. Temporal Dynamics:\") changes = G.layers.temporal_dynamics([\"2022\", \"2023\", \"2024\"], metric=\"edge_change\") for i, change in enumerate(changes):     year_from = [\"2022\", \"2023\"][i]     year_to = [\"2023\", \"2024\"][i]     print(f\"\\n   {year_from} \u2192 {year_to}:\")     print(f'     Edges added: {change[\"added\"]}')     print(f'     Edges removed: {change[\"removed\"]}')     print(f'     Net change: {change[\"net_change\"]:+d}') <pre>\n============================================================\nTESTING CROSS-LAYER ANALYTICS\n============================================================\n\n1. Vertex Presence Across Layers:\n   alice: ['2022', '2023', '2024', 'all_years']\n   bob: ['2022', '2023', 'all_years']\n   eve: ['2023', '2024', 'all_years']\n   frank: ['2024', 'all_years']\n\n2. Edge Presence (alice\u2192bob):\n   2022: ['edge_0']\n   2023: ['edge_4']\n   all_years: ['edge_4', 'edge_0']\n\n3. Conserved Edges (in 2+ layers):\n   Found 14 conserved edges:\n   edge_1: alice \u2192 charlie (in 2 layers)\n   edge_2: bob \u2192 diana (in 2 layers)\n   edge_0: alice \u2192 bob (in 2 layers)\n   edge_3: charlie \u2192 diana (in 2 layers)\n   edge_4: alice \u2192 bob (in 2 layers)\n\n4. Layer-Specific Edges:\n   2022 only: 0 edges\n   2023 only: 0 edges\n   2024 only: 0 edges\n\n5. Temporal Dynamics:\n\n   2022 \u2192 2023:\n     Edges added: 5\n     Edges removed: 4\n     Net change: +1\n\n   2023 \u2192 2024:\n     Edges added: 5\n     Edges removed: 5\n     Net change: +0\n</pre> In\u00a0[34]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING INDEXMANAGER\")\nprint(\"=\" * 60)\n\n# Entity lookups\nprint(\"\\n1. Entity Index Lookups:\")\nprint(f'   alice \u2192 row index: {G.idx.entity_to_row(\"alice\")}')\nprint(f'   diana \u2192 row index: {G.idx.entity_to_row(\"diana\")}')\nprint(f\"   Row 0 \u2192 entity: {G.idx.row_to_entity(0)}\")\nprint(f\"   Row 3 \u2192 entity: {G.idx.row_to_entity(3)}\")\n\n# Edge lookups\nprint(\"\\n2. Edge Index Lookups:\")\nedge_ids = list(G.edge_to_idx.keys())[:3]\nfor eid in edge_ids:\n    col = G.idx.edge_to_col(eid)\n    back = G.idx.col_to_edge(col)\n    print(f\"   {eid} \u2192 col {col} \u2192 {back}\")\n\n# Batch lookups\nprint(\"\\n3. Batch Lookups:\")\nvertices = [\"alice\", \"bob\", \"charlie\"]\nrows = G.idx.entities_to_rows(vertices)\nprint(f\"   {vertices}\")\nprint(f\"   \u2192 rows: {rows}\")\nback_entities = G.idx.rows_to_entities(rows)\nprint(f\"   \u2192 back: {back_entities}\")\n\n# Check existence\nprint(\"\\n4. Existence Checks:\")\nprint(f\"   'alice' exists: {G.idx.has_entity('alice')}\")\nprint(f\"   'unknown' exists: {G.idx.has_entity('unknown')}\")\nprint(f\"   edge count: {G.idx.edge_count()}\")\nprint(f\"   entity count: {G.idx.entity_count()}\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING INDEXMANAGER\") print(\"=\" * 60)  # Entity lookups print(\"\\n1. Entity Index Lookups:\") print(f'   alice \u2192 row index: {G.idx.entity_to_row(\"alice\")}') print(f'   diana \u2192 row index: {G.idx.entity_to_row(\"diana\")}') print(f\"   Row 0 \u2192 entity: {G.idx.row_to_entity(0)}\") print(f\"   Row 3 \u2192 entity: {G.idx.row_to_entity(3)}\")  # Edge lookups print(\"\\n2. Edge Index Lookups:\") edge_ids = list(G.edge_to_idx.keys())[:3] for eid in edge_ids:     col = G.idx.edge_to_col(eid)     back = G.idx.col_to_edge(col)     print(f\"   {eid} \u2192 col {col} \u2192 {back}\")  # Batch lookups print(\"\\n3. Batch Lookups:\") vertices = [\"alice\", \"bob\", \"charlie\"] rows = G.idx.entities_to_rows(vertices) print(f\"   {vertices}\") print(f\"   \u2192 rows: {rows}\") back_entities = G.idx.rows_to_entities(rows) print(f\"   \u2192 back: {back_entities}\")  # Check existence print(\"\\n4. Existence Checks:\") print(f\"   'alice' exists: {G.idx.has_entity('alice')}\") print(f\"   'unknown' exists: {G.idx.has_entity('unknown')}\") print(f\"   edge count: {G.idx.edge_count()}\") print(f\"   entity count: {G.idx.entity_count()}\") <pre>\n============================================================\nTESTING INDEXMANAGER\n============================================================\n\n1. Entity Index Lookups:\n   alice \u2192 row index: 0\n   diana \u2192 row index: 3\n   Row 0 \u2192 entity: alice\n   Row 3 \u2192 entity: diana\n\n2. Edge Index Lookups:\n   edge_0 \u2192 col 0 \u2192 edge_0\n   edge_1 \u2192 col 1 \u2192 edge_1\n   edge_2 \u2192 col 2 \u2192 edge_2\n\n3. Batch Lookups:\n   ['alice', 'bob', 'charlie']\n   \u2192 rows: [0, 1, 2]\n   \u2192 back: ['alice', 'bob', 'charlie']\n\n4. Existence Checks:\n   'alice' exists: True\n   'unknown' exists: False\n   edge count: 14\n   entity count: 6\n</pre> In\u00a0[36]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING CACHEMANAGER\")\nprint(\"=\" * 60)\n\n# Check cache status\nprint(\"\\n1. Initial Cache Status:\")\nprint(f\"   CSR cached: {G.cache.has_csr()}\")\nprint(f\"   CSC cached: {G.cache.has_csc()}\")\n\n# Build CSR\nprint(\"\\n2. Building CSR cache...\")\nimport time\n\nt0 = time.time()\ncsr = G.cache.get_csr()\nt1 = time.time()\nprint(f\"   \u2713 Built in {(t1 - t0) * 1000:.2f}ms\")\nprint(f\"   Shape: {csr.shape}\")\nprint(f\"   Type: {type(csr)}\")\n\n# Build CSC\nprint(\"\\n3. Building CSC cache...\")\nt0 = time.time()\ncsc = G.cache.get_csc()\nt1 = time.time()\nprint(f\"   \u2713 Built in {(t1 - t0) * 1000:.2f}ms\")\nprint(f\"   Shape: {csc.shape}\")\n\n# Check cache hit\nprint(\"\\n4. Cache Hit Test:\")\nt0 = time.time()\ncsr2 = G.cache.get_csr()  # Should be instant\nt1 = time.time()\nprint(f\"   \u2713 Retrieved in {(t1 - t0) * 1000:.4f}ms (cached)\")\n\n# Clear cache\nprint(\"\\n5. Clearing Cache:\")\nG.cache.clear()\nprint(f\"   CSR cached: {G.cache.has_csr()}\")\nprint(f\"   CSC cached: {G.cache.has_csc()}\")\n\n# Rebuild\nprint(\"\\n6. Rebuild All:\")\nG.cache.build()\nprint(f\"   \u2713 CSR cached: {G.cache.has_csr()}\")\nprint(f\"   \u2713 CSC cached: {G.cache.has_csc()}\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING CACHEMANAGER\") print(\"=\" * 60)  # Check cache status print(\"\\n1. Initial Cache Status:\") print(f\"   CSR cached: {G.cache.has_csr()}\") print(f\"   CSC cached: {G.cache.has_csc()}\")  # Build CSR print(\"\\n2. Building CSR cache...\") import time  t0 = time.time() csr = G.cache.get_csr() t1 = time.time() print(f\"   \u2713 Built in {(t1 - t0) * 1000:.2f}ms\") print(f\"   Shape: {csr.shape}\") print(f\"   Type: {type(csr)}\")  # Build CSC print(\"\\n3. Building CSC cache...\") t0 = time.time() csc = G.cache.get_csc() t1 = time.time() print(f\"   \u2713 Built in {(t1 - t0) * 1000:.2f}ms\") print(f\"   Shape: {csc.shape}\")  # Check cache hit print(\"\\n4. Cache Hit Test:\") t0 = time.time() csr2 = G.cache.get_csr()  # Should be instant t1 = time.time() print(f\"   \u2713 Retrieved in {(t1 - t0) * 1000:.4f}ms (cached)\")  # Clear cache print(\"\\n5. Clearing Cache:\") G.cache.clear() print(f\"   CSR cached: {G.cache.has_csr()}\") print(f\"   CSC cached: {G.cache.has_csc()}\")  # Rebuild print(\"\\n6. Rebuild All:\") G.cache.build() print(f\"   \u2713 CSR cached: {G.cache.has_csr()}\") print(f\"   \u2713 CSC cached: {G.cache.has_csc()}\") <pre>\n============================================================\nTESTING CACHEMANAGER\n============================================================\n\n1. Initial Cache Status:\n   CSR cached: False\n   CSC cached: False\n\n2. Building CSR cache...\n   \u2713 Built in 0.00ms\n   Shape: (8, 16)\n   Type: &lt;class 'scipy.sparse._csr.csr_matrix'&gt;\n\n3. Building CSC cache...\n   \u2713 Built in 0.00ms\n   Shape: (8, 16)\n\n4. Cache Hit Test:\n   \u2713 Retrieved in 0.0000ms (cached)\n\n5. Clearing Cache:\n   CSR cached: False\n   CSC cached: False\n\n6. Rebuild All:\n   \u2713 CSR cached: True\n   \u2713 CSC cached: True\n</pre> In\u00a0[38]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING GRAPHVIEW - BASIC FILTERING\")\nprint(\"=\" * 60)\n\n# View specific nodes\nprint(\"\\n1. View Specific Nodes:\")\nv = G.view(nodes=[\"alice\", \"bob\", \"charlie\"])\nprint(f\"   View: {v}\")\nprint(f\"   Nodes: {v.node_count}\")\nprint(f\"   Edges: {v.edge_count}\")\nprint(\"\\n   Node table:\")\nprint(v.obs)\n\n# View specific layer\nprint(\"\\n2. View Layer 2023:\")\nv2023 = G.view(layers=\"2023\")\nprint(f\"   View: {v2023}\")\nprint(f\"   Nodes: {v2023.node_count}\")\nprint(f\"   Edges: {v2023.edge_count}\")\n\n# View multiple layers\nprint(\"\\n3. View Layers 2022+2023:\")\nv_early = G.view(layers=[\"2022\", \"2023\"])\nprint(f\"   View: {v_early}\")\nprint(f\"   Nodes: {v_early.node_count}\")\nprint(f\"   Edges: {v_early.edge_count}\")\n\n# Combined filters\nprint(\"\\n4. Combined: Specific nodes in 2023:\")\nv_combo = G.view(nodes=[\"alice\", \"eve\"], layers=\"2023\")\nprint(f\"   View: {v_combo}\")\nprint(f\"   Nodes: {v_combo.node_count}\")\nprint(f\"   Edges: {v_combo.edge_count}\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING GRAPHVIEW - BASIC FILTERING\") print(\"=\" * 60)  # View specific nodes print(\"\\n1. View Specific Nodes:\") v = G.view(nodes=[\"alice\", \"bob\", \"charlie\"]) print(f\"   View: {v}\") print(f\"   Nodes: {v.node_count}\") print(f\"   Edges: {v.edge_count}\") print(\"\\n   Node table:\") print(v.obs)  # View specific layer print(\"\\n2. View Layer 2023:\") v2023 = G.view(layers=\"2023\") print(f\"   View: {v2023}\") print(f\"   Nodes: {v2023.node_count}\") print(f\"   Edges: {v2023.edge_count}\")  # View multiple layers print(\"\\n3. View Layers 2022+2023:\") v_early = G.view(layers=[\"2022\", \"2023\"]) print(f\"   View: {v_early}\") print(f\"   Nodes: {v_early.node_count}\") print(f\"   Edges: {v_early.edge_count}\")  # Combined filters print(\"\\n4. Combined: Specific nodes in 2023:\") v_combo = G.view(nodes=[\"alice\", \"eve\"], layers=\"2023\") print(f\"   View: {v_combo}\") print(f\"   Nodes: {v_combo.node_count}\") print(f\"   Edges: {v_combo.edge_count}\") <pre>\n============================================================\nTESTING GRAPHVIEW - BASIC FILTERING\n============================================================\n\n1. View Specific Nodes:\n   View: GraphView(nodes=3, edges=14)\n   Nodes: 3\n   Edges: 14\n\n   Node table:\nshape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vertex_id \u2506 name    \u2506 age \u2506 role     \u2506 salary \u2502\n\u2502 ---       \u2506 ---     \u2506 --- \u2506 ---      \u2506 ---    \u2502\n\u2502 str       \u2506 str     \u2506 i64 \u2506 str      \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 alice     \u2506 Alice   \u2506 25  \u2506 engineer \u2506 80000  \u2502\n\u2502 bob       \u2506 Bob     \u2506 30  \u2506 manager  \u2506 95000  \u2502\n\u2502 charlie   \u2506 Charlie \u2506 28  \u2506 engineer \u2506 85000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2. View Layer 2023:\n   View: GraphView(nodes=5, edges=5)\n   Nodes: 5\n   Edges: 5\n\n3. View Layers 2022+2023:\n   View: GraphView(nodes=5, edges=9)\n   Nodes: 5\n   Edges: 9\n\n4. Combined: Specific nodes in 2023:\n   View: GraphView(nodes=2, edges=1)\n   Nodes: 2\n   Edges: 1\n</pre> In\u00a0[40]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING GRAPHVIEW - PREDICATE FILTERING\")\nprint(\"=\" * 60)\n\n# View by node predicate (high salary)\nprint(\"\\n1. High Salary Employees (&gt;100k):\")\nv_rich = G.view(nodes=lambda vid: G.get_vertex_attrs(vid).get(\"salary\", 0) &gt; 100000)\nprint(f\"   View: {v_rich}\")\nprint(f\"   High earners: {sorted(v_rich.node_ids)}\")\nprint(\"\\n   Details:\")\nprint(v_rich.obs.select([\"vertex_id\", \"name\", \"salary\", \"role\"]))\n\n# View by edge predicate (strong collaboration)\nprint(\"\\n2. Strong Collaborations (weight &gt; 0.8):\")\nv_strong = G.view(edges=lambda eid: G.edge_weights.get(eid, 0) &gt; 0.8)\nprint(f\"   View: {v_strong}\")\nprint(f\"   Strong edges: {v_strong.edge_count}\")\nedges_df = v_strong.edges_df(include_weight=True)\nprint(\"\\n   Top collaborations:\")\nprint(edges_df.select([\"edge_id\", \"source\", \"target\", \"global_weight\"]).head(10))\n\n# Combined predicate (engineers in recent years)\nprint(\"\\n3. Engineers in 2023/2024:\")\nv_eng = G.view(\n    nodes=lambda vid: \"engineer\" in G.get_vertex_attrs(vid).get(\"role\", \"\"), layers=[\"2023\", \"2024\"]\n)\nprint(f\"   View: {v_eng}\")\nprint(f\"   Engineers: {sorted(v_eng.node_ids)}\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING GRAPHVIEW - PREDICATE FILTERING\") print(\"=\" * 60)  # View by node predicate (high salary) print(\"\\n1. High Salary Employees (&gt;100k):\") v_rich = G.view(nodes=lambda vid: G.get_vertex_attrs(vid).get(\"salary\", 0) &gt; 100000) print(f\"   View: {v_rich}\") print(f\"   High earners: {sorted(v_rich.node_ids)}\") print(\"\\n   Details:\") print(v_rich.obs.select([\"vertex_id\", \"name\", \"salary\", \"role\"]))  # View by edge predicate (strong collaboration) print(\"\\n2. Strong Collaborations (weight &gt; 0.8):\") v_strong = G.view(edges=lambda eid: G.edge_weights.get(eid, 0) &gt; 0.8) print(f\"   View: {v_strong}\") print(f\"   Strong edges: {v_strong.edge_count}\") edges_df = v_strong.edges_df(include_weight=True) print(\"\\n   Top collaborations:\") print(edges_df.select([\"edge_id\", \"source\", \"target\", \"global_weight\"]).head(10))  # Combined predicate (engineers in recent years) print(\"\\n3. Engineers in 2023/2024:\") v_eng = G.view(     nodes=lambda vid: \"engineer\" in G.get_vertex_attrs(vid).get(\"role\", \"\"), layers=[\"2023\", \"2024\"] ) print(f\"   View: {v_eng}\") print(f\"   Engineers: {sorted(v_eng.node_ids)}\") <pre>\n============================================================\nTESTING GRAPHVIEW - PREDICATE FILTERING\n============================================================\n\n1. High Salary Employees (&gt;100k):\n   View: GraphView(nodes=1, edges=14)\n   High earners: ['diana']\n\n   Details:\nshape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vertex_id \u2506 name  \u2506 salary \u2506 role     \u2502\n\u2502 ---       \u2506 ---   \u2506 ---    \u2506 ---      \u2502\n\u2502 str       \u2506 str   \u2506 i64    \u2506 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 diana     \u2506 Diana \u2506 120000 \u2506 director \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2. Strong Collaborations (weight &gt; 0.8):\n   View: GraphView(nodes=6, edges=6)\n   Strong edges: 6\n\n   Top collaborations:\nshape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id \u2506 source  \u2506 target  \u2506 global_weight \u2502\n\u2502 ---     \u2506 ---     \u2506 ---     \u2506 ---           \u2502\n\u2502 str     \u2506 str     \u2506 str     \u2506 f64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_1  \u2506 alice   \u2506 charlie \u2506 0.9           \u2502\n\u2502 edge_4  \u2506 alice   \u2506 bob     \u2506 0.85          \u2502\n\u2502 edge_5  \u2506 alice   \u2506 eve     \u2506 0.95          \u2502\n\u2502 edge_9  \u2506 alice   \u2506 frank   \u2506 0.9           \u2502\n\u2502 edge_10 \u2506 alice   \u2506 eve     \u2506 0.92          \u2502\n\u2502 edge_11 \u2506 charlie \u2506 eve     \u2506 0.85          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n3. Engineers in 2023/2024:\n   View: GraphView(nodes=3, edges=4)\n   Engineers: ['alice', 'charlie', 'eve']\n</pre> In\u00a0[42]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING GRAPHVIEW - ADVANCED OPERATIONS\")\nprint(\"=\" * 60)\n\n# Access properties\nprint(\"\\n1. View Properties:\")\nv = G.view(layers=\"2023\")\nprint(f\"   node_count: {v.node_count}\")\nprint(f\"   edge_count: {v.edge_count}\")\nprint(f\"   node_ids: {sorted(list(v.node_ids))}\")\nprint(f\"\\n   Matrix shape: {v.X.shape}\")\nprint(f\"   Matrix nnz: {v.X.nnz}\")\n\n# Get DataFrames\nprint(\"\\n2. View DataFrames:\")\nvertices_df = v.vertices_df()\nedges_df = v.edges_df(include_weight=True, include_directed=True)\nprint(f\"   Vertices DF: {vertices_df.shape}\")\nprint(f\"   Edges DF: {edges_df.shape}\")\n\n# Summary\nprint(\"\\n3. View Summary:\")\nprint(v.summary())\n\n# Nested views\nprint(\"\\n4. Nested Views:\")\nv1 = G.view(layers=\"2023\")\nprint(f\"   v1 (2023): {v1.node_count} nodes, {v1.edge_count} edges\")\n\nv2 = v1.subview(nodes=[\"alice\", \"bob\", \"eve\"])\nprint(f\"   v2 (alice/bob/eve in 2023): {v2.node_count} nodes, {v2.edge_count} edges\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING GRAPHVIEW - ADVANCED OPERATIONS\") print(\"=\" * 60)  # Access properties print(\"\\n1. View Properties:\") v = G.view(layers=\"2023\") print(f\"   node_count: {v.node_count}\") print(f\"   edge_count: {v.edge_count}\") print(f\"   node_ids: {sorted(list(v.node_ids))}\") print(f\"\\n   Matrix shape: {v.X.shape}\") print(f\"   Matrix nnz: {v.X.nnz}\")  # Get DataFrames print(\"\\n2. View DataFrames:\") vertices_df = v.vertices_df() edges_df = v.edges_df(include_weight=True, include_directed=True) print(f\"   Vertices DF: {vertices_df.shape}\") print(f\"   Edges DF: {edges_df.shape}\")  # Summary print(\"\\n3. View Summary:\") print(v.summary())  # Nested views print(\"\\n4. Nested Views:\") v1 = G.view(layers=\"2023\") print(f\"   v1 (2023): {v1.node_count} nodes, {v1.edge_count} edges\")  v2 = v1.subview(nodes=[\"alice\", \"bob\", \"eve\"]) print(f\"   v2 (alice/bob/eve in 2023): {v2.node_count} nodes, {v2.edge_count} edges\") <pre>\n============================================================\nTESTING GRAPHVIEW - ADVANCED OPERATIONS\n============================================================\n\n1. View Properties:\n   node_count: 5\n   edge_count: 5\n   node_ids: ['alice', 'bob', 'charlie', 'diana', 'eve']\n\n   Matrix shape: (5, 5)\n   Matrix nnz: 10\n\n2. View DataFrames:\n   Vertices DF: (5, 5)\n   Edges DF: (5, 13)\n\n3. View Summary:\nGraphView Summary\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nNodes: 5\nEdges: 5\nFilters: layers=['2023']\n\n4. Nested Views:\n   v1 (2023): 5 nodes, 5 edges\n   v2 (alice/bob/eve in 2023): 3 nodes, 2 edges\n</pre> In\u00a0[44]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING GRAPHVIEW - MATERIALIZATION\")\nprint(\"=\" * 60)\n\n# Materialize layer 2023\nprint(\"\\n1. Materialize Layer 2023:\")\nv2023 = G.view(layers=\"2023\")\nsubG = v2023.materialize(copy_attributes=True)\n\nprint(f\"   Original graph: {G.number_of_vertices()} nodes, {G.number_of_edges()} edges\")\nprint(f\"   Subgraph 2023: {subG.number_of_vertices()} nodes, {subG.number_of_edges()} edges\")\nprint(f\"   Subgraph vertices: {sorted(subG.vertices())}\")\n\n# Check attributes were copied\nprint(\"\\n   Sample attributes:\")\nalice_attrs = subG.get_vertex_attrs(\"alice\")\nprint(f\"   alice: {alice_attrs}\")\n\n# Materialize high earners\nprint(\"\\n2. Materialize High Earners Network:\")\nv_rich = G.view(nodes=lambda vid: G.get_vertex_attrs(vid).get(\"salary\", 0) &gt; 95000)\nrich_network = v_rich.materialize(copy_attributes=True)\n\nprint(f\"   High earners network: {rich_network.number_of_vertices()} nodes\")\nprint(f\"   Nodes: {sorted(rich_network.vertices())}\")\nprint(f\"   Edges: {rich_network.number_of_edges()}\")\n\n# Verify independence\nprint(\"\\n3. Verify Independence:\")\nprint(f\"   Original graph edges: {G.number_of_edges()}\")\nprint(f\"   Subgraph edges: {subG.number_of_edges()}\")\nsubG.add_vertex(\"test_node\")\nprint(\"   After modifying subgraph:\")\nprint(f\"     Original: {G.number_of_vertices()} nodes\")\nprint(f\"     Subgraph: {subG.number_of_vertices()} nodes\")\nprint(\"   \u2713 Graphs are independent\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING GRAPHVIEW - MATERIALIZATION\") print(\"=\" * 60)  # Materialize layer 2023 print(\"\\n1. Materialize Layer 2023:\") v2023 = G.view(layers=\"2023\") subG = v2023.materialize(copy_attributes=True)  print(f\"   Original graph: {G.number_of_vertices()} nodes, {G.number_of_edges()} edges\") print(f\"   Subgraph 2023: {subG.number_of_vertices()} nodes, {subG.number_of_edges()} edges\") print(f\"   Subgraph vertices: {sorted(subG.vertices())}\")  # Check attributes were copied print(\"\\n   Sample attributes:\") alice_attrs = subG.get_vertex_attrs(\"alice\") print(f\"   alice: {alice_attrs}\")  # Materialize high earners print(\"\\n2. Materialize High Earners Network:\") v_rich = G.view(nodes=lambda vid: G.get_vertex_attrs(vid).get(\"salary\", 0) &gt; 95000) rich_network = v_rich.materialize(copy_attributes=True)  print(f\"   High earners network: {rich_network.number_of_vertices()} nodes\") print(f\"   Nodes: {sorted(rich_network.vertices())}\") print(f\"   Edges: {rich_network.number_of_edges()}\")  # Verify independence print(\"\\n3. Verify Independence:\") print(f\"   Original graph edges: {G.number_of_edges()}\") print(f\"   Subgraph edges: {subG.number_of_edges()}\") subG.add_vertex(\"test_node\") print(\"   After modifying subgraph:\") print(f\"     Original: {G.number_of_vertices()} nodes\") print(f\"     Subgraph: {subG.number_of_vertices()} nodes\") print(\"   \u2713 Graphs are independent\") <pre>\n============================================================\nTESTING GRAPHVIEW - MATERIALIZATION\n============================================================\n\n1. Materialize Layer 2023:\n   Original graph: 6 nodes, 14 edges\n   Subgraph 2023: 5 nodes, 5 edges\n   Subgraph vertices: ['alice', 'bob', 'charlie', 'diana', 'eve']\n\n   Sample attributes:\n   alice: {'vertex_id': 'alice', 'name': 'Alice', 'age': 25, 'role': 'engineer', 'salary': 80000}\n\n2. Materialize High Earners Network:\n   High earners network: 2 nodes\n   Nodes: ['diana', 'frank']\n   Edges: 1\n\n3. Verify Independence:\n   Original graph edges: 14\n   Subgraph edges: 5\n   After modifying subgraph:\n     Original: 6 nodes\n     Subgraph: 6 nodes\n   \u2713 Graphs are independent\n</pre> In\u00a0[46]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"TESTING SNAPSHOT AND DIFF\")\nprint(\"=\" * 60)\n\n# Create initial snapshot\nprint(\"\\n1. Create Initial Snapshot:\")\nG.layers.active = \"2024\"\nsnap1 = G.snapshot(\"initial_state\")\nprint(f'   \u2713 Created snapshot: {snap1[\"label\"]}')\nprint(f'   Vertices: {snap1[\"counts\"][\"vertices\"]}')\nprint(f'   Edges: {snap1[\"counts\"][\"edges\"]}')\nprint(f'   Layers: {snap1[\"counts\"][\"layers\"]}')\n\n# Make changes\nprint(\"\\n2. Make Changes:\")\nprint(\"   Adding new vertices...\")\nG.add_vertex(\"grace\", name=\"Grace\", age=29, role=\"engineer\", salary=87000)\nG.add_vertex(\"henry\", name=\"Henry\", age=33, role=\"architect\", salary=115000)\n\nprint(\"   Adding new edges...\")\nG.add_edge(\"grace\", \"alice\", weight=0.85, project=\"Innovation\")\nG.add_edge(\"henry\", \"diana\", weight=0.9, project=\"Architecture\")\n\nprint(\"   Removing a vertex...\")\nG.remove_vertex(\"frank\")\n\n# Create second snapshot\nsnap2 = G.snapshot(\"after_changes\")\nprint(f'\\n   \u2713 Created snapshot: {snap2[\"label\"]}')\n\n# Diff\nprint(\"\\n3. Compare Snapshots:\")\ndiff = G.diff(\"initial_state\", \"after_changes\")\nprint(diff.summary())\n\nprint(\"\\n   Details:\")\nprint(f\"   Added vertices: {sorted(diff.vertices_added)}\")\nprint(f\"   Removed vertices: {sorted(diff.vertices_removed)}\")\nprint(f\"   Added edges: {len(diff.edges_added)}\")\nprint(f\"   Removed edges: {len(diff.edges_removed)}\")\n\n# Compare with current\nprint(\"\\n4. Compare with Current State:\")\nG.add_vertex(\"iris\", name=\"Iris\", age=26, role=\"data_scientist\", salary=92000)\ndiff_current = G.diff(\"after_changes\")\nprint(diff_current.summary())\n\n# List all snapshots\nprint(\"\\n5. List All Snapshots:\")\nsnapshots = G.list_snapshots()\nfor snap in snapshots:\n    print(f'\\n   {snap[\"label\"]}:')\n    print(f'     Timestamp: {snap[\"timestamp\"]}')\n    print(f'     Vertices: {snap[\"counts\"][\"vertices\"]}')\n    print(f'     Edges: {snap[\"counts\"][\"edges\"]}')\n</pre> print(\"\\n\" + \"=\" * 60) print(\"TESTING SNAPSHOT AND DIFF\") print(\"=\" * 60)  # Create initial snapshot print(\"\\n1. Create Initial Snapshot:\") G.layers.active = \"2024\" snap1 = G.snapshot(\"initial_state\") print(f'   \u2713 Created snapshot: {snap1[\"label\"]}') print(f'   Vertices: {snap1[\"counts\"][\"vertices\"]}') print(f'   Edges: {snap1[\"counts\"][\"edges\"]}') print(f'   Layers: {snap1[\"counts\"][\"layers\"]}')  # Make changes print(\"\\n2. Make Changes:\") print(\"   Adding new vertices...\") G.add_vertex(\"grace\", name=\"Grace\", age=29, role=\"engineer\", salary=87000) G.add_vertex(\"henry\", name=\"Henry\", age=33, role=\"architect\", salary=115000)  print(\"   Adding new edges...\") G.add_edge(\"grace\", \"alice\", weight=0.85, project=\"Innovation\") G.add_edge(\"henry\", \"diana\", weight=0.9, project=\"Architecture\")  print(\"   Removing a vertex...\") G.remove_vertex(\"frank\")  # Create second snapshot snap2 = G.snapshot(\"after_changes\") print(f'\\n   \u2713 Created snapshot: {snap2[\"label\"]}')  # Diff print(\"\\n3. Compare Snapshots:\") diff = G.diff(\"initial_state\", \"after_changes\") print(diff.summary())  print(\"\\n   Details:\") print(f\"   Added vertices: {sorted(diff.vertices_added)}\") print(f\"   Removed vertices: {sorted(diff.vertices_removed)}\") print(f\"   Added edges: {len(diff.edges_added)}\") print(f\"   Removed edges: {len(diff.edges_removed)}\")  # Compare with current print(\"\\n4. Compare with Current State:\") G.add_vertex(\"iris\", name=\"Iris\", age=26, role=\"data_scientist\", salary=92000) diff_current = G.diff(\"after_changes\") print(diff_current.summary())  # List all snapshots print(\"\\n5. List All Snapshots:\") snapshots = G.list_snapshots() for snap in snapshots:     print(f'\\n   {snap[\"label\"]}:')     print(f'     Timestamp: {snap[\"timestamp\"]}')     print(f'     Vertices: {snap[\"counts\"][\"vertices\"]}')     print(f'     Edges: {snap[\"counts\"][\"edges\"]}') <pre>\n============================================================\nTESTING SNAPSHOT AND DIFF\n============================================================\n\n1. Create Initial Snapshot:\n   \u2713 Created snapshot: initial_state\n   Vertices: 6\n   Edges: 14\n   Layers: 5\n\n2. Make Changes:\n   Adding new vertices...\n   Adding new edges...\n   Removing a vertex...\n\n   \u2713 Created snapshot: after_changes\n\n3. Compare Snapshots:\nDiff: initial_state \u2192 after_changes\n\nVertices: +2 added, 1 removed\nEdges: +2 added, 2 removed\nLayers: +0 added, 0 removed\n\n   Details:\n   Added vertices: ['grace', 'henry']\n   Removed vertices: ['frank']\n   Added edges: 2\n   Removed edges: 2\n\n4. Compare with Current State:\nDiff: after_changes \u2192 current\n\nVertices: +1 added, 0 removed\nEdges: +0 added, 0 removed\nLayers: +0 added, 0 removed\n\n5. List All Snapshots:\n\n   initial_state:\n     Timestamp: 2025-10-28T20:08:43.738089+00:00\n     Vertices: 6\n     Edges: 14\n\n   after_changes:\n     Timestamp: 2025-10-28T20:08:43.745527+00:00\n     Vertices: 7\n     Edges: 14\n</pre> In\u00a0[48]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"ADVANCED ANALYSIS - NETWORK METRICS\")\nprint(\"=\" * 60)\n\n# Per-layer analysis\nprint(\"\\n1. Per-Layer Network Metrics:\")\nfor layer_id in [\"2022\", \"2023\", \"2024\"]:\n    v = G.view(layers=layer_id)\n\n    print(f\"\\n   {layer_id}:\")\n    print(f\"     Nodes: {v.node_count}\")\n    print(f\"     Edges: {v.edge_count}\")\n\n    if v.edge_count &gt; 0:\n        avg_weight = v.var.select(\"weight\").mean().item() if \"weight\" in v.var.columns else 0\n        print(f\"     Avg edge weight: {avg_weight:.3f}\")\n\n    # Degree analysis (using materialized subgraph)\n    subG = v.materialize()\n    degrees = {vid: subG.degree(vid) for vid in subG.vertices()}\n    if degrees:\n        print(f\"     Avg degree: {sum(degrees.values()) / len(degrees):.2f}\")\n        print(f\"     Max degree: {max(degrees.values())}\")\n        max_degree_node = max(degrees, key=degrees.get)\n        print(f\"     Hub: {max_degree_node} (degree={degrees[max_degree_node]})\")\n\n# Compare layers\nprint(\"\\n2. Layer Comparison:\")\nchanges = G.layers.temporal_dynamics([\"2022\", \"2023\", \"2024\"], metric=\"vertex_change\")\nprint(\"\\n   Vertex changes over time:\")\nfor i, change in enumerate(changes):\n    year_from = [\"2022\", \"2023\"][i]\n    year_to = [\"2023\", \"2024\"][i]\n    print(f'   {year_from}\u2192{year_to}: {change[\"added\"]:+d} added, {change[\"removed\"]} removed')\n\nchanges = G.layers.temporal_dynamics([\"2022\", \"2023\", \"2024\"], metric=\"edge_change\")\nprint(\"\\n   Edge changes over time:\")\nfor i, change in enumerate(changes):\n    year_from = [\"2022\", \"2023\"][i]\n    year_to = [\"2023\", \"2024\"][i]\n    print(f'   {year_from}\u2192{year_to}: {change[\"added\"]:+d} added, {change[\"removed\"]} removed')\n</pre> print(\"\\n\" + \"=\" * 60) print(\"ADVANCED ANALYSIS - NETWORK METRICS\") print(\"=\" * 60)  # Per-layer analysis print(\"\\n1. Per-Layer Network Metrics:\") for layer_id in [\"2022\", \"2023\", \"2024\"]:     v = G.view(layers=layer_id)      print(f\"\\n   {layer_id}:\")     print(f\"     Nodes: {v.node_count}\")     print(f\"     Edges: {v.edge_count}\")      if v.edge_count &gt; 0:         avg_weight = v.var.select(\"weight\").mean().item() if \"weight\" in v.var.columns else 0         print(f\"     Avg edge weight: {avg_weight:.3f}\")      # Degree analysis (using materialized subgraph)     subG = v.materialize()     degrees = {vid: subG.degree(vid) for vid in subG.vertices()}     if degrees:         print(f\"     Avg degree: {sum(degrees.values()) / len(degrees):.2f}\")         print(f\"     Max degree: {max(degrees.values())}\")         max_degree_node = max(degrees, key=degrees.get)         print(f\"     Hub: {max_degree_node} (degree={degrees[max_degree_node]})\")  # Compare layers print(\"\\n2. Layer Comparison:\") changes = G.layers.temporal_dynamics([\"2022\", \"2023\", \"2024\"], metric=\"vertex_change\") print(\"\\n   Vertex changes over time:\") for i, change in enumerate(changes):     year_from = [\"2022\", \"2023\"][i]     year_to = [\"2023\", \"2024\"][i]     print(f'   {year_from}\u2192{year_to}: {change[\"added\"]:+d} added, {change[\"removed\"]} removed')  changes = G.layers.temporal_dynamics([\"2022\", \"2023\", \"2024\"], metric=\"edge_change\") print(\"\\n   Edge changes over time:\") for i, change in enumerate(changes):     year_from = [\"2022\", \"2023\"][i]     year_to = [\"2023\", \"2024\"][i]     print(f'   {year_from}\u2192{year_to}: {change[\"added\"]:+d} added, {change[\"removed\"]} removed') <pre>\n============================================================\nADVANCED ANALYSIS - NETWORK METRICS\n============================================================\n\n1. Per-Layer Network Metrics:\n\n   2022:\n     Nodes: 4\n     Edges: 4\n     Avg edge weight: 0.000\n     Avg degree: 2.00\n     Max degree: 2\n     Hub: bob (degree=2)\n\n   2023:\n     Nodes: 5\n     Edges: 5\n     Avg edge weight: 0.000\n     Avg degree: 2.00\n     Max degree: 3\n     Hub: eve (degree=3)\n\n   2024:\n     Nodes: 7\n     Edges: 5\n     Avg edge weight: 0.000\n     Avg degree: 1.43\n     Max degree: 3\n     Hub: eve (degree=3)\n\n2. Layer Comparison:\n\n   Vertex changes over time:\n   2022\u21922023: +1 added, 0 removed\n   2023\u21922024: +3 added, 1 removed\n\n   Edge changes over time:\n   2022\u21922023: +5 added, 4 removed\n   2023\u21922024: +5 added, 5 removed\n</pre> In\u00a0[50]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"ADVANCED ANALYSIS - POLARS QUERIES\")\nprint(\"=\" * 60)\n\n# Query 1: Top earners\nprint(\"\\n1. Top 3 Earners:\")\ntop_earners = G.obs.sort(\"salary\", descending=True).head(3)\nprint(top_earners.select([\"vertex_id\", \"name\", \"role\", \"salary\"]))\n\n# Query 2: Role distribution\nprint(\"\\n2. Role Distribution:\")\nrole_dist = (\n    G.obs.group_by(\"role\")\n    .agg([pl.count(\"vertex_id\").alias(\"count\"), pl.mean(\"salary\").alias(\"avg_salary\")])\n    .sort(\"count\", descending=True)\n)\nprint(role_dist)\n\n# Query 3: High-weight collaborations\nprint(\"\\n3. Top 5 Collaborations by Weight:\")\ntop_edges = (\n    G.view(layers=\"2023\")\n    .edges_df(\n        layer=\"2023\", include_weight=True, resolved_weight=True\n    )  # adds global_weight, layer_weight, effective_weight\n    .sort(\"effective_weight\", descending=True)\n    .select([\"edge_id\", \"effective_weight\"])\n    .head(5)\n)\nprint(top_edges)\n\n# Query 4: Projects by hours\nprint(\"\\n4. Total Hours by Project:\")\nif \"project\" in G.var.columns and \"hours\" in G.var.columns:\n    project_hours = (\n        G.var.group_by(\"project\")\n        .agg([pl.count(\"edge_id\").alias(\"collaborations\"), pl.sum(\"hours\").alias(\"total_hours\")])\n        .sort(\"total_hours\", descending=True)\n    )\n    print(project_hours)\n\n# Query 5: Salary growth (across snapshots if attributes updated)\nprint(\"\\n5. Salary Statistics:\")\nsalary_stats = G.obs.select(\n    [\n        pl.col(\"salary\").min().alias(\"min_salary\"),\n        pl.col(\"salary\").max().alias(\"max_salary\"),\n        pl.col(\"salary\").mean().alias(\"avg_salary\"),\n        pl.col(\"salary\").median().alias(\"median_salary\"),\n    ]\n)\nprint(salary_stats)\n</pre> print(\"\\n\" + \"=\" * 60) print(\"ADVANCED ANALYSIS - POLARS QUERIES\") print(\"=\" * 60)  # Query 1: Top earners print(\"\\n1. Top 3 Earners:\") top_earners = G.obs.sort(\"salary\", descending=True).head(3) print(top_earners.select([\"vertex_id\", \"name\", \"role\", \"salary\"]))  # Query 2: Role distribution print(\"\\n2. Role Distribution:\") role_dist = (     G.obs.group_by(\"role\")     .agg([pl.count(\"vertex_id\").alias(\"count\"), pl.mean(\"salary\").alias(\"avg_salary\")])     .sort(\"count\", descending=True) ) print(role_dist)  # Query 3: High-weight collaborations print(\"\\n3. Top 5 Collaborations by Weight:\") top_edges = (     G.view(layers=\"2023\")     .edges_df(         layer=\"2023\", include_weight=True, resolved_weight=True     )  # adds global_weight, layer_weight, effective_weight     .sort(\"effective_weight\", descending=True)     .select([\"edge_id\", \"effective_weight\"])     .head(5) ) print(top_edges)  # Query 4: Projects by hours print(\"\\n4. Total Hours by Project:\") if \"project\" in G.var.columns and \"hours\" in G.var.columns:     project_hours = (         G.var.group_by(\"project\")         .agg([pl.count(\"edge_id\").alias(\"collaborations\"), pl.sum(\"hours\").alias(\"total_hours\")])         .sort(\"total_hours\", descending=True)     )     print(project_hours)  # Query 5: Salary growth (across snapshots if attributes updated) print(\"\\n5. Salary Statistics:\") salary_stats = G.obs.select(     [         pl.col(\"salary\").min().alias(\"min_salary\"),         pl.col(\"salary\").max().alias(\"max_salary\"),         pl.col(\"salary\").mean().alias(\"avg_salary\"),         pl.col(\"salary\").median().alias(\"median_salary\"),     ] ) print(salary_stats) <pre>\n============================================================\nADVANCED ANALYSIS - POLARS QUERIES\n============================================================\n\n1. Top 3 Earners:\nshape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vertex_id \u2506 name  \u2506 role      \u2506 salary \u2502\n\u2502 ---       \u2506 ---   \u2506 ---       \u2506 ---    \u2502\n\u2502 str       \u2506 str   \u2506 str       \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 diana     \u2506 Diana \u2506 director  \u2506 120000 \u2502\n\u2502 henry     \u2506 Henry \u2506 architect \u2506 115000 \u2502\n\u2502 bob       \u2506 Bob   \u2506 manager   \u2506 95000  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n2. Role Distribution:\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 role           \u2506 count \u2506 avg_salary \u2502\n\u2502 ---            \u2506 ---   \u2506 ---        \u2502\n\u2502 str            \u2506 u32   \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 engineer       \u2506 4     \u2506 83750.0    \u2502\n\u2502 manager        \u2506 1     \u2506 95000.0    \u2502\n\u2502 architect      \u2506 1     \u2506 115000.0   \u2502\n\u2502 director       \u2506 1     \u2506 120000.0   \u2502\n\u2502 data_scientist \u2506 1     \u2506 92000.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n3. Top 5 Collaborations by Weight:\nshape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id \u2506 effective_weight \u2502\n\u2502 ---     \u2506 ---              \u2502\n\u2502 str     \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_5  \u2506 0.95             \u2502\n\u2502 edge_4  \u2506 0.85             \u2502\n\u2502 edge_7  \u2506 0.8              \u2502\n\u2502 edge_6  \u2506 0.75             \u2502\n\u2502 edge_8  \u2506 0.7              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n4. Total Hours by Project:\nshape: (9, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 project      \u2506 collaborations \u2506 total_hours \u2502\n\u2502 ---          \u2506 ---            \u2506 ---         \u2502\n\u2502 str          \u2506 u32            \u2506 i64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 ProjectZ     \u2506 2              \u2506 320         \u2502\n\u2502 NextGen      \u2506 2              \u2506 290         \u2502\n\u2502 ProjectX     \u2506 2              \u2506 270         \u2502\n\u2502 ProjectW     \u2506 2              \u2506 170         \u2502\n\u2502 Management   \u2506 2              \u2506 170         \u2502\n\u2502 Strategy     \u2506 1              \u2506 80          \u2502\n\u2502 ProjectY     \u2506 1              \u2506 60          \u2502\n\u2502 Architecture \u2506 1              \u2506 0           \u2502\n\u2502 Innovation   \u2506 1              \u2506 0           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n5. Salary Statistics:\nshape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 min_salary \u2506 max_salary \u2506 avg_salary \u2506 median_salary \u2502\n\u2502 ---        \u2506 ---        \u2506 ---        \u2506 ---           \u2502\n\u2502 i64        \u2506 i64        \u2506 f64        \u2506 f64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 80000      \u2506 120000     \u2506 94625.0    \u2506 89500.0       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[52]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"PERFORMANCE BENCHMARKS\")\nprint(\"=\" * 60)\n\nimport time\n\n# Benchmark 1: View creation\nprint(\"\\n1. View Creation (1000 iterations):\")\nt0 = time.time()\nfor _ in range(1000):\n    v = G.view(layers=\"2023\")\nt1 = time.time()\nprint(f\"   Time: {(t1 - t0) * 1000:.2f}ms total ({(t1 - t0):.4f}ms per view)\")\n\n# Benchmark 2: Property access\nprint(\"\\n2. Property Access (1000 iterations):\")\nv = G.view(layers=\"2023\")\nt0 = time.time()\nfor _ in range(1000):\n    _ = v.obs\n    _ = v.var\nt1 = time.time()\nprint(f\"   Time: {(t1 - t0) * 1000:.2f}ms total\")\n\n# Benchmark 3: Materialization\nprint(\"\\n3. Materialization (100 iterations):\")\nv = G.view(layers=\"2023\")\nt0 = time.time()\nfor _ in range(100):\n    subG = v.materialize(copy_attributes=False)\nt1 = time.time()\nprint(f\"   Time: {(t1 - t0) * 1000:.2f}ms total ({(t1 - t0) * 10:.2f}ms per materialization)\")\n\n# Benchmark 4: Snapshot creation\nprint(\"\\n4. Snapshot Creation (100 iterations):\")\nt0 = time.time()\nfor i in range(100):\n    G.snapshot(f\"bench_{i}\")\nt1 = time.time()\nprint(f\"   Time: {(t1 - t0) * 1000:.2f}ms total ({(t1 - t0) * 10:.2f}ms per snapshot)\")\nprint(f\"   Total snapshots: {len(G._snapshots)}\")\n\n# Benchmark 5: DataFrame filtering\nprint(\"\\n5. DataFrame Filtering (1000 iterations):\")\nt0 = time.time()\nfor _ in range(1000):\n    filtered = G.obs.filter(pl.col(\"salary\") &gt; 90000)\nt1 = time.time()\nprint(f\"   Time: {(t1 - t0) * 1000:.2f}ms total ({(t1 - t0):.4f}ms per filter)\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"PERFORMANCE BENCHMARKS\") print(\"=\" * 60)  import time  # Benchmark 1: View creation print(\"\\n1. View Creation (1000 iterations):\") t0 = time.time() for _ in range(1000):     v = G.view(layers=\"2023\") t1 = time.time() print(f\"   Time: {(t1 - t0) * 1000:.2f}ms total ({(t1 - t0):.4f}ms per view)\")  # Benchmark 2: Property access print(\"\\n2. Property Access (1000 iterations):\") v = G.view(layers=\"2023\") t0 = time.time() for _ in range(1000):     _ = v.obs     _ = v.var t1 = time.time() print(f\"   Time: {(t1 - t0) * 1000:.2f}ms total\")  # Benchmark 3: Materialization print(\"\\n3. Materialization (100 iterations):\") v = G.view(layers=\"2023\") t0 = time.time() for _ in range(100):     subG = v.materialize(copy_attributes=False) t1 = time.time() print(f\"   Time: {(t1 - t0) * 1000:.2f}ms total ({(t1 - t0) * 10:.2f}ms per materialization)\")  # Benchmark 4: Snapshot creation print(\"\\n4. Snapshot Creation (100 iterations):\") t0 = time.time() for i in range(100):     G.snapshot(f\"bench_{i}\") t1 = time.time() print(f\"   Time: {(t1 - t0) * 1000:.2f}ms total ({(t1 - t0) * 10:.2f}ms per snapshot)\") print(f\"   Total snapshots: {len(G._snapshots)}\")  # Benchmark 5: DataFrame filtering print(\"\\n5. DataFrame Filtering (1000 iterations):\") t0 = time.time() for _ in range(1000):     filtered = G.obs.filter(pl.col(\"salary\") &gt; 90000) t1 = time.time() print(f\"   Time: {(t1 - t0) * 1000:.2f}ms total ({(t1 - t0):.4f}ms per filter)\") <pre>\n============================================================\nPERFORMANCE BENCHMARKS\n============================================================\n\n1. View Creation (1000 iterations):\n   Time: 1.00ms total (0.0010ms per view)\n\n2. Property Access (1000 iterations):\n   Time: 587.01ms total\n\n3. Materialization (100 iterations):\n   Time: 89.60ms total (0.90ms per materialization)\n\n4. Snapshot Creation (100 iterations):\n   Time: 0.00ms total (0.00ms per snapshot)\n   Total snapshots: 102\n\n5. DataFrame Filtering (1000 iterations):\n   Time: 229.62ms total (0.2296ms per filter)\n</pre> In\u00a0[53]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"ANNNET COMPLETE TEST SUMMARY\")\nprint(\"=\" * 60)\n\nprint(\"\\n\ud83d\udcca GRAPH STATISTICS\")\nprint(f\"   Vertices: {G.number_of_vertices()}\")\nprint(f\"   Edges: {G.number_of_edges()}\")\nprint(f\"   Layers: {G.layers.count()}\")\nprint(f\"   Snapshots: {len(G._snapshots)}\")\n\nprint(\"\\n\u2705 TESTED FEATURES\")\nfeatures = [\n    \"AnnNet Properties (X, obs, var, uns)\",\n    \"LayerManager (add, remove, union, intersect, stats)\",\n    \"IndexManager (entity/edge lookups)\",\n    \"CacheManager (CSR/CSC caching)\",\n    \"GraphView (filtering, predicates, materialization)\",\n    \"Snapshot &amp; Diff (versioning, comparison)\",\n    \"I/O (save/load .annnet format)\",\n    \"Cross-layer analytics\",\n    \"Polars integration\",\n    \"Performance benchmarks\",\n]\n\nfor i, feature in enumerate(features, 1):\n    print(f\"   {i:2d}. {feature}\")\n\nprint(\"\\n\ud83d\udcc8 LAYER DETAILS\")\nprint(G.layers.summary())\n\nprint(\"\\n\ud83c\udfaf RECOMMENDATIONS\")\nprint(\"   1. Use views for large subgraph operations (lazy, efficient)\")\nprint(\"   2. Create snapshots before major graph modifications\")\nprint(\"   3. Use layers for temporal/contextual data organization\")\nprint(\"   4. Leverage Polars for fast attribute queries\")\nprint(\"   5. Cache CSR/CSC for repeated matrix operations\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\u2713 ALL TESTS COMPLETED SUCCESSFULLY!\")\nprint(\"=\" * 60)\n</pre> print(\"\\n\" + \"=\" * 60) print(\"ANNNET COMPLETE TEST SUMMARY\") print(\"=\" * 60)  print(\"\\n\ud83d\udcca GRAPH STATISTICS\") print(f\"   Vertices: {G.number_of_vertices()}\") print(f\"   Edges: {G.number_of_edges()}\") print(f\"   Layers: {G.layers.count()}\") print(f\"   Snapshots: {len(G._snapshots)}\")  print(\"\\n\u2705 TESTED FEATURES\") features = [     \"AnnNet Properties (X, obs, var, uns)\",     \"LayerManager (add, remove, union, intersect, stats)\",     \"IndexManager (entity/edge lookups)\",     \"CacheManager (CSR/CSC caching)\",     \"GraphView (filtering, predicates, materialization)\",     \"Snapshot &amp; Diff (versioning, comparison)\",     \"I/O (save/load .annnet format)\",     \"Cross-layer analytics\",     \"Polars integration\",     \"Performance benchmarks\", ]  for i, feature in enumerate(features, 1):     print(f\"   {i:2d}. {feature}\")  print(\"\\n\ud83d\udcc8 LAYER DETAILS\") print(G.layers.summary())  print(\"\\n\ud83c\udfaf RECOMMENDATIONS\") print(\"   1. Use views for large subgraph operations (lazy, efficient)\") print(\"   2. Create snapshots before major graph modifications\") print(\"   3. Use layers for temporal/contextual data organization\") print(\"   4. Leverage Polars for fast attribute queries\") print(\"   5. Cache CSR/CSC for repeated matrix operations\")  print(\"\\n\" + \"=\" * 60) print(\"\u2713 ALL TESTS COMPLETED SUCCESSFULLY!\") print(\"=\" * 60) <pre>\n============================================================\nANNNET COMPLETE TEST SUMMARY\n============================================================\n\n\ud83d\udcca GRAPH STATISTICS\n   Vertices: 8\n   Edges: 14\n   Layers: 5\n   Snapshots: 102\n\n\u2705 TESTED FEATURES\n    1. AnnNet Properties (X, obs, var, uns)\n    2. LayerManager (add, remove, union, intersect, stats)\n    3. IndexManager (entity/edge lookups)\n    4. CacheManager (CSR/CSC caching)\n    5. GraphView (filtering, predicates, materialization)\n    6. Snapshot &amp; Diff (versioning, comparison)\n    7. I/O (save/load .annnet format)\n    8. Cross-layer analytics\n    9. Polars integration\n   10. Performance benchmarks\n\n\ud83d\udcc8 LAYER DETAILS\nLayers: 5\n\u251c\u2500 default: 0 vertices, 0 edges\n\u251c\u2500 2022: 4 vertices, 4 edges\n\u251c\u2500 2023: 5 vertices, 5 edges\n\u251c\u2500 2024: 7 vertices, 5 edges\n\u2514\u2500 all_years: 5 vertices, 12 edges\n\n\ud83c\udfaf RECOMMENDATIONS\n   1. Use views for large subgraph operations (lazy, efficient)\n   2. Create snapshots before major graph modifications\n   3. Use layers for temporal/contextual data organization\n   4. Leverage Polars for fast attribute queries\n   5. Cache CSR/CSC for repeated matrix operations\n\n============================================================\n\u2713 ALL TESTS COMPLETED SUCCESSFULLY!\n============================================================\n</pre> In\u00a0[54]: Copied! <pre>print(\"\\n\" + \"=\" * 60)\nprint(\"CLEANUP\")\nprint(\"=\" * 60)\n\nimport os\nimport shutil\n\n# Clean up test files\nfiles_to_remove = [\n    \"company_network.annnet\",\n    \"network_2023.annnet\",\n]\n\nprint(\"\\nRemoving test files:\")\nfor filepath in files_to_remove:\n    if os.path.exists(filepath):\n        if os.path.isdir(filepath):\n            shutil.rmtree(filepath)\n            print(f\"   \u2713 Removed directory: {filepath}\")\n        else:\n            os.remove(filepath)\n            print(f\"   \u2713 Removed file: {filepath}\")\n    else:\n        print(f\"   \u2298 Not found: {filepath}\")\n\nprint(\"\\n\u2713 Cleanup complete\")\n</pre> print(\"\\n\" + \"=\" * 60) print(\"CLEANUP\") print(\"=\" * 60)  import os import shutil  # Clean up test files files_to_remove = [     \"company_network.annnet\",     \"network_2023.annnet\", ]  print(\"\\nRemoving test files:\") for filepath in files_to_remove:     if os.path.exists(filepath):         if os.path.isdir(filepath):             shutil.rmtree(filepath)             print(f\"   \u2713 Removed directory: {filepath}\")         else:             os.remove(filepath)             print(f\"   \u2713 Removed file: {filepath}\")     else:         print(f\"   \u2298 Not found: {filepath}\")  print(\"\\n\u2713 Cleanup complete\") <pre>\n============================================================\nCLEANUP\n============================================================\n\nRemoving test files:\n   \u2298 Not found: company_network.annnet\n   \u2298 Not found: network_2023.annnet\n\n\u2713 Cleanup complete\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"sbml_io/#sbml-adapter-elowitz-repressilator","title":"SBML adapter (Elowitz repressilator)\u00b6","text":""},{"location":"sbml_io/#annnet-api","title":"AnnNet API\u00b6","text":""},{"location":"stress_notebook/","title":"Graph \u2014 End\u2011to\u2011End Stress Notebook","text":"In\u00a0[2]: Copied! <pre># Robust import of Graph\nimport os\nimport random\nimport sys\nfrom time import perf_counter\n\nsys.path.insert(0, os.path.abspath(\"..\"))  # must be the parent folder that CONTAINS 'annnet'\n\nfrom annnet.core.graph import Graph\n\nG = Graph(directed=True)\n\n\nimport polars as pl\n</pre> # Robust import of Graph import os import random import sys from time import perf_counter  sys.path.insert(0, os.path.abspath(\"..\"))  # must be the parent folder that CONTAINS 'annnet'  from annnet.core.graph import Graph  G = Graph(directed=True)   import polars as pl In\u00a0[4]: Copied! <pre># Parameters \u2014 choose a scale\n# - DEMO runs fast on laptops\n# - STRESS creates 10^4\u201310^5 scale objects; adjust upward to your machine limits\n\nSCALE = \"DEMO\"  # \"DEMO\" or \"STRESS\"\n\nif SCALE.upper() == \"DEMO\":\n    N_PROTEINS = 5_00\n    N_TRANSCRIPTS = 2_00\n    N_METABOLITES = 1_00\n    N_EDGE_ENTITIES = 40\n    N_BIN_EDGES = 25_00  # binary protein-protein interactions (base slice)\n    N_HYPER_COMPLEX = 1_00  # undirected complexes\n    N_HYPER_CASCADE = 1_00  # directed signaling cascades\n    N_vertex_EDGE_BIDIR = 2_00  # vertex&lt;-&gt;edge-entity links (counted as pairs)\nelse:\n    N_PROTEINS = 30_000\n    N_TRANSCRIPTS = 12_000\n    N_METABOLITES = 8_000\n    N_EDGE_ENTITIES = 2_500\n    N_BIN_EDGES = 160_000\n    N_HYPER_COMPLEX = 4, 000  # use commas? We'll correct below to int\n    N_HYPER_CASCADE = 4_000\n    N_vertex_EDGE_BIDIR = 10_000\n\n# fix typo for N_HYPER_COMPLEX in STRESS case\nif isinstance(N_HYPER_COMPLEX, tuple):\n    N_HYPER_COMPLEX = 4000\n\nsliceS = [\"Healthy\", \"Stressed\", \"Disease\", \"DrugA\", \"DrugB\"]\nORDERED_FOR_TEMPORAL = [\"Healthy\", \"Stressed\", \"Disease\", \"DrugA\", \"DrugB\"]\n\n# How many parallel edges to create as duplicates between random pairs\nN_PARALLEL_DUPES = max(1, N_BIN_EDGES // 20)\n\n# Fraction of vertices seeded into each non-default slice (to make propagate='shared'/'all' meaningful)\nSEED_FRAC_PER_slice = 0.6\n</pre> # Parameters \u2014 choose a scale # - DEMO runs fast on laptops # - STRESS creates 10^4\u201310^5 scale objects; adjust upward to your machine limits  SCALE = \"DEMO\"  # \"DEMO\" or \"STRESS\"  if SCALE.upper() == \"DEMO\":     N_PROTEINS = 5_00     N_TRANSCRIPTS = 2_00     N_METABOLITES = 1_00     N_EDGE_ENTITIES = 40     N_BIN_EDGES = 25_00  # binary protein-protein interactions (base slice)     N_HYPER_COMPLEX = 1_00  # undirected complexes     N_HYPER_CASCADE = 1_00  # directed signaling cascades     N_vertex_EDGE_BIDIR = 2_00  # vertex&lt;-&gt;edge-entity links (counted as pairs) else:     N_PROTEINS = 30_000     N_TRANSCRIPTS = 12_000     N_METABOLITES = 8_000     N_EDGE_ENTITIES = 2_500     N_BIN_EDGES = 160_000     N_HYPER_COMPLEX = 4, 000  # use commas? We'll correct below to int     N_HYPER_CASCADE = 4_000     N_vertex_EDGE_BIDIR = 10_000  # fix typo for N_HYPER_COMPLEX in STRESS case if isinstance(N_HYPER_COMPLEX, tuple):     N_HYPER_COMPLEX = 4000  sliceS = [\"Healthy\", \"Stressed\", \"Disease\", \"DrugA\", \"DrugB\"] ORDERED_FOR_TEMPORAL = [\"Healthy\", \"Stressed\", \"Disease\", \"DrugA\", \"DrugB\"]  # How many parallel edges to create as duplicates between random pairs N_PARALLEL_DUPES = max(1, N_BIN_EDGES // 20)  # Fraction of vertices seeded into each non-default slice (to make propagate='shared'/'all' meaningful) SEED_FRAC_PER_slice = 0.6 In\u00a0[6]: Copied! <pre># Helpers\n\n\ndef rand_weight(base=1.0, jitter=0.5):\n    # positive weight with variability\n    w = base + (random.random() - 0.5) * 2 * jitter\n    return max(0.01, w)\n\n\ndef try_to_pandas(df):\n    if df is None:\n        return None\n    if \"polars\" in type(df).__module__.lower():\n        return df.to_pandas() if hasattr(df, \"to_pandas\") else None\n    return df  # assume already pandas-like\n\n\ndef head_df(df, n=5):\n    p = try_to_pandas(df)\n    return p.head(n) if p is not None else None\n</pre> # Helpers   def rand_weight(base=1.0, jitter=0.5):     # positive weight with variability     w = base + (random.random() - 0.5) * 2 * jitter     return max(0.01, w)   def try_to_pandas(df):     if df is None:         return None     if \"polars\" in type(df).__module__.lower():         return df.to_pandas() if hasattr(df, \"to_pandas\") else None     return df  # assume already pandas-like   def head_df(df, n=5):     p = try_to_pandas(df)     return p.head(n) if p is not None else None In\u00a0[8]: Copied! <pre># Build graph &amp; slices\nt0 = perf_counter()\nG = Graph(directed=True)\n\nfor lid in sliceS:\n    G.add_slice(lid, desc=f\"condition={lid}\")\nG.set_active_slice(sliceS[0])\nbuild_slices_time = perf_counter() - t0\nprint(\"slices ready:\", G.list_slices(), \"active:\", G.get_active_slice())\n</pre> # Build graph &amp; slices t0 = perf_counter() G = Graph(directed=True)  for lid in sliceS:     G.add_slice(lid, desc=f\"condition={lid}\") G.set_active_slice(sliceS[0]) build_slices_time = perf_counter() - t0 print(\"slices ready:\", G.list_slices(), \"active:\", G.get_active_slice()) <pre>slices ready: ['Healthy', 'Stressed', 'Disease', 'DrugA', 'DrugB'] active: Healthy\n</pre> In\u00a0[10]: Copied! <pre># ---- progress helpers ----\nfrom time import perf_counter\n\ntry:\n    from tqdm.auto import tqdm  # uses notebook bar if available\n\n    _TQDM = True\nexcept Exception:\n    _TQDM = False\n\n\ndef prog_iter(it, total=None, desc=\"\", mininterval=0.25):\n    \"\"\"Wrap any iterable with a progress display (tqdm if available, else no-op).\"\"\"\n    if _TQDM:\n        return tqdm(it, total=total, desc=desc, mininterval=mininterval, leave=False)\n    return it\n\n\ndef batched(iterable, batch_size):\n    \"\"\"Yield lists of size &lt;= batch_size (Py&lt;3.12 compatible).\"\"\"\n    buf = []\n    for x in iterable:\n        buf.append(x)\n        if len(buf) == batch_size:\n            yield buf\n            buf = []\n    if buf:\n        yield buf\n</pre> # ---- progress helpers ---- from time import perf_counter  try:     from tqdm.auto import tqdm  # uses notebook bar if available      _TQDM = True except Exception:     _TQDM = False   def prog_iter(it, total=None, desc=\"\", mininterval=0.25):     \"\"\"Wrap any iterable with a progress display (tqdm if available, else no-op).\"\"\"     if _TQDM:         return tqdm(it, total=total, desc=desc, mininterval=mininterval, leave=False)     return it   def batched(iterable, batch_size):     \"\"\"Yield lists of size &lt;= batch_size (Py&lt;3.12 compatible).\"\"\"     buf = []     for x in iterable:         buf.append(x)         if len(buf) == batch_size:             yield buf             buf = []     if buf:         yield buf In\u00a0[12]: Copied! <pre>import numpy as np\n\nrng = np.random.default_rng(42)\n\nt = perf_counter()\n\nproteins = [f\"P{i}\" for i in range(1, N_PROTEINS + 1)]\ntranscripts = [f\"T{i}\" for i in range(1, N_TRANSCRIPTS + 1)]\nmetabolites = [f\"M{i}\" for i in range(1, N_METABOLITES + 1)]\nedge_entities = [f\"EE{i}\" for i in range(1, N_EDGE_ENTITIES + 1)]\n\n# --- Seed vertices in \"Healthy\" ---\nkinase_mask = rng.random(len(proteins)) &lt; 0.15\nG.add_vertices_bulk(\n    (\n        {\"vertex_id\": p, \"kind\": \"protein\", **({\"family\": \"kinase\"} if km else {})}\n        for p, km in zip(proteins, kinase_mask)\n    ),\n    slice=\"Healthy\",\n)\nG.add_vertices_bulk(({\"vertex_id\": t, \"kind\": \"transcript\"} for t in transcripts), slice=\"Healthy\")\nG.add_vertices_bulk(({\"vertex_id\": m, \"kind\": \"metabolite\"} for m in metabolites), slice=\"Healthy\")\n\n# --- Edge-entities in \"Healthy\" (bulk) ---\npathways = np.array([\"glycolysis\", \"tca\", \"mapk\", \"pi3k\"])\ndrawn_pathways = pathways[rng.integers(0, len(pathways), size=len(edge_entities))]\nG.add_edge_entities_bulk(\n    (\n        {\"edge_entity_id\": ee, \"role\": \"enzyme\", \"pathway\": pw}\n        for ee, pw in zip(edge_entities, drawn_pathways)\n    ),\n    slice=\"Healthy\",\n)\n\n# --- Seed presence into other slices (bulk per slice) ---\np_keep = SEED_FRAC_PER_slice\nfor lid in sliceS[1:]:\n    pmask = rng.random(len(proteins)) &lt; p_keep\n    tmask = rng.random(len(transcripts)) &lt; p_keep\n    mmask = rng.random(len(metabolites)) &lt; p_keep\n\n    G.add_vertices_bulk(\n        ({\"vertex_id\": p, \"kind\": \"protein\"} for p, keep in zip(proteins, pmask) if keep),\n        slice=lid,\n    )\n    G.add_vertices_bulk(\n        ({\"vertex_id\": t, \"kind\": \"transcript\"} for t, keep in zip(transcripts, tmask) if keep),\n        slice=lid,\n    )\n    G.add_vertices_bulk(\n        ({\"vertex_id\": m, \"kind\": \"metabolite\"} for m, keep in zip(metabolites, mmask) if keep),\n        slice=lid,\n    )\n\nbuild_vertices_time = perf_counter() - t\nprint(\n    \"vertices done. #vertices:\",\n    G.number_of_vertices(),\n    \"Edge-entities:\",\n    sum(1 for k, v in G.entity_types.items() if v == \"edge\"),\n    \"time(s)=\",\n    round(build_vertices_time, 3),\n)\n</pre> import numpy as np  rng = np.random.default_rng(42)  t = perf_counter()  proteins = [f\"P{i}\" for i in range(1, N_PROTEINS + 1)] transcripts = [f\"T{i}\" for i in range(1, N_TRANSCRIPTS + 1)] metabolites = [f\"M{i}\" for i in range(1, N_METABOLITES + 1)] edge_entities = [f\"EE{i}\" for i in range(1, N_EDGE_ENTITIES + 1)]  # --- Seed vertices in \"Healthy\" --- kinase_mask = rng.random(len(proteins)) &lt; 0.15 G.add_vertices_bulk(     (         {\"vertex_id\": p, \"kind\": \"protein\", **({\"family\": \"kinase\"} if km else {})}         for p, km in zip(proteins, kinase_mask)     ),     slice=\"Healthy\", ) G.add_vertices_bulk(({\"vertex_id\": t, \"kind\": \"transcript\"} for t in transcripts), slice=\"Healthy\") G.add_vertices_bulk(({\"vertex_id\": m, \"kind\": \"metabolite\"} for m in metabolites), slice=\"Healthy\")  # --- Edge-entities in \"Healthy\" (bulk) --- pathways = np.array([\"glycolysis\", \"tca\", \"mapk\", \"pi3k\"]) drawn_pathways = pathways[rng.integers(0, len(pathways), size=len(edge_entities))] G.add_edge_entities_bulk(     (         {\"edge_entity_id\": ee, \"role\": \"enzyme\", \"pathway\": pw}         for ee, pw in zip(edge_entities, drawn_pathways)     ),     slice=\"Healthy\", )  # --- Seed presence into other slices (bulk per slice) --- p_keep = SEED_FRAC_PER_slice for lid in sliceS[1:]:     pmask = rng.random(len(proteins)) &lt; p_keep     tmask = rng.random(len(transcripts)) &lt; p_keep     mmask = rng.random(len(metabolites)) &lt; p_keep      G.add_vertices_bulk(         ({\"vertex_id\": p, \"kind\": \"protein\"} for p, keep in zip(proteins, pmask) if keep),         slice=lid,     )     G.add_vertices_bulk(         ({\"vertex_id\": t, \"kind\": \"transcript\"} for t, keep in zip(transcripts, tmask) if keep),         slice=lid,     )     G.add_vertices_bulk(         ({\"vertex_id\": m, \"kind\": \"metabolite\"} for m, keep in zip(metabolites, mmask) if keep),         slice=lid,     )  build_vertices_time = perf_counter() - t print(     \"vertices done. #vertices:\",     G.number_of_vertices(),     \"Edge-entities:\",     sum(1 for k, v in G.entity_types.items() if v == \"edge\"),     \"time(s)=\",     round(build_vertices_time, 3), ) <pre>vertices done. #vertices: 800 Edge-entities: 40 time(s)= 0.41\n</pre> In\u00a0[14]: Copied! <pre># Binary edges (PPIs mostly among proteins), defined in Healthy then sliceed variants\nfrom time import perf_counter\n\nt = perf_counter()\n\n# ---------- 1) Bulk create binary edges on \"Healthy\" ----------\npairs = []\nneed = N_BIN_EDGES\nnames = proteins\nn = len(names)\n\n# Generate candidate pairs quickly; reject self-loops\nwhile len(pairs) &lt; need:\n    k = min(need - len(pairs), max(1024, need // 4))\n    us = random.choices(names, k=k)\n    vs = random.choices(names, k=k)\n    for u, v in zip(us, vs):\n        if u != v:\n            pairs.append((u, v))\n        if len(pairs) == need:\n            break\n\ndirs = [random.random() &lt; 0.8 for _ in range(need)]\nws = [rand_weight(1.2, 0.6) for _ in range(need)]\n\nbulk = [\n    {\n        \"source\": u,\n        \"target\": v,\n        \"weight\": w,\n        \"edge_directed\": d,\n        \"edge_type\": \"regular\",\n        \"slice\": \"Healthy\",\n    }\n    for (u, v), w, d in zip(pairs, ws, dirs)\n]\nppis = G.add_edges_bulk(bulk, slice=\"Healthy\")  # list of edge_ids\n\n# ---------- 2) Bulk add parallel dupes ----------\nif ppis and N_PARALLEL_DUPES &gt; 0:\n    chosen = random.choices(ppis, k=N_PARALLEL_DUPES)\n    par_edges = []\n    for eid in chosen:\n        u, v, _ = G.edge_definitions[eid]\n        par_edges.append(\n            {\n                \"source\": u,\n                \"target\": v,\n                \"weight\": rand_weight(1.0, 0.3),\n                \"edge_type\": \"regular\",\n                \"slice\": \"Healthy\",\n            }\n        )\n    G.add_edges_bulk(par_edges, slice=\"Healthy\")\n\n# ---------- 3) Bulk per-slice variants ----------\nbase_w = {eid: G.edge_weights[eid] for eid in ppis}\n\nfor lid in sliceS[1:]:\n    # Add all PPI edges to this slice in one shot\n    G.add_edges_to_slice_bulk(lid, ppis)\n\n    # Compute modifiers and upsert all weights for this slice at once\n    weights_rows = []\n    for eid in ppis:\n        bw = base_w[eid]\n        factor = {\n            \"Stressed\": rand_weight(1.10, 0.10),\n            \"Disease\": (0.7 if random.random() &lt; 0.4 else rand_weight(1.30, 0.15)),\n            \"DrugA\": rand_weight(0.9, 0.25),\n            \"DrugB\": rand_weight(1.2, 0.20),\n        }[lid]\n        weights_rows.append((eid, {\"weight\": bw * factor, \"note\": f\"slice={lid}\"}))\n\n    G.set_edge_slice_attrs_bulk(lid, weights_rows)\n\nbuild_binary_time = perf_counter() - t\nprint(\n    \"Binary edges built:\",\n    len(ppis),\n    \"total edges now:\",\n    G.number_of_edges(),\n    \"time(s)=\",\n    round(build_binary_time, 3),\n)\n</pre> # Binary edges (PPIs mostly among proteins), defined in Healthy then sliceed variants from time import perf_counter  t = perf_counter()  # ---------- 1) Bulk create binary edges on \"Healthy\" ---------- pairs = [] need = N_BIN_EDGES names = proteins n = len(names)  # Generate candidate pairs quickly; reject self-loops while len(pairs) &lt; need:     k = min(need - len(pairs), max(1024, need // 4))     us = random.choices(names, k=k)     vs = random.choices(names, k=k)     for u, v in zip(us, vs):         if u != v:             pairs.append((u, v))         if len(pairs) == need:             break  dirs = [random.random() &lt; 0.8 for _ in range(need)] ws = [rand_weight(1.2, 0.6) for _ in range(need)]  bulk = [     {         \"source\": u,         \"target\": v,         \"weight\": w,         \"edge_directed\": d,         \"edge_type\": \"regular\",         \"slice\": \"Healthy\",     }     for (u, v), w, d in zip(pairs, ws, dirs) ] ppis = G.add_edges_bulk(bulk, slice=\"Healthy\")  # list of edge_ids  # ---------- 2) Bulk add parallel dupes ---------- if ppis and N_PARALLEL_DUPES &gt; 0:     chosen = random.choices(ppis, k=N_PARALLEL_DUPES)     par_edges = []     for eid in chosen:         u, v, _ = G.edge_definitions[eid]         par_edges.append(             {                 \"source\": u,                 \"target\": v,                 \"weight\": rand_weight(1.0, 0.3),                 \"edge_type\": \"regular\",                 \"slice\": \"Healthy\",             }         )     G.add_edges_bulk(par_edges, slice=\"Healthy\")  # ---------- 3) Bulk per-slice variants ---------- base_w = {eid: G.edge_weights[eid] for eid in ppis}  for lid in sliceS[1:]:     # Add all PPI edges to this slice in one shot     G.add_edges_to_slice_bulk(lid, ppis)      # Compute modifiers and upsert all weights for this slice at once     weights_rows = []     for eid in ppis:         bw = base_w[eid]         factor = {             \"Stressed\": rand_weight(1.10, 0.10),             \"Disease\": (0.7 if random.random() &lt; 0.4 else rand_weight(1.30, 0.15)),             \"DrugA\": rand_weight(0.9, 0.25),             \"DrugB\": rand_weight(1.2, 0.20),         }[lid]         weights_rows.append((eid, {\"weight\": bw * factor, \"note\": f\"slice={lid}\"}))      G.set_edge_slice_attrs_bulk(lid, weights_rows)  build_binary_time = perf_counter() - t print(     \"Binary edges built:\",     len(ppis),     \"total edges now:\",     G.number_of_edges(),     \"time(s)=\",     round(build_binary_time, 3), ) <pre>Binary edges built: 2500 total edges now: 2625 time(s)= 0.133\n</pre> In\u00a0[16]: Copied! <pre># Propagation semantics via add_edge(..., propagate=...)\nt = perf_counter()\n# Ensure varied vertex presence across slices for a few pairs\npairs = [(random.choice(proteins), random.choice(transcripts)) for _ in range(2000)]\nfor u, v in pairs:\n    # 'shared': only slices where both endpoints already present\n    G.add_edge(\n        u, v, slice=\"Healthy\", edge_type=\"regular\", weight=rand_weight(0.8, 0.2), propagate=\"shared\"\n    )\npairs2 = [(random.choice(proteins), random.choice(metabolites)) for _ in range(2000)]\nfor u, v in pairs2:\n    # 'all': appears everywhere either endpoint exists (pulls other endpoint in)\n    G.add_edge(\n        u, v, slice=\"Healthy\", edge_type=\"regular\", weight=rand_weight(0.8, 0.2), propagate=\"all\"\n    )\n\nbuild_propagation_time = perf_counter() - t\nprint(\"Propagation examples added (shared/all).\")\n</pre> # Propagation semantics via add_edge(..., propagate=...) t = perf_counter() # Ensure varied vertex presence across slices for a few pairs pairs = [(random.choice(proteins), random.choice(transcripts)) for _ in range(2000)] for u, v in pairs:     # 'shared': only slices where both endpoints already present     G.add_edge(         u, v, slice=\"Healthy\", edge_type=\"regular\", weight=rand_weight(0.8, 0.2), propagate=\"shared\"     ) pairs2 = [(random.choice(proteins), random.choice(metabolites)) for _ in range(2000)] for u, v in pairs2:     # 'all': appears everywhere either endpoint exists (pulls other endpoint in)     G.add_edge(         u, v, slice=\"Healthy\", edge_type=\"regular\", weight=rand_weight(0.8, 0.2), propagate=\"all\"     )  build_propagation_time = perf_counter() - t print(\"Propagation examples added (shared/all).\") <pre>Propagation examples added (shared/all).\n</pre> In\u00a0[18]: Copied! <pre># Hyperedges: undirected complexes, directed cascades\nt = perf_counter()\n\ncomplex_ids = []\nfor _ in range(N_HYPER_COMPLEX):\n    size = random.choice([3, 4, 5, 6])\n    members = set(random.sample(proteins, size))\n    hid = G.add_hyperedge(\n        members=members, slice=\"Healthy\", weight=rand_weight(1.0, 0.2), tag=\"complex\"\n    )\n    complex_ids.append(hid)\n    for lid in sliceS[1:]:\n        G.add_edge_to_slice(lid, hid)\n\ncascade_ids = []\ntries = 0\nwhile len(cascade_ids) &lt; N_HYPER_CASCADE and tries &lt; N_HYPER_CASCADE * 5:\n    tries += 1\n    head = set(random.sample(proteins, random.choice([1, 2])))\n    tail = set(random.sample(proteins, random.choice([2, 3, 4])))\n    if head &amp; tail:\n        continue\n    hid = G.add_hyperedge(\n        head=head, tail=tail, slice=\"Healthy\", weight=rand_weight(1.0, 0.4), tag=\"cascade\"\n    )\n    cascade_ids.append(hid)\n    for lid in sliceS[1:]:\n        G.add_edge_to_slice(lid, hid)\n\nbuild_hyper_time = perf_counter() - t\nprint(\"Hyperedges built: complexes=\", len(complex_ids), \"cascades=\", len(cascade_ids))\n</pre> # Hyperedges: undirected complexes, directed cascades t = perf_counter()  complex_ids = [] for _ in range(N_HYPER_COMPLEX):     size = random.choice([3, 4, 5, 6])     members = set(random.sample(proteins, size))     hid = G.add_hyperedge(         members=members, slice=\"Healthy\", weight=rand_weight(1.0, 0.2), tag=\"complex\"     )     complex_ids.append(hid)     for lid in sliceS[1:]:         G.add_edge_to_slice(lid, hid)  cascade_ids = [] tries = 0 while len(cascade_ids) &lt; N_HYPER_CASCADE and tries &lt; N_HYPER_CASCADE * 5:     tries += 1     head = set(random.sample(proteins, random.choice([1, 2])))     tail = set(random.sample(proteins, random.choice([2, 3, 4])))     if head &amp; tail:         continue     hid = G.add_hyperedge(         head=head, tail=tail, slice=\"Healthy\", weight=rand_weight(1.0, 0.4), tag=\"cascade\"     )     cascade_ids.append(hid)     for lid in sliceS[1:]:         G.add_edge_to_slice(lid, hid)  build_hyper_time = perf_counter() - t print(\"Hyperedges built: complexes=\", len(complex_ids), \"cascades=\", len(cascade_ids)) <pre>Hyperedges built: complexes= 100 cascades= 100\n</pre> In\u00a0[20]: Copied! <pre># vertex\u2013edge (edge-entity) reactions: A -&gt; EE -&gt; B (bidir variants)\nt = perf_counter()\nfor _ in range(N_vertex_EDGE_BIDIR):\n    ee = random.choice(edge_entities)\n    s = random.choice(proteins + transcripts + metabolites)\n    tvertex = random.choice(proteins + transcripts + metabolites)\n    G.add_edge(s, ee, slice=\"Healthy\", edge_type=\"vertex_edge\", weight=rand_weight(1.0, 0.5))\n    G.add_edge(ee, tvertex, slice=\"Healthy\", edge_type=\"vertex_edge\", weight=rand_weight(1.0, 0.5))\n    # reflect into other slices\n    for lid in sliceS[1:]:\n        G.add_edge_to_slice(lid, list(G.get_edge_ids(s, ee))[-1])\n        G.add_edge_to_slice(lid, list(G.get_edge_ids(ee, tvertex))[-1])\n\nbuild_vertexedge_time = perf_counter() - t\nprint(\"vertex\u2013edge reaction links added (pairs):\", N_vertex_EDGE_BIDIR)\n</pre> # vertex\u2013edge (edge-entity) reactions: A -&gt; EE -&gt; B (bidir variants) t = perf_counter() for _ in range(N_vertex_EDGE_BIDIR):     ee = random.choice(edge_entities)     s = random.choice(proteins + transcripts + metabolites)     tvertex = random.choice(proteins + transcripts + metabolites)     G.add_edge(s, ee, slice=\"Healthy\", edge_type=\"vertex_edge\", weight=rand_weight(1.0, 0.5))     G.add_edge(ee, tvertex, slice=\"Healthy\", edge_type=\"vertex_edge\", weight=rand_weight(1.0, 0.5))     # reflect into other slices     for lid in sliceS[1:]:         G.add_edge_to_slice(lid, list(G.get_edge_ids(s, ee))[-1])         G.add_edge_to_slice(lid, list(G.get_edge_ids(ee, tvertex))[-1])  build_vertexedge_time = perf_counter() - t print(\"vertex\u2013edge reaction links added (pairs):\", N_vertex_EDGE_BIDIR) <pre>vertex\u2013edge reaction links added (pairs): 200\n</pre> In\u00a0[22]: Copied! <pre># Sanity &amp; counts\nprint(\"vertices:\", G.number_of_vertices(), \"Edges:\", G.number_of_edges())\nassert G.number_of_vertices() &gt; 0 and G.number_of_edges() &gt; 0\n\n# Edge-entity count\nedge_entity_count = sum(\n    1 for _id, et in G.entity_types.items() if et == \"edge\" and _id in set(edge_entities)\n)\nprint(\"Edge-entities:\", edge_entity_count)\nassert edge_entity_count == len(edge_entities)\n</pre> # Sanity &amp; counts print(\"vertices:\", G.number_of_vertices(), \"Edges:\", G.number_of_edges()) assert G.number_of_vertices() &gt; 0 and G.number_of_edges() &gt; 0  # Edge-entity count edge_entity_count = sum(     1 for _id, et in G.entity_types.items() if et == \"edge\" and _id in set(edge_entities) ) print(\"Edge-entities:\", edge_entity_count) assert edge_entity_count == len(edge_entities) <pre>vertices: 800 Edges: 7225\nEdge-entities: 40\n</pre> In\u00a0[24]: Copied! <pre># Views &amp; Top edges per slice\ntry:\n    for lid in sliceS:\n        EV = G.edges_view(slice=lid, resolved_weight=True)\n        print(\n            f\"[{lid}] edges_view rows:\", getattr(EV, \"height\", getattr(EV, \"shape\", [\"?\", \"?\"])[0])\n        )\n        # filter binary only, sort by effective weight\n        if pl is not None and isinstance(EV, pl.DataFrame):\n            top = (\n                EV.filter(pl.col(\"kind\") == \"binary\")\n                .sort(\"effective_weight\", descending=True)\n                .select([\"edge_id\", \"source\", \"target\", \"effective_weight\"])\n                .head(5)\n            )\n            print(f\"Top 5 binary edges in {lid}:\\n\", top)\n        else:\n            # Try pandas-like\n            try:\n                df = EV\n                if hasattr(df, \"query\"):\n                    bf = (\n                        df.query(\"kind == 'binary'\")\n                        .sort_values(\"effective_weight\", ascending=False)\n                        .head(5)\n                    )\n                    print(bf[[\"edge_id\", \"source\", \"target\", \"effective_weight\"]])\n            except Exception:\n                pass\nexcept Exception as e:\n    print(\"edges_view failed softly:\", e)\n\ntry:\n    NV = G.vertices_view()\n    LV = G.slices_view()\n    print(\"vertices view cols:\", getattr(NV, \"columns\", None))\n    print(\"slices view cols:\", getattr(LV, \"columns\", None))\nexcept Exception as e:\n    print(\"vertices_view/slices_view failed softly:\", e)\n</pre> # Views &amp; Top edges per slice try:     for lid in sliceS:         EV = G.edges_view(slice=lid, resolved_weight=True)         print(             f\"[{lid}] edges_view rows:\", getattr(EV, \"height\", getattr(EV, \"shape\", [\"?\", \"?\"])[0])         )         # filter binary only, sort by effective weight         if pl is not None and isinstance(EV, pl.DataFrame):             top = (                 EV.filter(pl.col(\"kind\") == \"binary\")                 .sort(\"effective_weight\", descending=True)                 .select([\"edge_id\", \"source\", \"target\", \"effective_weight\"])                 .head(5)             )             print(f\"Top 5 binary edges in {lid}:\\n\", top)         else:             # Try pandas-like             try:                 df = EV                 if hasattr(df, \"query\"):                     bf = (                         df.query(\"kind == 'binary'\")                         .sort_values(\"effective_weight\", ascending=False)                         .head(5)                     )                     print(bf[[\"edge_id\", \"source\", \"target\", \"effective_weight\"]])             except Exception:                 pass except Exception as e:     print(\"edges_view failed softly:\", e)  try:     NV = G.vertices_view()     LV = G.slices_view()     print(\"vertices view cols:\", getattr(NV, \"columns\", None))     print(\"slices view cols:\", getattr(LV, \"columns\", None)) except Exception as e:     print(\"vertices_view/slices_view failed softly:\", e) <pre>[Healthy] edges_view rows: 7225\nTop 5 binary edges in Healthy:\n shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id   \u2506 source \u2506 target \u2506 effective_weight \u2502\n\u2502 ---       \u2506 ---    \u2506 ---    \u2506 ---              \u2502\n\u2502 str       \u2506 str    \u2506 str    \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_104  \u2506 P416   \u2506 P182   \u2506 1.798773         \u2502\n\u2502 edge_653  \u2506 P231   \u2506 P147   \u2506 1.798487         \u2502\n\u2502 edge_65   \u2506 P144   \u2506 P324   \u2506 1.798136         \u2502\n\u2502 edge_1855 \u2506 P215   \u2506 P14    \u2506 1.797699         \u2502\n\u2502 edge_721  \u2506 P389   \u2506 P47    \u2506 1.797358         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[Stressed] edges_view rows: 7225\nTop 5 binary edges in Stressed:\n shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id   \u2506 source \u2506 target \u2506 effective_weight \u2502\n\u2502 ---       \u2506 ---    \u2506 ---    \u2506 ---              \u2502\n\u2502 str       \u2506 str    \u2506 str    \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_1489 \u2506 P319   \u2506 P305   \u2506 2.144127         \u2502\n\u2502 edge_2051 \u2506 P223   \u2506 P310   \u2506 2.123425         \u2502\n\u2502 edge_1697 \u2506 P468   \u2506 P208   \u2506 2.116178         \u2502\n\u2502 edge_2084 \u2506 P354   \u2506 P475   \u2506 2.112782         \u2502\n\u2502 edge_1472 \u2506 P92    \u2506 P125   \u2506 2.104228         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[Disease] edges_view rows: 7225\nTop 5 binary edges in Disease:\n shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id   \u2506 source \u2506 target \u2506 effective_weight \u2502\n\u2502 ---       \u2506 ---    \u2506 ---    \u2506 ---              \u2502\n\u2502 str       \u2506 str    \u2506 str    \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_1256 \u2506 P199   \u2506 P228   \u2506 2.584078         \u2502\n\u2502 edge_319  \u2506 P1     \u2506 P172   \u2506 2.5715           \u2502\n\u2502 edge_680  \u2506 P340   \u2506 P369   \u2506 2.569214         \u2502\n\u2502 edge_1696 \u2506 P117   \u2506 P448   \u2506 2.561676         \u2502\n\u2502 edge_660  \u2506 P437   \u2506 P312   \u2506 2.54105          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[DrugA] edges_view rows: 7225\nTop 5 binary edges in DrugA:\n shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id   \u2506 source \u2506 target \u2506 effective_weight \u2502\n\u2502 ---       \u2506 ---    \u2506 ---    \u2506 ---              \u2502\n\u2502 str       \u2506 str    \u2506 str    \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_513  \u2506 P247   \u2506 P137   \u2506 2.058131         \u2502\n\u2502 edge_1753 \u2506 P38    \u2506 P3     \u2506 2.026587         \u2502\n\u2502 edge_1472 \u2506 P92    \u2506 P125   \u2506 2.021271         \u2502\n\u2502 edge_451  \u2506 P416   \u2506 P280   \u2506 2.020798         \u2502\n\u2502 edge_834  \u2506 P131   \u2506 P302   \u2506 2.015885         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n[DrugB] edges_view rows: 7225\nTop 5 binary edges in DrugB:\n shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 edge_id   \u2506 source \u2506 target \u2506 effective_weight \u2502\n\u2502 ---       \u2506 ---    \u2506 ---    \u2506 ---              \u2502\n\u2502 str       \u2506 str    \u2506 str    \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 edge_692  \u2506 P280   \u2506 P330   \u2506 2.499118         \u2502\n\u2502 edge_1256 \u2506 P199   \u2506 P228   \u2506 2.485477         \u2502\n\u2502 edge_307  \u2506 P53    \u2506 P445   \u2506 2.474695         \u2502\n\u2502 edge_2476 \u2506 P469   \u2506 P448   \u2506 2.47073          \u2502\n\u2502 edge_65   \u2506 P144   \u2506 P324   \u2506 2.456394         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nvertices view cols: ['vertex_id', 'kind', 'family', 'pathway', 'role']\nslices view cols: ['slice_id', 'desc']\n</pre> In\u00a0[26]: Copied! <pre># Presence queries\nany_e = next(iter(G.edge_to_idx.keys()))\nprint(\"Edge presence across slices:\", G.edge_presence_across_slices(edge_id=any_e))\n\nany_p = random.choice(proteins)\nprint(\"vertex presence across slices:\", G.vertex_presence_across_slices(any_p))\n\n# Hyperedge presence by members and head/tail\nif complex_ids:\n    m = random.choice(complex_ids)\n    members = G.hyperedge_definitions[m].get(\"members\", set())\n    if members:\n        print(\n            \"Hyperedge presence (members):\",\n            G.hyperedge_presence_across_slices(members=set(members)),\n        )\nif cascade_ids:\n    h = random.choice(cascade_ids)\n    hd = G.hyperedge_definitions[h]\n    if hd.get(\"head\") and hd.get(\"tail\"):\n        print(\n            \"Hyperedge presence (head/tail):\",\n            G.hyperedge_presence_across_slices(head=set(hd[\"head\"]), tail=set(hd[\"tail\"])),\n        )\n</pre> # Presence queries any_e = next(iter(G.edge_to_idx.keys())) print(\"Edge presence across slices:\", G.edge_presence_across_slices(edge_id=any_e))  any_p = random.choice(proteins) print(\"vertex presence across slices:\", G.vertex_presence_across_slices(any_p))  # Hyperedge presence by members and head/tail if complex_ids:     m = random.choice(complex_ids)     members = G.hyperedge_definitions[m].get(\"members\", set())     if members:         print(             \"Hyperedge presence (members):\",             G.hyperedge_presence_across_slices(members=set(members)),         ) if cascade_ids:     h = random.choice(cascade_ids)     hd = G.hyperedge_definitions[h]     if hd.get(\"head\") and hd.get(\"tail\"):         print(             \"Hyperedge presence (head/tail):\",             G.hyperedge_presence_across_slices(head=set(hd[\"head\"]), tail=set(hd[\"tail\"])),         ) <pre>Edge presence across slices: ['Healthy', 'Stressed', 'Disease', 'DrugA', 'DrugB']\nvertex presence across slices: ['Healthy', 'Stressed', 'Disease', 'DrugA', 'DrugB']\nHyperedge presence (members): {'Healthy': ['edge_6689'], 'Stressed': ['edge_6689'], 'Disease': ['edge_6689'], 'DrugA': ['edge_6689'], 'DrugB': ['edge_6689']}\nHyperedge presence (head/tail): {'Healthy': ['edge_6762'], 'Stressed': ['edge_6762'], 'Disease': ['edge_6762'], 'DrugA': ['edge_6762'], 'DrugB': ['edge_6762']}\n</pre> In\u00a0[28]: Copied! <pre># Traversal\nq = random.choice(proteins)\nprint(f\"Neighbors({q}) sample:\", G.neighbors(q)[:10])\nprint(f\"Out({q}) sample:\", G.out_neighbors(q)[:10])\nprint(f\"In({q}) sample:\", G.in_neighbors(q)[:10])\n</pre> # Traversal q = random.choice(proteins) print(f\"Neighbors({q}) sample:\", G.neighbors(q)[:10]) print(f\"Out({q}) sample:\", G.out_neighbors(q)[:10]) print(f\"In({q}) sample:\", G.in_neighbors(q)[:10]) <pre>Neighbors(P459) sample: ['T108', 'P228', 'P76', 'P7', 'T2', 'M19', 'T181', 'T31', 'P37', 'P328']\nOut(P459) sample: ['T108', 'P76', 'P7', 'T2', 'M19', 'T181', 'T31', 'P37', 'P469']\nIn(P459) sample: ['P228', 'P7', 'P336', 'P475', 'P37', 'P328', 'P449', 'P283', 'EE16']\n</pre> In\u00a0[30]: Copied! <pre># slice analytics, conserved/specific, temporal\nstats = G.slice_statistics()\nprint(\"slice stats keys:\", list(stats.keys())[:5])\n\nconserved = G.conserved_edges(min_slices=len(sliceS))\nprint(\"Conserved edges (present in all slices):\", len(conserved))\n\ndisease_specific = G.slice_specific_edges(\"Disease\")\nprint(\"Disease-specific edges:\", len(disease_specific))\n\nchanges_e = G.temporal_dynamics(ORDERED_FOR_TEMPORAL, metric=\"edge_change\")\nchanges_n = G.temporal_dynamics(ORDERED_FOR_TEMPORAL, metric=\"vertex_change\")\nprint(\"Temporal edge changes entries:\", len(changes_e), \"vertex changes entries:\", len(changes_n))\n</pre> # slice analytics, conserved/specific, temporal stats = G.slice_statistics() print(\"slice stats keys:\", list(stats.keys())[:5])  conserved = G.conserved_edges(min_slices=len(sliceS)) print(\"Conserved edges (present in all slices):\", len(conserved))  disease_specific = G.slice_specific_edges(\"Disease\") print(\"Disease-specific edges:\", len(disease_specific))  changes_e = G.temporal_dynamics(ORDERED_FOR_TEMPORAL, metric=\"edge_change\") changes_n = G.temporal_dynamics(ORDERED_FOR_TEMPORAL, metric=\"vertex_change\") print(\"Temporal edge changes entries:\", len(changes_e), \"vertex changes entries:\", len(changes_n)) <pre>slice stats keys: ['Healthy', 'Stressed', 'Disease', 'DrugA', 'DrugB']\nConserved edges (present in all slices): 5327\nDisease-specific edges: 0\nTemporal edge changes entries: 4 vertex changes entries: 4\n</pre> In\u00a0[32]: Copied! <pre># slice set ops and derived slices\nu = G.slice_union([\"Healthy\", \"Stressed\"])\ni = G.slice_intersection([\"Healthy\", \"Stressed\"])\nd = G.slice_difference(\"Healthy\", \"Stressed\")\nprint(\"Union edges&gt;=intersection edges:\", len(u[\"edges\"]) &gt;= len(i[\"edges\"]))\n\nlid_u = G.create_slice_from_operation(\"HS_union\", u, desc=\"H\u222aS\")\nlid_i = G.create_slice_from_operation(\"HS_intersection\", i, desc=\"H\u2229S\")\nlid_d = G.create_slice_from_operation(\"H_minus_S\", d, desc=\"H\\\\S\")\nprint(\n    \"Derived slices exist:\",\n    G.has_slice(\"HS_union\"),\n    G.has_slice(\"HS_intersection\"),\n    G.has_slice(\"H_minus_S\"),\n)\n\n# Aggregations\nG.create_aggregated_slice([\"Healthy\", \"Stressed\"], \"Agg_union\", method=\"union\", tag=\"agg_u\")\nG.create_aggregated_slice(\n    [\"Healthy\", \"Stressed\"], \"Agg_intersection\", method=\"intersection\", tag=\"agg_i\"\n)\nprint(\"Aggregated slices added.\")\n</pre> # slice set ops and derived slices u = G.slice_union([\"Healthy\", \"Stressed\"]) i = G.slice_intersection([\"Healthy\", \"Stressed\"]) d = G.slice_difference(\"Healthy\", \"Stressed\") print(\"Union edges&gt;=intersection edges:\", len(u[\"edges\"]) &gt;= len(i[\"edges\"]))  lid_u = G.create_slice_from_operation(\"HS_union\", u, desc=\"H\u222aS\") lid_i = G.create_slice_from_operation(\"HS_intersection\", i, desc=\"H\u2229S\") lid_d = G.create_slice_from_operation(\"H_minus_S\", d, desc=\"H\\\\S\") print(     \"Derived slices exist:\",     G.has_slice(\"HS_union\"),     G.has_slice(\"HS_intersection\"),     G.has_slice(\"H_minus_S\"), )  # Aggregations G.create_aggregated_slice([\"Healthy\", \"Stressed\"], \"Agg_union\", method=\"union\", tag=\"agg_u\") G.create_aggregated_slice(     [\"Healthy\", \"Stressed\"], \"Agg_intersection\", method=\"intersection\", tag=\"agg_i\" ) print(\"Aggregated slices added.\") <pre>Union edges&gt;=intersection edges: True\nDerived slices exist: True True True\nAggregated slices added.\n</pre> In\u00a0[34]: Copied! <pre># Edge list &amp; global counts\nel = G.edge_list()\nprint(\"Edge list tuple length check (u,v,kind,id):\", all(len(t) == 4 for t in el))\nprint(\"Global entity/edge counts:\", G.global_entity_count(), G.global_edge_count())\n</pre> # Edge list &amp; global counts el = G.edge_list() print(\"Edge list tuple length check (u,v,kind,id):\", all(len(t) == 4 for t in el)) print(\"Global entity/edge counts:\", G.global_entity_count(), G.global_edge_count()) <pre>Edge list tuple length check (u,v,kind,id): True\nGlobal entity/edge counts: 840 7225\n</pre> In\u00a0[36]: Copied! <pre># Subgraph &amp; copy\nSG = G.subgraph_from_slice(\"DrugB\", resolve_slice_weights=True)\nprint(\"DrugB subgraph vertices/edges:\", SG.number_of_vertices(), SG.number_of_edges())\n\nCP = G.copy()\n# quick consistency checks\nassert set(CP.vertices()) == set(G.vertices())\nassert set(CP.edges()) == set(G.edges())\nany_hyper = next(e for e, k in G.edge_kind.items() if k == \"hyper\")\nassert CP.edge_kind.get(any_hyper) == \"hyper\"\nfor lid in G.list_slices(include_default=True):\n    assert CP._slices[lid][\"vertices\"] == G._slices[lid][\"vertices\"]\n    assert CP._slices[lid][\"edges\"] == G._slices[lid][\"edges\"]\nprint(\"Deep copy OK.\")\n</pre> # Subgraph &amp; copy SG = G.subgraph_from_slice(\"DrugB\", resolve_slice_weights=True) print(\"DrugB subgraph vertices/edges:\", SG.number_of_vertices(), SG.number_of_edges())  CP = G.copy() # quick consistency checks assert set(CP.vertices()) == set(G.vertices()) assert set(CP.edges()) == set(G.edges()) any_hyper = next(e for e, k in G.edge_kind.items() if k == \"hyper\") assert CP.edge_kind.get(any_hyper) == \"hyper\" for lid in G.list_slices(include_default=True):     assert CP._slices[lid][\"vertices\"] == G._slices[lid][\"vertices\"]     assert CP._slices[lid][\"edges\"] == G._slices[lid][\"edges\"] print(\"Deep copy OK.\") <pre>DrugB subgraph vertices/edges: 788 6193\nDeep copy OK.\n</pre> In\u00a0[37]: Copied! <pre># Removals: drop a slice of vertices/edges\n\n# Drop ~1% of proteins\ndrop_vertices = random.sample(proteins, max(1, len(proteins) // 100))\nG.remove_vertices(drop_vertices)  # one pass\n\n# Drop up to 500 edges\ndrop_edges = list(G.edge_to_idx.keys())[: min(500, len(G.edge_to_idx))]\nG.remove_edges(drop_edges)  # one pass\n\nprint(\"After removals: vertices=\", G.number_of_vertices(), \"edges=\", G.number_of_edges())\n</pre> # Removals: drop a slice of vertices/edges  # Drop ~1% of proteins drop_vertices = random.sample(proteins, max(1, len(proteins) // 100)) G.remove_vertices(drop_vertices)  # one pass  # Drop up to 500 edges drop_edges = list(G.edge_to_idx.keys())[: min(500, len(G.edge_to_idx))] G.remove_edges(drop_edges)  # one pass  print(\"After removals: vertices=\", G.number_of_vertices(), \"edges=\", G.number_of_edges()) <pre>After removals: vertices= 795 edges= 6603\n</pre> In\u00a0[39]: Copied! <pre># Audit &amp; memory\naudit = G.audit_attributes()\nmem_bytes = G.memory_usage()\nprint(\"Audit keys:\", list(audit.keys())[:10])\nprint(\"Approx memory usage (bytes):\", int(mem_bytes))\n</pre> # Audit &amp; memory audit = G.audit_attributes() mem_bytes = G.memory_usage() print(\"Audit keys:\", list(audit.keys())[:10]) print(\"Approx memory usage (bytes):\", int(mem_bytes)) <pre>Audit keys: ['extra_vertex_rows', 'extra_edge_rows', 'missing_vertex_rows', 'missing_edge_rows', 'invalid_edge_slice_rows']\nApprox memory usage (bytes): 1583876\n</pre> In\u00a0[42]: Copied! <pre># Timing summary\nimport pandas as pd\n\ntimings = {\n    \"build_slices\": build_slices_time,\n    \"build_vertices\": build_vertices_time,\n    \"build_binary_edges\": build_binary_time,\n    \"build_hyperedges\": build_hyper_time,\n    \"build_vertexedge\": build_vertexedge_time,\n}\n\ndf = pd.DataFrame(sorted(timings.items(), key=lambda x: x[0]), columns=[\"stage\", \"seconds\"])\n\nprint(\"Build timings (seconds)\", df)\n\n# Simple chart\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.bar(df[\"stage\"], df[\"seconds\"])\nplt.xticks(rotation=45, ha=\"right\")\nplt.ylabel(\"seconds\")\nplt.title(\"Graph Build Timings\")\nplt.tight_layout()\nplt.show()\n</pre> # Timing summary import pandas as pd  timings = {     \"build_slices\": build_slices_time,     \"build_vertices\": build_vertices_time,     \"build_binary_edges\": build_binary_time,     \"build_hyperedges\": build_hyper_time,     \"build_vertexedge\": build_vertexedge_time, }  df = pd.DataFrame(sorted(timings.items(), key=lambda x: x[0]), columns=[\"stage\", \"seconds\"])  print(\"Build timings (seconds)\", df)  # Simple chart import matplotlib.pyplot as plt  plt.figure() plt.bar(df[\"stage\"], df[\"seconds\"]) plt.xticks(rotation=45, ha=\"right\") plt.ylabel(\"seconds\") plt.title(\"Graph Build Timings\") plt.tight_layout() plt.show() <pre>Build timings (seconds)                 stage   seconds\n0  build_binary_edges  0.132508\n1    build_hyperedges  0.031174\n2        build_slices  0.002961\n3    build_vertexedge  0.456517\n4      build_vertices  0.409886\n</pre> In\u00a0[43]: Copied! <pre>from pathlib import Path\n\nout_dir = Path('stress'); out_dir.mkdir(exist_ok=True, parents=True)\ndemo_path = out_dir/'demo.annnet'\n\nG.write(demo_path, overwrite=True)  # lossless save\nprint('Wrote:', demo_path)\n\nimport annnet\n# Round-trip check\nG2 = annnet.Graph.read(demo_path)\nprint('Round-trip OK?', (G2.num_vertices, G2.num_edges) == (G.num_vertices, G.num_edges))\n</pre> from pathlib import Path  out_dir = Path('stress'); out_dir.mkdir(exist_ok=True, parents=True) demo_path = out_dir/'demo.annnet'  G.write(demo_path, overwrite=True)  # lossless save print('Wrote:', demo_path)  import annnet # Round-trip check G2 = annnet.Graph.read(demo_path) print('Round-trip OK?', (G2.num_vertices, G2.num_edges) == (G.num_vertices, G.num_edges)) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[43], line 6\n      3 out_dir = Path('stress'); out_dir.mkdir(exist_ok=True, parents=True)\n      4 demo_path = out_dir/'demo.annnet'\n----&gt; 6 G.write(demo_path, overwrite=True)  # lossless save\n      7 print('Wrote:', demo_path)\n      9 import annnet\n\nFile ~\\AnnNet\\annnet\\core\\graph.py:8922, in Graph.write(self, path, **kwargs)\n   8919 \"\"\"Save to .annnet format (zero loss).\"\"\"\n   8920 from ..io.io_annnet import write\n-&gt; 8922 write(self, path, **kwargs)\n\nFile ~\\AnnNet\\annnet\\io\\io_annnet.py:70, in write(graph, path, compression, overwrite)\n     56 root.mkdir(parents=True, exist_ok=overwrite)\n     58 # 1. Write manifest\n     59 manifest = {\n     60     \"format\": \"annnet\",\n     61     \"version\": \"1.0.0\",\n     62     \"created\": datetime.now(UTC).isoformat(),\n     63     \"annnet_version\": \"0.1.0\",\n     64     \"graph_version\": graph._version,\n     65     \"directed\": graph.directed,\n     66     \"counts\": {\n     67         \"vertices\": sum(1 for t in graph.entity_types.values() if t == \"vertex\"),\n     68         \"edges\": graph._num_edges,\n     69         \"entities\": graph._num_entities,\n---&gt; 70         \"layers\": len(graph._layers),\n     71         \"hyperedges\": sum(1 for k in graph.edge_kind.values() if k == \"hyper\"),\n     72     },\n     73     \"layers\": list(graph._layers.keys()),\n     74     \"active_layer\": graph._current_layer,\n     75     \"default_layer\": graph._default_layer,\n     76     \"compression\": compression,\n     77     # make encoding explicit for tests/docs\n     78     \"encoding\": {\"zarr\": \"v3\", \"parquet\": \"2.0\"},\n     79 }\n     80 (root / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n     82 # 2. Write structure/ (topology)\n\nAttributeError: 'Graph' object has no attribute '_layers'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"stress_notebook/#graph-endtoend-stress-notebook","title":"Graph \u2014 End\u2011to\u2011End Stress Notebook\u00b6","text":"<p>Goal: build a realistic, multi\u2011slice biological interaction graph with tens of thousands of vertices and a mix of binary edges, hyperedges, and vertex\u2013edge (edge\u2011entity) links, then exercise every public API: slices, presence queries, propagation (<code>shared</code> / <code>all</code>), views, analytics, set operations, aggregations, subgraph/copy, deletions, auditing, and memory usage.</p>"},{"location":"testing_new_updates/","title":"Testing new updates","text":"In\u00a0[1]: Copied! <pre>import sys\nimport os\nsys.path.insert(0, os.path.abspath(\"..\"))\nfrom annnet.core.graph import Graph\n</pre> import sys import os sys.path.insert(0, os.path.abspath(\"..\")) from annnet.core.graph import Graph <pre>/home/youssef/miniconda3/lib/python3.13/site-packages/numpy/_core/getlimits.py:552: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for &lt;class 'numpy.longdouble'&gt; does not match any known type: falling back to type probe function.\nThis warnings indicates broken support for the dtype!\n  machar = _get_machar(dtype)\n</pre> In\u00a0[4]: Copied! <pre>G = Graph()\n\nG.set_vertex_key(\"name\", \"sex\")\n\nm = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"man\")\nw = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"woman\")\n\nprint(m, w, m == w)   # expect two different ids, False\n</pre>  G = Graph()  G.set_vertex_key(\"name\", \"sex\")  m = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"man\") w = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"woman\")  print(m, w, m == w)   # expect two different ids, False   <pre>kv:name='a'|sex='man' kv:name='a'|sex='woman' False\n</pre> In\u00a0[6]: Copied! <pre>G.set_vertex_key(\"name\")\n</pre> G.set_vertex_key(\"name\")  <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~\\AnnNet\\annnet\\core\\graph.py:4217, in Graph.set_vertex_key(self, *fields)\n   4216 if owner is not None and owner != vid:\n-&gt; 4217     raise ValueError(f\"Composite key conflict for {key}: {owner} vs {vid}\")\n   4218 self._vertex_key_index[key] = vid\n\nValueError: Composite key conflict for ('a',): kv:name='a'|sex='man' vs kv:name='a'|sex='woman'\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 G.set_vertex_key(\"name\")\n\nFile ~\\AnnNet\\annnet\\core\\graph.py:4228, in Graph.set_vertex_key(self, *fields)\n   4226 owner = self._vertex_key_index.get(key)\n   4227 if owner is not None and owner != vid:\n-&gt; 4228     raise ValueError(f\"Composite key conflict for {key}: {owner} vs {vid}\")\n   4229 self._vertex_key_index[key] = vid\n\nValueError: Composite key conflict for ('a',): kv:name='a'|sex='man' vs kv:name='a'|sex='woman'</pre> In\u00a0[\u00a0]: Copied! <pre>m = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"man\")\nw = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"woman\")\nprint(m, w, m == w)   # same id, True\n</pre> m = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"man\") w = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"woman\") print(m, w, m == w)   # same id, True In\u00a0[9]: Copied! <pre>G = Graph()\nG.set_vertex_key(\"name\",\"sex\")\n\nu = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"man\")\nv = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"woman\")\n\n# Helper to read signs\ndef signs(G, eid):\n    s,t,_ = G.edge_definitions[eid]\n    col = G.edge_to_idx[eid]\n    si = G.entity_to_idx[s]; ti = G.entity_to_idx[t]\n    M = G._matrix\n    return M.get((si,col),0), M.get((ti,col),0)\n\n# Edge-scope policy\ne = G.add_edge(u, v,\n    flexible={\"var\":\"capacity\", \"threshold\":0.7, \"scope\":\"edge\", \"above\":\"s-&gt;t\", \"tie\": \"undirected\"},\n    capacity=0.5, weight=1.0)\n\nprint(\"init:\", signs(G,e))            # expect (-1, +1)\nG.set_edge_attrs(e, capacity=0.9)\nprint(\"after:\", signs(G,e))           # expect (+1, -1)\n</pre> G = Graph() G.set_vertex_key(\"name\",\"sex\")  u = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"man\") v = G.get_or_create_vertex_by_attrs(name=\"a\", sex=\"woman\")  # Helper to read signs def signs(G, eid):     s,t,_ = G.edge_definitions[eid]     col = G.edge_to_idx[eid]     si = G.entity_to_idx[s]; ti = G.entity_to_idx[t]     M = G._matrix     return M.get((si,col),0), M.get((ti,col),0)  # Edge-scope policy e = G.add_edge(u, v,     flexible={\"var\":\"capacity\", \"threshold\":0.7, \"scope\":\"edge\", \"above\":\"s-&gt;t\", \"tie\": \"undirected\"},     capacity=0.5, weight=1.0)  print(\"init:\", signs(G,e))            # expect (-1, +1) G.set_edge_attrs(e, capacity=0.9) print(\"after:\", signs(G,e))           # expect (+1, -1)    <pre>init: (-1.0, 1.0)\nafter: (1.0, -1.0)\n</pre> In\u00a0[11]: Copied! <pre># Vertex-scope policy\nG2 = Graph()\nG2.set_vertex_key(\"name\",\"sex\")\ns = G2.get_or_create_vertex_by_attrs(name=\"a\", sex=\"man\")\nt = G2.get_or_create_vertex_by_attrs(name=\"a\", sex=\"woman\")\nG2.set_vertex_attrs(s, temp=10.0)\nG2.set_vertex_attrs(t, temp=20.0)\n\ne2 = G2.add_edge(s, t,\n    flexible={\"var\":\"temp\",\"threshold\":0.0,\"scope\":\"vertex\",\"above\":\"s-&gt;t\",\"tie\":\"undirected\"},\n    weight=1.0)\n\nprint(\"xs&lt;xt:\", signs(G2,e2))         # (-1, +1)\nG2.set_vertex_attrs(s, temp=25.0)\nprint(\"xs&gt;xt:\", signs(G2,e2))         # (+1, -1)\nG2.set_vertex_attrs(t, temp=25.0)\nprint(\"tie  :\", signs(G2,e2))         # since \"tie\":\"undirected\", it becomes: (+1, +1)\n</pre> # Vertex-scope policy G2 = Graph() G2.set_vertex_key(\"name\",\"sex\") s = G2.get_or_create_vertex_by_attrs(name=\"a\", sex=\"man\") t = G2.get_or_create_vertex_by_attrs(name=\"a\", sex=\"woman\") G2.set_vertex_attrs(s, temp=10.0) G2.set_vertex_attrs(t, temp=20.0)  e2 = G2.add_edge(s, t,     flexible={\"var\":\"temp\",\"threshold\":0.0,\"scope\":\"vertex\",\"above\":\"s-&gt;t\",\"tie\":\"undirected\"},     weight=1.0)  print(\"xsxt:\", signs(G2,e2))         # (+1, -1) G2.set_vertex_attrs(t, temp=25.0) print(\"tie  :\", signs(G2,e2))         # since \"tie\":\"undirected\", it becomes: (+1, +1) <pre>xs&lt;xt: (-1.0, 1.0)\nxs&gt;xt: (1.0, -1.0)\ntie  : (1.0, 1.0)\n</pre> In\u00a0[2]: Copied! <pre># ---------- build a tiny Kivela multilayer graph ----------\n\nG = Graph(directed=True)\n\n# 1) Define aspects and elementary layers\nG.set_aspects(\n    aspects=[\"time\"],\n    elem_layers={\"time\": [\"t1\", \"t2\"]},\n)\n\n# 2) Add vertices in the base graph\nfor u in [\"A\", \"B\", \"C\"]:\n    G.add_vertex(u, name = f\"named {u}\")\n\n# 3) Declare vertex-layer presence (V_M)\nfor u in [\"A\", \"B\", \"C\"]:\n    G.add_presence(u, (\"t1\",))\n    G.add_presence(u, (\"t2\",))\n\n# 4) Add Kivela edges (all go through incidence under the hood)\n\n# intra edges in t1\nG.add_intra_edge_nl(\"A\", \"B\", (\"t1\",), weight=1.0)\nG.add_intra_edge_nl(\"B\", \"C\", (\"t1\",), weight=1.0)\n\n# intra edges in t2\nG.add_intra_edge_nl(\"A\", \"C\", (\"t2\",), weight=2.0)\n\n# inter-layer edge between A@t1 and B@t2\nG.add_inter_edge_nl(\"A\", (\"t1\",), \"B\", (\"t2\",), weight=0.5)\n\n# coupling edges A: t1 &lt;-&gt; t2 and B: t1 &lt;-&gt; t2\nG.add_coupling_edge_nl(\"A\", (\"t1\",), (\"t2\",), weight=1.0)\nG.add_coupling_edge_nl(\"B\", (\"t1\",), (\"t2\",), weight=1.0)\n\n# ---------- annotate layers and vertex-layer pairs ----------\n\n# Elementary layer attributes, stored in G.layer_attributes\nG.set_elementary_layer_attrs(\"time\", \"t1\", order=1, name=\"early\")\nG.set_elementary_layer_attrs(\"time\", \"t2\", order=2, name=\"late\")\n\n# vertex-layer attributes, stored in _vertex_layer_attrs\nG.set_vertex_layer_attrs(\"A\", (\"t1\",), activity=0.2, color=\"blue\")\nG.set_vertex_layer_attrs(\"A\", (\"t2\",), activity=0.9, color=\"red\")\nG.set_vertex_layer_attrs(\"B\", (\"t1\",), activity=0.5)\nG.set_vertex_layer_attrs(\"B\", (\"t2\",), activity=0.7)\n\nprint(\"vertex-layer attrs A@t1:\", G.get_vertex_layer_attrs(\"A\", (\"t1\",)))\nprint(\"vertex-layer attrs A@t2:\", G.get_vertex_layer_attrs(\"A\", (\"t2\",)))\nprint()\n\n# ---------- supra structures ----------\n\nA_supra = G.supra_adjacency()\nprint(\"Supra adjacency shape:\", A_supra.shape)\nprint(\"Supra adjacency matrix:\\n\", A_supra.toarray())\nprint()\n\nL_comb = G.supra_laplacian(kind=\"comb\")\nL_norm = G.supra_laplacian(kind=\"norm\")\nprint(\"Combinatorial Laplacian:\\n\", L_comb.toarray())\nprint()\nprint(\"Normalized Laplacian:\\n\", L_norm.toarray())\nprint()\n\nprint(\"vertex-layer index (row -&gt; (vertex, layer_tuple)):\")\nfor i, (u, aa) in enumerate(G._row_to_nl):\n    print(f\"  {i}: {u} @ {aa}\")\nprint()\n\ndeg = G.supra_degree()\nprint(\"Supra degrees per vertex-layer row:\", deg)\nprint()\n\nprint(\"Participation coefficient per vertex:\", G.participation_coefficient())\nprint(\"Versatility per vertex:\", G.versatility())\nprint()\n\nprint(\"Layer attribute table (Polars):\")\nprint(G.layer_attributes)\n</pre> # ---------- build a tiny Kivela multilayer graph ----------  G = Graph(directed=True)  # 1) Define aspects and elementary layers G.set_aspects(     aspects=[\"time\"],     elem_layers={\"time\": [\"t1\", \"t2\"]}, )  # 2) Add vertices in the base graph for u in [\"A\", \"B\", \"C\"]:     G.add_vertex(u, name = f\"named {u}\")  # 3) Declare vertex-layer presence (V_M) for u in [\"A\", \"B\", \"C\"]:     G.add_presence(u, (\"t1\",))     G.add_presence(u, (\"t2\",))  # 4) Add Kivela edges (all go through incidence under the hood)  # intra edges in t1 G.add_intra_edge_nl(\"A\", \"B\", (\"t1\",), weight=1.0) G.add_intra_edge_nl(\"B\", \"C\", (\"t1\",), weight=1.0)  # intra edges in t2 G.add_intra_edge_nl(\"A\", \"C\", (\"t2\",), weight=2.0)  # inter-layer edge between A@t1 and B@t2 G.add_inter_edge_nl(\"A\", (\"t1\",), \"B\", (\"t2\",), weight=0.5)  # coupling edges A: t1 &lt;-&gt; t2 and B: t1 &lt;-&gt; t2 G.add_coupling_edge_nl(\"A\", (\"t1\",), (\"t2\",), weight=1.0) G.add_coupling_edge_nl(\"B\", (\"t1\",), (\"t2\",), weight=1.0)  # ---------- annotate layers and vertex-layer pairs ----------  # Elementary layer attributes, stored in G.layer_attributes G.set_elementary_layer_attrs(\"time\", \"t1\", order=1, name=\"early\") G.set_elementary_layer_attrs(\"time\", \"t2\", order=2, name=\"late\")  # vertex-layer attributes, stored in _vertex_layer_attrs G.set_vertex_layer_attrs(\"A\", (\"t1\",), activity=0.2, color=\"blue\") G.set_vertex_layer_attrs(\"A\", (\"t2\",), activity=0.9, color=\"red\") G.set_vertex_layer_attrs(\"B\", (\"t1\",), activity=0.5) G.set_vertex_layer_attrs(\"B\", (\"t2\",), activity=0.7)  print(\"vertex-layer attrs A@t1:\", G.get_vertex_layer_attrs(\"A\", (\"t1\",))) print(\"vertex-layer attrs A@t2:\", G.get_vertex_layer_attrs(\"A\", (\"t2\",))) print()  # ---------- supra structures ----------  A_supra = G.supra_adjacency() print(\"Supra adjacency shape:\", A_supra.shape) print(\"Supra adjacency matrix:\\n\", A_supra.toarray()) print()  L_comb = G.supra_laplacian(kind=\"comb\") L_norm = G.supra_laplacian(kind=\"norm\") print(\"Combinatorial Laplacian:\\n\", L_comb.toarray()) print() print(\"Normalized Laplacian:\\n\", L_norm.toarray()) print()  print(\"vertex-layer index (row -&gt; (vertex, layer_tuple)):\") for i, (u, aa) in enumerate(G._row_to_nl):     print(f\"  {i}: {u} @ {aa}\") print()  deg = G.supra_degree() print(\"Supra degrees per vertex-layer row:\", deg) print()  print(\"Participation coefficient per vertex:\", G.participation_coefficient()) print(\"Versatility per vertex:\", G.versatility()) print()  print(\"Layer attribute table (Polars):\") print(G.layer_attributes)  <pre>vertex-layer attrs A@t1: {'activity': 0.2, 'color': 'blue'}\nvertex-layer attrs A@t2: {'activity': 0.9, 'color': 'red'}\n\nSupra adjacency shape: (9, 9)\nSupra adjacency matrix:\n [[0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  1.  0.  1.  0.5 0.  0.  0. ]\n [0.  1.  0.  0.  0.  0.  0.  0.  2. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  1.  0.  0.  0.  1.  0.  1.  0. ]\n [0.  0.5 0.  0.  1.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  1.  0.  0.  0.  0. ]\n [0.  0.  2.  0.  0.  0.  0.  0.  0. ]]\n\nCombinatorial Laplacian:\n [[ 0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n [ 0.   2.5 -1.   0.  -1.  -0.5  0.   0.   0. ]\n [ 0.  -1.   3.   0.   0.   0.   0.   0.  -2. ]\n [ 0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n [ 0.  -1.   0.   0.   3.  -1.   0.  -1.   0. ]\n [ 0.  -0.5  0.   0.  -1.   1.5  0.   0.   0. ]\n [ 0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n [ 0.   0.   0.   0.  -1.   0.   0.   1.   0. ]\n [ 0.   0.  -2.   0.   0.   0.   0.   0.   2. ]]\n\nNormalized Laplacian:\n [[ 1.          0.          0.          0.          0.          0.\n   0.          0.          0.        ]\n [ 0.          1.         -0.36514837  0.         -0.36514837 -0.25819889\n   0.          0.          0.        ]\n [ 0.         -0.36514837  1.          0.          0.          0.\n   0.          0.         -0.81649658]\n [ 0.          0.          0.          1.          0.          0.\n   0.          0.          0.        ]\n [ 0.         -0.36514837  0.          0.          1.         -0.47140452\n   0.         -0.57735027  0.        ]\n [ 0.         -0.25819889  0.          0.         -0.47140452  1.\n   0.          0.          0.        ]\n [ 0.          0.          0.          0.          0.          0.\n   1.          0.          0.        ]\n [ 0.          0.          0.          0.         -0.57735027  0.\n   0.          1.          0.        ]\n [ 0.          0.         -0.81649658  0.          0.          0.\n   0.          0.          1.        ]]\n\nvertex-layer index (row -&gt; (vertex, layer_tuple)):\n  0: A @ ('default',)\n  1: A @ ('t1',)\n  2: A @ ('t2',)\n  3: B @ ('default',)\n  4: B @ ('t1',)\n  5: B @ ('t2',)\n  6: C @ ('default',)\n  7: C @ ('t1',)\n  8: C @ ('t2',)\n\nSupra degrees per vertex-layer row: [0.  2.5 3.  0.  3.  1.5 0.  1.  2. ]\n\nParticipation coefficient per vertex: {'A': 0.4444444444444444, 'B': 0.0, 'C': 0.4444444444444444}\nVersatility per vertex: {'A': 1.0, 'B': 0.5504028580467587, 'C': 0.6167626148477086}\n\nLayer attribute table (Polars):\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 layer_id \u2506 order \u2506 name  \u2502\n\u2502 ---      \u2506 ---   \u2506 ---   \u2502\n\u2502 str      \u2506 i64   \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 time_t1  \u2506 1     \u2506 early \u2502\n\u2502 time_t2  \u2506 2     \u2506 late  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> In\u00a0[3]: Copied! <pre># ---------- Kivela layer algebra &amp; slice operations ----------\n\naa_t1 = (\"t1\",)\naa_t2 = (\"t2\",)\n\nprint(\"\\n=== Kivela layer vertex/edge sets ===\")\nprint(\"Vertices in layer t1:\", G.layer_vertex_set(aa_t1))\nprint(\"Vertices in layer t2:\", G.layer_vertex_set(aa_t2))\n\nprint(\"Intra edges in t1:\", G.layer_edge_set(aa_t1))\nprint(\"Intra edges in t2:\", G.layer_edge_set(aa_t2))\n\nprint(\"Edges touching t1 (intra+inter+coupling):\",\n      G.layer_edge_set(aa_t1, include_inter=True, include_coupling=True))\nprint(\"Edges touching t2 (intra+inter+coupling):\",\n      G.layer_edge_set(aa_t2, include_inter=True, include_coupling=True))\n\n# ---------- pure Kivela layer algebra (set-based) ----------\n\nprint(\"\\n=== Kivela layer algebra (set view) ===\")\nres_union = G.layer_union([aa_t1, aa_t2])\nprint(\"Union vertices (t1 \u222a t2):\", res_union[\"vertices\"])\nprint(\"Union edges   (t1 \u222a t2):\", res_union[\"edges\"])\n\nres_inter = G.layer_intersection([aa_t1, aa_t2])\nprint(\"Intersection vertices (t1 \u2229 t2):\", res_inter[\"vertices\"])\nprint(\"Intersection edges   (t1 \u2229 t2):\", res_inter[\"edges\"])\n\nres_diff = G.layer_difference(aa_t1, aa_t2)\nprint(\"Difference vertices (t1 \\\\ t2):\", res_diff[\"vertices\"])\nprint(\"Difference edges   (t1 \\\\ t2):\", res_diff[\"edges\"])\n\n# ---------- Kivela \u2192 slice bridge ----------\n\nprint(\"\\n=== Slice creation from Kivela layers ===\")\n\n# single-layer slices\nsid_t1 = G.create_slice_from_layer(\"L_t1\", aa_t1)\nsid_t2 = G.create_slice_from_layer(\"L_t2\", aa_t2)\n\nprint(\"All slices (incl. default):\", G.list_slices(include_default=True))\nprint(\"Slice L_t1 vertices:\", G.get_slice_vertices(sid_t1))\nprint(\"Slice L_t1 edges   :\", G.get_slice_edges(sid_t1))\nprint(\"Slice L_t2 vertices:\", G.get_slice_vertices(sid_t2))\nprint(\"Slice L_t2 edges   :\", G.get_slice_edges(sid_t2))\n\n# union / intersection / difference slices\nsid_union = G.create_slice_from_layer_union(\"L_union_t1_t2\", [aa_t1, aa_t2])\nsid_inter = G.create_slice_from_layer_intersection(\"L_inter_t1_t2\", [aa_t1, aa_t2])\nsid_diff  = G.create_slice_from_layer_difference(\"L_diff_t1_t2\", aa_t1, aa_t2)\n\nprint(\"Slice L_union_t1_t2 vertices:\", G.get_slice_vertices(sid_union))\nprint(\"Slice L_union_t1_t2 edges   :\", G.get_slice_edges(sid_union))\n\nprint(\"Slice L_inter_t1_t2 vertices:\", G.get_slice_vertices(sid_inter))\nprint(\"Slice L_inter_t1_t2 edges   :\", G.get_slice_edges(sid_inter))\n\nprint(\"Slice L_diff_t1_t2 vertices:\", G.get_slice_vertices(sid_diff))\nprint(\"Slice L_diff_t1_t2 edges   :\", G.get_slice_edges(sid_diff))\n\n# ---------- Subgraphs from Kivela layers ----------\n\nprint(\"\\n=== Subgraphs from Kivela layers ===\")\n\nGs_t1 = G.subgraph_from_layer_tuple(aa_t1)\nGs_t2 = G.subgraph_from_layer_tuple(aa_t2)\nGs_union = G.subgraph_from_layer_union([aa_t1, aa_t2])\nGs_diff = G.subgraph_from_layer_difference(aa_t1, aa_t2)\n\nprint(\"Subgraph t1 vertices:\", set(Gs_t1.entity_types.keys()))\nprint(\"Subgraph t1 edges   :\", set(Gs_t1.edge_definitions.keys()))\n\nprint(\"Subgraph t2 vertices:\", set(Gs_t2.entity_types.keys()))\nprint(\"Subgraph t2 edges   :\", set(Gs_t2.edge_definitions.keys()))\n\nprint(\"Subgraph union(t1,t2) vertices:\", set(Gs_union.entity_types.keys()))\nprint(\"Subgraph union(t1,t2) edges   :\", set(Gs_union.edge_definitions.keys()))\n\nprint(\"Subgraph t1\\\\t2 vertices:\", set(Gs_diff.entity_types.keys()))\nprint(\"Subgraph t1\\\\t2 edges   :\", set(Gs_diff.edge_definitions.keys()))\n\n# ---------- Optional: test LayerManager if exposed as G.layers ----------\n\nif hasattr(G, \"layers\"):\n    print(\"\\n=== LayerManager high-level API ===\")\n    print(\"All layer tuples:\", G.layers.layer_tuples())\n    print(\"vertex_set(t1):\", G.layers.vertex_set(aa_t1))\n    print(\"edge_set(t1):\", G.layers.edge_set(aa_t1))\n\n    lm_union = G.layers.union([aa_t1, aa_t2])\n    print(\"LayerManager union vertices:\", lm_union[\"vertices\"])\n    print(\"LayerManager union edges   :\", lm_union[\"edges\"])\n\n    lm_sid_t1 = G.layers.to_slice(aa_t1, slice_id=\"LM_t1\")\n    print(\"LM_t1 slice vertices:\", G.get_slice_vertices(lm_sid_t1))\n    print(\"LM_t1 slice edges   :\", G.get_slice_edges(lm_sid_t1))\n\n    Gs_lm = G.layers.subgraph(aa_t1)\n    print(\"LayerManager subgraph(t1) vertices:\", set(Gs_lm.entity_types.keys()))\n    print(\"LayerManager subgraph(t1) edges   :\", set(Gs_lm.edge_definitions.keys()))\n</pre> # ---------- Kivela layer algebra &amp; slice operations ----------  aa_t1 = (\"t1\",) aa_t2 = (\"t2\",)  print(\"\\n=== Kivela layer vertex/edge sets ===\") print(\"Vertices in layer t1:\", G.layer_vertex_set(aa_t1)) print(\"Vertices in layer t2:\", G.layer_vertex_set(aa_t2))  print(\"Intra edges in t1:\", G.layer_edge_set(aa_t1)) print(\"Intra edges in t2:\", G.layer_edge_set(aa_t2))  print(\"Edges touching t1 (intra+inter+coupling):\",       G.layer_edge_set(aa_t1, include_inter=True, include_coupling=True)) print(\"Edges touching t2 (intra+inter+coupling):\",       G.layer_edge_set(aa_t2, include_inter=True, include_coupling=True))  # ---------- pure Kivela layer algebra (set-based) ----------  print(\"\\n=== Kivela layer algebra (set view) ===\") res_union = G.layer_union([aa_t1, aa_t2]) print(\"Union vertices (t1 \u222a t2):\", res_union[\"vertices\"]) print(\"Union edges   (t1 \u222a t2):\", res_union[\"edges\"])  res_inter = G.layer_intersection([aa_t1, aa_t2]) print(\"Intersection vertices (t1 \u2229 t2):\", res_inter[\"vertices\"]) print(\"Intersection edges   (t1 \u2229 t2):\", res_inter[\"edges\"])  res_diff = G.layer_difference(aa_t1, aa_t2) print(\"Difference vertices (t1 \\\\ t2):\", res_diff[\"vertices\"]) print(\"Difference edges   (t1 \\\\ t2):\", res_diff[\"edges\"])  # ---------- Kivela \u2192 slice bridge ----------  print(\"\\n=== Slice creation from Kivela layers ===\")  # single-layer slices sid_t1 = G.create_slice_from_layer(\"L_t1\", aa_t1) sid_t2 = G.create_slice_from_layer(\"L_t2\", aa_t2)  print(\"All slices (incl. default):\", G.list_slices(include_default=True)) print(\"Slice L_t1 vertices:\", G.get_slice_vertices(sid_t1)) print(\"Slice L_t1 edges   :\", G.get_slice_edges(sid_t1)) print(\"Slice L_t2 vertices:\", G.get_slice_vertices(sid_t2)) print(\"Slice L_t2 edges   :\", G.get_slice_edges(sid_t2))  # union / intersection / difference slices sid_union = G.create_slice_from_layer_union(\"L_union_t1_t2\", [aa_t1, aa_t2]) sid_inter = G.create_slice_from_layer_intersection(\"L_inter_t1_t2\", [aa_t1, aa_t2]) sid_diff  = G.create_slice_from_layer_difference(\"L_diff_t1_t2\", aa_t1, aa_t2)  print(\"Slice L_union_t1_t2 vertices:\", G.get_slice_vertices(sid_union)) print(\"Slice L_union_t1_t2 edges   :\", G.get_slice_edges(sid_union))  print(\"Slice L_inter_t1_t2 vertices:\", G.get_slice_vertices(sid_inter)) print(\"Slice L_inter_t1_t2 edges   :\", G.get_slice_edges(sid_inter))  print(\"Slice L_diff_t1_t2 vertices:\", G.get_slice_vertices(sid_diff)) print(\"Slice L_diff_t1_t2 edges   :\", G.get_slice_edges(sid_diff))  # ---------- Subgraphs from Kivela layers ----------  print(\"\\n=== Subgraphs from Kivela layers ===\")  Gs_t1 = G.subgraph_from_layer_tuple(aa_t1) Gs_t2 = G.subgraph_from_layer_tuple(aa_t2) Gs_union = G.subgraph_from_layer_union([aa_t1, aa_t2]) Gs_diff = G.subgraph_from_layer_difference(aa_t1, aa_t2)  print(\"Subgraph t1 vertices:\", set(Gs_t1.entity_types.keys())) print(\"Subgraph t1 edges   :\", set(Gs_t1.edge_definitions.keys()))  print(\"Subgraph t2 vertices:\", set(Gs_t2.entity_types.keys())) print(\"Subgraph t2 edges   :\", set(Gs_t2.edge_definitions.keys()))  print(\"Subgraph union(t1,t2) vertices:\", set(Gs_union.entity_types.keys())) print(\"Subgraph union(t1,t2) edges   :\", set(Gs_union.edge_definitions.keys()))  print(\"Subgraph t1\\\\t2 vertices:\", set(Gs_diff.entity_types.keys())) print(\"Subgraph t1\\\\t2 edges   :\", set(Gs_diff.edge_definitions.keys()))  # ---------- Optional: test LayerManager if exposed as G.layers ----------  if hasattr(G, \"layers\"):     print(\"\\n=== LayerManager high-level API ===\")     print(\"All layer tuples:\", G.layers.layer_tuples())     print(\"vertex_set(t1):\", G.layers.vertex_set(aa_t1))     print(\"edge_set(t1):\", G.layers.edge_set(aa_t1))      lm_union = G.layers.union([aa_t1, aa_t2])     print(\"LayerManager union vertices:\", lm_union[\"vertices\"])     print(\"LayerManager union edges   :\", lm_union[\"edges\"])      lm_sid_t1 = G.layers.to_slice(aa_t1, slice_id=\"LM_t1\")     print(\"LM_t1 slice vertices:\", G.get_slice_vertices(lm_sid_t1))     print(\"LM_t1 slice edges   :\", G.get_slice_edges(lm_sid_t1))      Gs_lm = G.layers.subgraph(aa_t1)     print(\"LayerManager subgraph(t1) vertices:\", set(Gs_lm.entity_types.keys()))     print(\"LayerManager subgraph(t1) edges   :\", set(Gs_lm.edge_definitions.keys()))  <pre>\n=== Kivela layer vertex/edge sets ===\nVertices in layer t1: {'B', 'A', 'C'}\nVertices in layer t2: {'A', 'B', 'C'}\nIntra edges in t1: {'B--C@t1', 'A--B@t1'}\nIntra edges in t2: {'A--C@t2'}\nEdges touching t1 (intra+inter+coupling): {'B--C@t1', 'A--B@t1', 'B--B==t1~t2', 'A--A==t1~t2', 'A--B==t1~t2'}\nEdges touching t2 (intra+inter+coupling): {'A--B==t1~t2', 'B--B==t1~t2', 'A--C@t2', 'A--A==t1~t2'}\n\n=== Kivela layer algebra (set view) ===\nUnion vertices (t1 \u222a t2): {'A', 'C', 'B'}\nUnion edges   (t1 \u222a t2): {'B--C@t1', 'A--C@t2', 'A--B@t1'}\nIntersection vertices (t1 \u2229 t2): {'B', 'C', 'A'}\nIntersection edges   (t1 \u2229 t2): set()\nDifference vertices (t1 \\ t2): set()\nDifference edges   (t1 \\ t2): {'B--C@t1', 'A--B@t1'}\n\n=== Slice creation from Kivela layers ===\nAll slices (incl. default): ['default', 'L_t1', 'L_t2']\nSlice L_t1 vertices: {'B', 'A', 'C'}\nSlice L_t1 edges   : {'B--C@t1', 'A--B@t1'}\nSlice L_t2 vertices: {'A', 'B', 'C'}\nSlice L_t2 edges   : {'A--C@t2'}\nSlice L_union_t1_t2 vertices: {'B', 'C', 'A'}\nSlice L_union_t1_t2 edges   : {'B--C@t1', 'A--C@t2', 'A--B@t1'}\nSlice L_inter_t1_t2 vertices: {'B', 'C', 'A'}\nSlice L_inter_t1_t2 edges   : set()\nSlice L_diff_t1_t2 vertices: set()\nSlice L_diff_t1_t2 edges   : {'B--C@t1', 'A--B@t1'}\n\n=== Subgraphs from Kivela layers ===\nSubgraph t1 vertices: {'B', 'C', 'A'}\nSubgraph t1 edges   : {'B--C@t1', 'A--B@t1'}\nSubgraph t2 vertices: {'B', 'C', 'A'}\nSubgraph t2 edges   : {'A--C@t2'}\nSubgraph union(t1,t2) vertices: {'B', 'A', 'C'}\nSubgraph union(t1,t2) edges   : {'B--C@t1', 'A--C@t2', 'A--B@t1'}\nSubgraph t1\\t2 vertices: set()\nSubgraph t1\\t2 edges   : set()\n\n=== LayerManager high-level API ===\nAll layer tuples: [('t1',), ('t2',)]\nvertex_set(t1): {'B', 'A', 'C'}\nedge_set(t1): {'B--C@t1', 'A--B@t1'}\nLayerManager union vertices: {'A', 'C', 'B'}\nLayerManager union edges   : {'B--C@t1', 'A--C@t2', 'A--B@t1'}\nLM_t1 slice vertices: {'B', 'A', 'C'}\nLM_t1 slice edges   : {'B--C@t1', 'A--B@t1'}\nLayerManager subgraph(t1) vertices: {'B', 'C', 'A'}\nLayerManager subgraph(t1) edges   : {'B--C@t1', 'A--B@t1'}\n</pre> In\u00a0[3]: Copied! <pre>import graph_tool.all as gt\n</pre> import graph_tool.all as gt In\u00a0[4]: Copied! <pre>import os\nimport sys\n\nROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\nif ROOT not in sys.path:\n    sys.path.insert(0, ROOT)\n\nfrom annnet.adapters import graphtool_adapter as gtt\n</pre> import os import sys  ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\")) if ROOT not in sys.path:     sys.path.insert(0, ROOT)  from annnet.adapters import graphtool_adapter as gtt In\u00a0[5]: Copied! <pre>gtG, manifest = gtt.to_graphtool(G)\n\n# reconstruct an AnnNet graph:\nG2 = gtt.from_graphtool(gtG, manifest)\n</pre> gtG, manifest = gtt.to_graphtool(G)  # reconstruct an AnnNet graph: G2 = gtt.from_graphtool(gtG, manifest)  In\u00a0[6]: Copied! <pre>gtG\n</pre> gtG Out[6]: <pre>&lt;Graph object, directed, with 3 vertices and 6 edges, 1 internal vertex property, 2 internal edge properties, at 0x7fb77f1612b0&gt;</pre> In\u00a0[8]: Copied! <pre>pos = gt.sfdp_layout(gtG)\n\n# Draw with vertex ids as labels\ngt.graph_draw(\n    gtG,\n    pos=pos,\n    vertex_text=gtG.vp[\"id\"],\n    vertex_font_size=10,\n    output_size=(300,300),\n)\n</pre> pos = gt.sfdp_layout(gtG)  # Draw with vertex ids as labels gt.graph_draw(     gtG,     pos=pos,     vertex_text=gtG.vp[\"id\"],     vertex_font_size=10,     output_size=(300,300), ) Out[8]: <pre>&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7fb77f1612b0, at 0x7fb8a0ef3cd0&gt;</pre> In\u00a0[9]: Copied! <pre># choose a layer tuple\naa = (\"t1\",)  # for single-aspect \"time\"\n\n# subgraph of that layer\nG_t1 = G.subgraph_from_layer_tuple(aa)\n\n# project and draw\ngtG_t1, man_t1 = gtt.to_graphtool(G_t1)\npos_t1 = gt.sfdp_layout(gtG_t1)\ngt.graph_draw(\n    gtG_t1,\n    pos=pos_t1,\n    vertex_text=gtG_t1.vp[\"id\"],\n    vertex_font_size=10,\n    output_size=(600, 600),\n)\n</pre> # choose a layer tuple aa = (\"t1\",)  # for single-aspect \"time\"  # subgraph of that layer G_t1 = G.subgraph_from_layer_tuple(aa)  # project and draw gtG_t1, man_t1 = gtt.to_graphtool(G_t1) pos_t1 = gt.sfdp_layout(gtG_t1) gt.graph_draw(     gtG_t1,     pos=pos_t1,     vertex_text=gtG_t1.vp[\"id\"],     vertex_font_size=10,     output_size=(600, 600), ) Out[9]: <pre>&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7fb77f0f6850, at 0x7fb77e0fdc50&gt;</pre> In\u00a0[10]: Copied! <pre>manifest\n</pre> manifest Out[10]: <pre>{'version': 1,\n 'graph': {'directed': True, 'attributes': {}},\n 'vertices': {'types': {'A': 'vertex', 'B': 'vertex', 'C': 'vertex'},\n  'attributes': [{'vertex_id': 'A', 'name': 'named A'},\n   {'vertex_id': 'B', 'name': 'named B'},\n   {'vertex_id': 'C', 'name': 'named C'}]},\n 'edges': {'definitions': {'A--B@t1': ('A', 'B', 'regular'),\n   'B--C@t1': ('B', 'C', 'regular'),\n   'A--C@t2': ('A', 'C', 'regular'),\n   'A--B==t1~t2': ('A', 'B', 'regular'),\n   'A--A==t1~t2': ('A', 'A', 'regular'),\n   'B--B==t1~t2': ('B', 'B', 'regular')},\n  'weights': {'A--B@t1': 1.0,\n   'B--C@t1': 1.0,\n   'A--C@t2': 2.0,\n   'A--B==t1~t2': 0.5,\n   'A--A==t1~t2': 1.0,\n   'B--B==t1~t2': 1.0},\n  'directed': {'A--B@t1': True,\n   'B--C@t1': True,\n   'A--C@t2': True,\n   'A--B==t1~t2': True,\n   'A--A==t1~t2': True,\n   'B--B==t1~t2': True},\n  'direction_policy': {},\n  'hyperedges': {},\n  'attributes': [{'edge_id': 'A--B@t1', 'layer': 't1'},\n   {'edge_id': 'B--C@t1', 'layer': 't1'},\n   {'edge_id': 'A--C@t2', 'layer': 't2'}],\n  'kivela': {'edge_kind': {'A--B@t1': 'intra',\n    'B--C@t1': 'intra',\n    'A--C@t2': 'intra',\n    'A--B==t1~t2': 'inter',\n    'A--A==t1~t2': 'coupling',\n    'B--B==t1~t2': 'coupling'},\n   'edge_layers': {'A--B@t1': {'kind': 'single', 'layers': [['t1']]},\n    'B--C@t1': {'kind': 'single', 'layers': [['t1']]},\n    'A--C@t2': {'kind': 'single', 'layers': [['t2']]},\n    'A--B==t1~t2': {'kind': 'pair', 'layers': [['t1'], ['t2']]},\n    'A--A==t1~t2': {'kind': 'pair', 'layers': [['t1'], ['t2']]},\n    'B--B==t1~t2': {'kind': 'pair', 'layers': [['t1'], ['t2']]}}}},\n 'slices': {'data': {'default': {'vertices': ['C', 'B', 'A'],\n    'edges': ['A--C@t2',\n     'A--A==t1~t2',\n     'B--B==t1~t2',\n     'A--B@t1',\n     'A--B==t1~t2',\n     'B--C@t1'],\n    'attributes': {}}},\n  'slice_attributes': [],\n  'edge_slice_attributes': []},\n 'multilayer': {'aspects': ['time'],\n  'aspect_attrs': {},\n  'elem_layers': {'time': ['t1', 't2']},\n  'VM': [{'node': 'C', 'layer': ['t1']},\n   {'node': 'C', 'layer': ['t2']},\n   {'node': 'B', 'layer': ['default']},\n   {'node': 'A', 'layer': ['default']},\n   {'node': 'B', 'layer': ['t1']},\n   {'node': 'A', 'layer': ['t1']},\n   {'node': 'B', 'layer': ['t2']},\n   {'node': 'C', 'layer': ['default']},\n   {'node': 'A', 'layer': ['t2']}],\n  'edge_kind': {'A--B@t1': 'intra',\n   'B--C@t1': 'intra',\n   'A--C@t2': 'intra',\n   'A--B==t1~t2': 'inter',\n   'A--A==t1~t2': 'coupling',\n   'B--B==t1~t2': 'coupling'},\n  'edge_layers': {'A--B@t1': {'kind': 'single', 'layers': [['t1']]},\n   'B--C@t1': {'kind': 'single', 'layers': [['t1']]},\n   'A--C@t2': {'kind': 'single', 'layers': [['t2']]},\n   'A--B==t1~t2': {'kind': 'pair', 'layers': [['t1'], ['t2']]},\n   'A--A==t1~t2': {'kind': 'pair', 'layers': [['t1'], ['t2']]},\n   'B--B==t1~t2': {'kind': 'pair', 'layers': [['t1'], ['t2']]}},\n  'node_layer_attrs': [],\n  'layer_tuple_attrs': [],\n  'layer_attributes': [{'layer_id': 'time_t1', 'order': 1, 'name': 'early'},\n   {'layer_id': 'time_t2', 'order': 2, 'name': 'late'}]},\n 'tables': {'vertex_attributes': [{'vertex_id': 'A', 'name': 'named A'},\n   {'vertex_id': 'B', 'name': 'named B'},\n   {'vertex_id': 'C', 'name': 'named C'}],\n  'edge_attributes': [{'edge_id': 'A--B@t1', 'layer': 't1'},\n   {'edge_id': 'B--C@t1', 'layer': 't1'},\n   {'edge_id': 'A--C@t2', 'layer': 't2'}],\n  'slice_attributes': [],\n  'edge_slice_attributes': [],\n  'layer_attributes': [{'layer_id': 'time_t1', 'order': 1, 'name': 'early'},\n   {'layer_id': 'time_t2', 'order': 2, 'name': 'late'}]}}</pre>"},{"location":"testing_new_updates/#composite-indexing","title":"Composite indexing\u00b6","text":""},{"location":"testing_new_updates/#flexible-directionality","title":"Flexible directionality\u00b6","text":""},{"location":"testing_new_updates/#kivela-multilayers","title":"Kivela Multilayers\u00b6","text":""},{"location":"testing_new_updates/#graphtool","title":"graphtool\u00b6","text":""},{"location":"community/","title":"Join Us","text":"<p>Welcome to the annnet community! We follow open-source principles and encourage any sort of contribution. We communicate on GitHub, where we also organise our projects.</p> <ul> <li> <p> Where to Start</p> <p>If you'd like to learn how to contribute to our projects, please follow the steps outlined in the contribution guide.</p> <p> To the contribution guide</p> </li> </ul>"},{"location":"community/#contributing-guidelines-github-links","title":"Contributing Guidelines GitHub Links","text":"<ul> <li> <p>Contribution guidelines</p> </li> <li> <p>Code of Conduct</p> </li> <li> <p>Developer Guide</p> </li> </ul>"},{"location":"community/contribute-codebase/","title":"Developer Guide","text":"<p>Thank you for considering to contribute to the project! This guide will help you to get started with the development of the project. If you have any questions, please feel free to ask them in the issue tracker on GitHub.</p>"},{"location":"community/contribute-codebase/#small-contributions","title":"Small Contributions","text":"<p>If you want to contribute a small change (e.g., a bugfix), you can probably immediately go ahead and create a pull request. For more substantial changes or additions, please read on.</p>"},{"location":"community/contribute-codebase/#larger-contributions","title":"Larger Contributions","text":"<p>If you want to contribute a larger change, please create an issue first. This will allow us to discuss the change and make sure that it fits into the project.  It can happen that development for a feature is already in progress, so it is important to check first to avoid duplicate work. If you have any questions, feel free to approach us in any way you like.</p>"},{"location":"community/contribute-codebase/#dependency-management","title":"Dependency management","text":"<p>We use Poetry for dependency management. Please make sure that you have installed Poetry and set up the environment correctly before starting development.</p>"},{"location":"community/contribute-codebase/#setting-up-the-environment","title":"Setting up the environment","text":"<ul> <li> <p>Install dependencies from the lock file: <code>poetry install</code></p> </li> <li> <p>Use the environment: You can either run commands directly with <code>poetry run &lt;command&gt;</code> or open a shell with <code>poetry shell</code> and then run commands directly.</p> </li> </ul>"},{"location":"community/contribute-codebase/#updating-the-environment","title":"Updating the environment","text":"<p>If you want to fix dependency issues, please do so in the Poetry framework. If Poetry does not work for you for some reason, please let us know.</p> <p>The Poetry dependencies are organized in groups. There are groups with dependencies needed for running annnet (<code>[tool.poetry.dependencies]</code> with the group name <code>main</code>) and a group with dependencies needed for development (<code>[tool.poetry.group.dev.dependencies]</code> with the group name <code>dev</code>).</p> <p>For adding new dependencies:</p> <ul> <li> <p>Add new dependencies via <code>poetry add</code>: <code>poetry add &lt;dependency&gt; --group &lt;group&gt;</code>. This will update the <code>pyproject.toml</code> and lock file automatically.</p> </li> <li> <p>Add new dependencies via <code>pyproject.toml</code>: Add the dependency to the <code>pyproject.toml</code> file in the correct group, including version. Then update the lock file: <code>poetry lock</code> and install the dependencies: <code>poetry install</code>.</p> </li> </ul>"},{"location":"community/contribute-codebase/#code-quality-and-formal-requirements","title":"Code quality and formal requirements","text":"<p>For ensuring code quality, the following tools are used:</p> <ul> <li> <p>isort for sorting imports</p> </li> <li> <p>black for automated code formatting</p> </li> <li> <p>pre-commit-hooks for ensuring some general rules</p> </li> <li> <p>pep585-upgrade for automatically upgrading type hints to the new native types defined in PEP 585</p> </li> <li> <p>pygrep-hooks for ensuring some general naming rules</p> </li> <li> <p>Ruff An extremely fast Python linter and code formatter, written in Rust</p> </li> </ul> <p>We recommend configuring your IDE to execute Ruff on save/type, which will automatically keep your code clean and fix some linting errors as you type. This is made possible by the fast execution of Ruff and removes the need to run a dedicated pre-commit step. For instance, in VSCode or Cursor, you can add this to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"editor.formatOnType\": true,\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.fixAll.ruff\": \"explicit\",\n        \"source.organizeImports.ruff\": \"explicit\"\n    },\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n}\n</code></pre> <p>Alternatively, pre-commit hooks can be used to automatically or manually run these tools before each commit. They are defined in <code>.pre-commit-config.yaml</code>. To install the hooks run <code>poetry run pre-commit install</code>. The hooks are then executed before each commit. For running the hook for all project files (not only the changed ones) run <code>poetry run pre-commit run --all-files</code>. Our CI runs the pre-commit hooks, so running them locally is a good way to check if your code conforms to the formatting rules.</p>"},{"location":"community/contribute-codebase/#testing","title":"Testing","text":"<p>The project uses pytest for testing. To run the tests, please run <code>pytest</code> in the root directory of the project. We are developing annnet using test-driven development. Please make sure that you add tests for your code before submitting a pull request.</p> <p>The existing tests can also help you to understand how the code works. If you have any questions, please feel free to ask them in the issue tracker or on Zulip.</p> <p>Before submitting a pull request, please make sure that all tests pass and that the documentation builds correctly.</p>"},{"location":"community/contribute-codebase/#versioning","title":"Versioning","text":"<p>We use semantic versioning for the project. This means that the version number is incremented according to the following scheme:</p> <ul> <li> <p>Increment the major version number if you make incompatible API changes.</p> </li> <li> <p>Increment the minor version number if you add functionality in a backwards-   compatible manner. Since we are still in the 0.x.y version range, most of the   significant changes will increase the minor version number.</p> </li> <li> <p>Increment the patch version number if you make backwards-compatible bug fixes.</p> </li> </ul> <p>We use the <code>bumpversion</code> tool to update the version number in the <code>pyproject.toml</code> file. This will create a new git tag automatically. Usually, versioning is done by the maintainers, so please do not increment versions in pull requests by default.</p>"},{"location":"community/contribute-codebase/#finding-an-issue-to-contribute-to","title":"Finding an issue to contribute to","text":"<p>If you are brand new to annnet or open-source development, we recommend searching the GitHub \"Issues\" tab to find issues that interest you. Unassigned issues labeled <code>Docs</code> and <code>good first</code> are typically good for newer contributors.</p> <p>Once you've found an interesting issue, it's a good idea to assign the issue to yourself, so nobody else duplicates the work on it.</p> <p>If for whatever reason you are not able to continue working with the issue, please unassign it, so other people know it's available again. If you want to work on an issue that is currently assigned but you're unsure whether work is actually being done, feel free to kindly ask the current assignee if you can take over (please allow at least a week of inactivity before getting in touch).</p>"},{"location":"community/contribute-codebase/#submitting-a-pull-request","title":"Submitting a Pull Request","text":""},{"location":"community/contribute-codebase/#tips-for-a-successful-pull-request","title":"Tips for a successful pull request","text":"<p>To improve the chances of your pull request being reviewed, you should:</p> <ul> <li>Reference an open issue for non-trivial changes to clarify the PR's purpose.</li> <li>Ensure you have appropriate tests. Tests should be the focus of any PR (apart from documentation changes).</li> <li>Keep your pull requests as simple as possible. Larger PRs take longer to review.</li> <li>Ensure that CI is in a green state. Reviewers may tell you to fix the CI before looking at anything else.</li> </ul>"},{"location":"community/contribute-codebase/#version-control-git-and-github","title":"Version control, Git, and GitHub","text":"<p>annnet is hosted on GitHub, and to contribute, you will need to sign up for a free GitHub account. We use Git for version control to allow many people to work together on the project.</p> <p>If you are new to Git, you can reference some of these resources for learning Git. Feel free to reach out to the contributor community for help if needed:</p> <ul> <li>Git documentation.</li> </ul> <p>The project follows a forking workflow further described on this page whereby contributors fork the repository, make changes and then create a Pull Request. So please be sure to read and follow all the instructions in this guide.</p> <p>If you are new to contributing to projects through forking on GitHub, take a look at the GitHub documentation for contributing to projects. GitHub provides a quick tutorial using a test repository that may help you become more familiar with forking a repository, cloning a fork, creating a feature branch, pushing changes and making Pull Requests.</p> <p>Below are some useful resources for learning more about forking and Pull Requests on GitHub:</p> <ul> <li> <p>the GitHub documentation for forking a repo.</p> </li> <li> <p>the GitHub documentation for collaborating with Pull Requests.</p> </li> <li> <p>the GitHub documentation for working with forks.</p> </li> </ul> <p>There are also many unwritten rules and conventions that are helpful in interacting with other open-source contributors. These lessons from PyOpenSci are a good resource for learning more about how to interact with other open-source contributors in scientific computing.</p>"},{"location":"community/contribute-codebase/#getting-started-with-git","title":"Getting started with Git","text":"<p>GitHub has instructions for installing git, setting up your SSH key, and configuring git. All these steps need to be completed before you can work seamlessly between your local repository and GitHub.</p>"},{"location":"community/contribute-codebase/#create-a-fork-of-annnet","title":"Create a fork of annnet","text":"<p>You will need your own fork of annnetin order to eventually open a Pull Request. Go to the annnet project page and hit the Fork button. Please uncheck the box to copy only the main branch before selecting Create Fork. You will then want to clone your fork to your machine.</p> <pre><code>git clone https://github.com/your-user-name/annnet.git\ncd annnet\ngit remote add upstream https://github.com/annnet/annnet.git\ngit fetch upstream\n</code></pre> <p>This creates the directory <code>annnet</code> and connects your repository to the upstream (main project) annnet repository. They have the same name, but your local repository and fork are separate from the upstream repository.</p>"},{"location":"community/contribute-codebase/#creating-a-feature-branch","title":"Creating a feature branch","text":"<p>Your local <code>main</code> branch should always reflect the current state of annnet repository. First ensure it's up-to-date with the main annnet repository.</p> <pre><code>git checkout main\ngit pull upstream main --ff-only\n</code></pre> <p>Then, create a feature branch for making your changes. For example, we are going to create a branch called <code>my-new-feature-for-annnet</code></p> <pre><code>git checkout -b my-new-feature-for-annnet\n</code></pre> <p>This changes your working branch from <code>main</code> to the <code>my-new-feature-for-annnet</code> branch. Keep any changes in this branch specific to one bug or feature so it is clear what the branch brings to annnet. You can have many feature branches and switch between them using the <code>git checkout</code> command.</p>"},{"location":"community/contribute-codebase/#making-code-changes","title":"Making code changes","text":"<p>Before modifying any code, ensure you follow the contributing environment guidelines to set up an appropriate development environment.</p> <p>When making changes, follow these annnet-specific guidelines:</p> <ol> <li> <p>Keep changes of that branch/PR focused on a single feature or bug fix.</p> </li> <li> <p>Follow roughly the conventional commit message conventions.</p> </li> </ol>"},{"location":"community/contribute-codebase/#pushing-your-changes","title":"Pushing your changes","text":"<p>When you want your committed changes to appear publicly on your GitHub page, you can push your forked feature branch's commits to your forked repository on GitHub.</p> <p>Now your code is on GitHub, but it is not yet a part of the annnet project. For that to happen, a Pull Request (PR) needs to be submitted.</p>"},{"location":"community/contribute-codebase/#opening-a-pull-request-pr","title":"Opening a Pull Request (PR)","text":"<p>If everything looks good according to the general guidelines, you are ready to make a Pull Request. A Pull Request is how code from your fork becomes available to the project maintainers to review and merge into the project to appear in the next release. To submit a Pull Request:</p> <ol> <li> <p>Navigate to your repository on GitHub.</p> </li> <li> <p>Click on the Compare &amp; Pull Request button.</p> </li> <li> <p>You can then click on Commits and Files Changed to make sure everything looks okay one last time.</p> </li> <li> <p>Write a descriptive title that includes prefixes. annnet uses a convention for title prefixes, most commonly, <code>feat:</code> for features, <code>fix:</code> for bug fixes, and <code>refactor:</code> for refactoring.</p> </li> <li> <p>Write a description of your changes in the <code>Preview Discussion</code> tab. This description will inform the reviewers about the changes you made, so please include all relevant information, including the motivation, implementation details, and references to any issues that you are addressing.</p> </li> <li> <p>Make sure to <code>Allow edits from maintainers</code>; this allows the maintainers to make changes to your PR directly, which is useful if you are not sure how to fix the PR.</p> </li> <li> <p>Click <code>Send Pull Request</code>.</p> </li> <li> <p>Optionally, you can assign reviewers to your PR, if you know who should review it.</p> </li> </ol> <p>This request then goes to the repository maintainers, and they will review the code.</p>"},{"location":"community/contribute-codebase/#updating-your-pull-request","title":"Updating your Pull Request","text":"<p>Based on the review you get on your pull request, you will probably need to make some changes to the code. You can follow the steps above again to address any feedback and update your pull request.</p>"},{"location":"community/contribute-codebase/#parallel-changes-in-the-upstream-main-branch","title":"Parallel changes in the upstream <code>main</code> branch","text":"<p>In case of simultaneous changes to the upstream code, it is important that these changes are reflected in your pull request. To update your feature branch with changes in the annnet <code>main</code> branch, run:</p> <pre><code>    git checkout my-new-feature-for-annnet\n    git fetch upstream\n    git merge upstream/main\n</code></pre> <p>If there are no conflicts (or they could be fixed automatically), a file with a default commit message will open, and you can simply save and quit this file.</p> <p>If there are merge conflicts, you need to resolve those conflicts. See here for an explanation on how to do this.</p> <p>Once the conflicts are resolved, run:</p> <ol> <li><code>git add -u</code> to stage any files you've updated;</li> <li><code>git commit</code> to finish the merge.</li> </ol> <p>After the feature branch has been updated locally, you can now update your pull request by pushing to the branch on GitHub:</p> <pre><code>    git push origin my-new-feature-for-annnet\n</code></pre> <p>Any <code>git push</code> will automatically update your pull request with your branch's changes and restart the <code>Continuous Integration</code> checks.</p>"},{"location":"community/contribute-docs/","title":"Contributing to the documentation","text":"<p>Contributing to the documentation benefits everyone who uses annnet. We encourage you to help us improve the documentation, and you don't have to be an expert on annnet to do so! In fact, there are sections of the docs that are worse off after being written by experts. If something in the docs doesn't make sense to you, updating the relevant section after you figure it out is a great way to ensure it will help the next person.</p>"},{"location":"community/contribute-docs/#how-to-contribute-to-the-documentation","title":"How to contribute to the documentation","text":"<p>The documentation is written in Markdown, which is almost like writing in plain English, and built using Material for MkDocs. The simplest way to contribute to the docs is to click on the <code>Edit</code> button (pen and paper) at the top right of any page. This will take you to the source file on GitHub, where you can make your changes and create a pull request using GitHub's web interface (the <code>Commit changes...</code> button).</p> <p>Some other important things to know about the docs:</p> <ul> <li> <p>The annnet documentation consists of two parts: the docstrings in the code   itself and the docs in the <code>docs/</code> folder. The docstrings provide a clear   explanation of the usage of the individual functions, while the documentation   website you are looking at is built from the <code>docs/</code> folder.</p> </li> <li> <p>The docstrings follow a convention, based on the Google Docstring   Standard.</p> </li> <li> <p>Our API documentation files in <code>docs/reference/source</code> contain the   instructions for the auto-generated documentation from the docstrings. For   classes, there are a few subtleties around controlling which methods and   attributes have pages auto-generated.</p> </li> </ul>"},{"location":"community/contribute/","title":"How to Start Contributing","text":"<p>There are many valuable ways to contribute besides writing code. Thank you for dedicating your time to improve our project!</p>"},{"location":"community/contribute/#bug-reports-and-enhancement-requests","title":"Bug reports and enhancement requests","text":"<p>Bug reports and enhancement requests are an important part of making any software more stable. We curate them though Github issues. When opening an issue or request, please select the appropriate category and fill out the issue form fully to ensure others and the core development team can fully understand the scope of the issue. If your category is not listed, you can create a blank issue.</p> <p>The issue will then show up to the annnet community and be open to comments/ideas from others.</p>"},{"location":"community/contribute/#categories","title":"Categories","text":"<ul> <li>Bug Report: Report incorrect behavior in the annnet library</li> <li>Register New Component: Register a new component in the annnet ecosystem, either one you have created, or one that you would like to see added</li> <li>Documentation Improvement: Report wrong or missing documentation</li> <li>Feature Request: Suggest an idea for annnet</li> </ul>"},{"location":"community/contribute/#detailed-guides","title":"Detailed Guides","text":"<ul> <li> <p> Contributing to the Documentation</p> <p>A simple way to get started is to contribute to the documentation. Please follow the guide here to learn how to do so.</p> <p> To the contribution guide</p> </li> </ul> <ul> <li> <p> Contributing to the Code Base</p> <p>The best way to contribute code is to open a pull request on Github. Please follow the guide here to learn how to do so.</p> <p> To the contribution guide</p> </li> </ul>"},{"location":"reference/api/","title":"API Reference","text":"<p>annnet: single import, full API.</p>"},{"location":"reference/source/annnet/_metadata-docs/","title":"_metadata","text":""},{"location":"reference/source/annnet/_metadata-docs/#description","title":"Description","text":"<p>The <code>_metadata.py</code> file defines the structures and logic for handling metadata associated with cached items in the <code>annnet</code> package. It provides classes and helper functions to manage, store, and retrieve metadata fields, ensuring that each cache entry can be enriched with flexible, structured, and queryable information. This enables advanced search, filtering, and organization of cached data based on user-defined or system-generated metadata.</p>"},{"location":"reference/source/annnet/_metadata-docs/#main-components","title":"Main Components","text":"<ul> <li>Metadata Function:   Encapsulates the metadata for a cache item, providing methods to set, get, update, and validate metadata fields.</li> </ul> <p>Package metadata (version, authors, etc).</p>"},{"location":"reference/source/annnet/_metadata-docs/#annnet._metadata-functions","title":"Functions","text":""},{"location":"reference/source/annnet/_metadata-docs/#annnet._metadata.get_metadata","title":"<code>get_metadata()</code>","text":"<p>Basic package metadata.</p> <p>Retrieves package metadata from the current project directory or from the installed package.</p> Source code in <code>annnet/_metadata.py</code> <pre><code>def get_metadata() -&gt; dict:\n    \"\"\"Basic package metadata.\n\n    Retrieves package metadata from the current project directory or from\n    the installed package.\n    \"\"\"\n\n    here = pathlib.Path(__file__).parent\n    pyproj_toml = \"pyproject.toml\"\n    meta = {}\n\n    for project_dir in (here, here.parent):\n        toml_path = str(project_dir.joinpath(pyproj_toml).absolute())\n\n        if os.path.exists(toml_path):\n            pyproject = toml.load(toml_path)\n\n            meta = {\n                \"name\": pyproject[\"tool\"][\"poetry\"][\"name\"],\n                \"version\": pyproject[\"tool\"][\"poetry\"][\"version\"],\n                \"author\": pyproject[\"tool\"][\"poetry\"][\"authors\"],\n                \"license\": pyproject[\"tool\"][\"poetry\"][\"license\"],\n                \"full_metadata\": pyproject,\n            }\n\n            break\n\n    if not meta:\n        try:\n            meta = {k.lower(): v for k, v in importlib.metadata.metadata(here.name).items()}\n\n        except importlib.metadata.PackageNotFoundError:\n            pass\n\n    meta[\"version\"] = meta.get(\"version\", None) or _VERSION\n\n    return meta\n</code></pre>"},{"location":"source/conf/","title":"Conf","text":"<p>Configuration file for the Sphinx documentation builder.</p> <p>For the full list of built-in configuration values, see the documentation: https://www.sphinx-doc.org/en/master/usage/configuration.html</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\n</pre> import os import sys In\u00a0[\u00a0]: Copied! <pre>sys.path.insert(0, os.path.abspath(\".\"))\nsys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"..\")))\nsys.path.insert(0, os.path.abspath(\"../../src/annnet\"))\nsys.path.insert(0, os.path.abspath(\"../../src/annnet/demo\"))\n</pre> sys.path.insert(0, os.path.abspath(\".\")) sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"..\"))) sys.path.insert(0, os.path.abspath(\"../../src/annnet\")) sys.path.insert(0, os.path.abspath(\"../../src/annnet/demo\")) <p>-- Project information ----------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information</p> In\u00a0[\u00a0]: Copied! <pre>project = \"annnet\"\ncopyright = \"2025, Bottazzi Daniele\"\nauthor = \"Bottazzi Daniele\"\nrelease = \"0.0.1\"\n</pre> project = \"annnet\" copyright = \"2025, Bottazzi Daniele\" author = \"Bottazzi Daniele\" release = \"0.0.1\" <p>-- General configuration --------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration</p> In\u00a0[\u00a0]: Copied! <pre>extensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosectionlabel\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx.ext.mathjax\",\n]\n</pre> extensions = [     \"sphinx.ext.autodoc\",     \"sphinx.ext.autosectionlabel\",     \"sphinx.ext.napoleon\",     \"sphinx.ext.autosummary\",     \"sphinx.ext.doctest\",     \"sphinx.ext.viewcode\",     \"sphinx.ext.mathjax\", ] In\u00a0[\u00a0]: Copied! <pre>autosectionlabel_prefix_document = True\n</pre> autosectionlabel_prefix_document = True In\u00a0[\u00a0]: Copied! <pre>autodoc_default_options = {\n    \"members\": True,\n    \"undoc-members\": True,\n    \"private-members\": False,\n    \"special-members\": \"__init__\",\n    \"inherited-members\": True,\n    \"show-inheritance\": True,\n}\n</pre> autodoc_default_options = {     \"members\": True,     \"undoc-members\": True,     \"private-members\": False,     \"special-members\": \"__init__\",     \"inherited-members\": True,     \"show-inheritance\": True, } In\u00a0[\u00a0]: Copied! <pre># bibtex_bibfiles = ['refs.bib']\nbibtex_default_style = \"plain\"\ntemplates_path = [\"_templates\"]\nexclude_patterns = []\n</pre> # bibtex_bibfiles = ['refs.bib'] bibtex_default_style = \"plain\" templates_path = [\"_templates\"] exclude_patterns = [] <p>-- Options for HTML output ------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output</p> In\u00a0[\u00a0]: Copied! <pre>html_theme = \"sphinx_rtd_theme\"\nhtml_theme_options = {\n    \"sidebarwidth\": \"25%\",\n}\n</pre> html_theme = \"sphinx_rtd_theme\" html_theme_options = {     \"sidebarwidth\": \"25%\", } In\u00a0[\u00a0]: Copied! <pre>templates_path = [\"_templates\"]\n</pre> templates_path = [\"_templates\"] In\u00a0[\u00a0]: Copied! <pre>html_static_path = [\"_static\"]\n</pre> html_static_path = [\"_static\"] In\u00a0[\u00a0]: Copied! <pre>def skip_member(app, what, name, obj, skip, options):\n    if name == \"CustomArgumentParser\":\n        return True\n    return skip\n</pre> def skip_member(app, what, name, obj, skip, options):     if name == \"CustomArgumentParser\":         return True     return skip In\u00a0[\u00a0]: Copied! <pre>def setup(app):\n    app.connect(\"autodoc-skip-member\", skip_member)\n</pre> def setup(app):     app.connect(\"autodoc-skip-member\", skip_member)"}]}